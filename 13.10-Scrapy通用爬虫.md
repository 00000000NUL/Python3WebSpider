# 13.10 Scrapy通用爬虫

通过Scrapy，我们可以轻松地完成一个站点爬虫的编写。但如果抓取的站点量非常大，比如爬取各大媒体的新闻信息，多个Spider则可能包含很多重复代码。

如果我们将各个站点的Spider的公共部分保留下来，不同的部分提取出来作为单独的配置，如爬取规则、页面解析方式等抽离出来做成一个配置文件，那么我们在新增一个爬虫的时候，只需要实现这些网站的爬取规则和提取规则即可。

本节我们就来探究一下Scrapy通用爬虫的实现方法。

### 1. CrawlSpider

在实现通用爬虫之前我们需要先了解一下 CrawlSpider，其官方文档链接为：[http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider](http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider)。

CrawlSpider是Scrapy提供的一个通用Spider。在Spider里，我们可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门的数据结构Rule表示。Rule里包含提取和跟进页面的配置，Spider会根据Rule来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析等。

CrawlSpider继承自Spider类。除了Spider类的所有方法和属性，它还提供了一个非常重要的属性和方法。

* rules，它是爬取规则属性，是包含一个或多个Rule对象的列表。每个Rule对爬取网站的动作都做了定义，CrawlSpider会读取rules的每一个Rule并进行解析。

* parse_start_url()，它是一个可重写的方法。当start_urls里对应的Request得到Response时，该方法被调用，它会分析Response并必须返回Item对象或者Request对象。

这里最重要的内容莫过于Rule的定义了，它的定义和参数如下所示：

```python
class scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
```

下面对其参数依次说明：

* link_extractor，是一个 Link Extractor 对象。通过它，Spider可以知道从爬取的页面中提取哪些链接。提取出的链接会自动生成Request。它又是一个数据结构，一般常用LxmlLinkExtractor对象作为参数，其定义和参数如下所示：

```python
class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=False, unique=True, process_value=None, strip=True)
```

allow是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进。deny则相反。allow_domains定义了符合要求的域名，只有此域名的链接才会被跟进生成新的Request，它相当于域名白名单。deny_domains则相反，相当于域名黑名单。restrict_xpaths定义了从当前页面中XPath匹配的区域提取链接，其值是XPath表达式或XPath表达式列表。restrict_css定义了从当前页面中CSS选择器匹配的区域提取链接，其值是CSS选择器或CSS选择器列表。还有一些其他参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高。可以参考文档的参数说明：http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml。

* callback，即回调函数，和之前定义Request的callback有相同的意义。每次从link_extractor中获取到链接时，该函数将会调用。该回调函数接收一个response作为其第一个参数，并返回一个包含Item或Request对象的列表。注意，避免使用parse()作为回调函数。由于CrawlSpider使用parse()方法来实现其逻辑，如果parse()方法覆盖了，CrawlSpider将会运行失败。

* cb_kwargs，字典，它包含传递给回调函数的参数。

* follow，布尔值，即True或False，它指定根据该规则从response提取的链接是否需要跟进。如果callback参数为None，follow默认设置为True，否则默认为False。

* process_links，指定处理函数，从link_extractor中获取到链接列表时，该函数将会调用，它主要用于过滤。

* process_request，同样是指定处理函数，根据该Rule提取到每个Request时，该函数都会调用，对Request进行处理。该函数必须返回Request或者None。

以上内容便是CrawlSpider中的核心Rule的基本用法。但这些内容可能还不足以完成一个CrawlSpider爬虫。下面我们利用CrawlSpider实现新闻网站的爬取实例，来更好地理解Rule的用法。

### 2. Item Loader

我们了解了利用CrawlSpider的Rule来定义页面的爬取逻辑，这是可配置化的一部分内容。但是，Rule并没有对Item的提取方式做规则定义。对于Item的提取，我们需要借助另一个模块Item Loader来实现。

Item Loader提供一种便捷的机制来帮助我们方便地提取Item。它提供的一系列API可以分析原始数据对Item进行赋值。Item提供的是保存抓取数据的容器，而Item Loader提供的是填充容器的机制。有了它，数据的提取会变得更加规则化。

Item Loader的API如下所示：

```python
class scrapy.loader.ItemLoader([item, selector, response, ] **kwargs)
```

Item Loader的API返回一个新的Item Loader来填充给定的Item。如果没有给出Item，则使用default_item_class中的类自动实例化。另外，它传入selector和response参数来使用选择器或响应参数实例化。

下面将依次说明Item Loader的API参数。

* item，Item 对象，可以调用 add_xpath()、add_css() 或 add_value() 等方法来填充 Item 对象。
* selector，Selector 对象，用来提取填充数据的选择器。
* response，Response 对象，用于使用构造选择器的 Response。

一个比较典型的 Item Loader 实例如下：

```python
from scrapy.loader import ItemLoader
from project.items import Product

def parse(self, response):
    loader = ItemLoader(item=Product(), response=response)
    loader.add_xpath('name', '//div[@class="product_name"]')
    loader.add_xpath('name', '//div[@class="product_title"]')
    loader.add_xpath('price', '//p[@id="price"]')
    loader.add_css('stock', 'p#stock]')
    loader.add_value('last_updated', 'today')
    return loader.load_item()
```

首先声明了一个 Product Item，然后用该 Item 和 Response 对象实例化了 ItemLoader，随后调用了 add_xpath() 方法把来自两个不同位置的数据提取出来，分配给 name 属性，随后类似地，再用 add_xpath()、add_css()、add_value() 等方法对不同属性依次赋值，最后调用 load_item() 方法实现 Item 的解析，可以发现这种方式比较规则化，我们可以把一些参数和规则单独提取出来做成配置文件或存到数据库即可实现可配置化。

另外 Item Loader 在每个字段中都包含了一个 Input Processor（输入处理器）和一个 Output Processor（输出处理器）｡ Input Processor 收到数据时立刻提取数据，然后 Input Processor 的结果被收集起来并且保存在 ItemLoader 内，但是不分配给 Item，收集到所有的数据后, 调用 load_item() 方法来填充再生成 Item 对象，在调用时会先调用 Output Processor 来处理之前收集到的数据，然后再存入 Item 中，这样就生成了 Item。

在这里将一些内置的的 Processor 介绍如下：

#### Identity

这是最简单的 Processor，不进行任何处理，直接返回原来的数据。

#### TakeFirst

可以返回列表的第一个非空值，类似 extract_first() 的功能，常用作 Output Processor，例如：

```python
from scrapy.loader.processors import TakeFirst
processor = TakeFirst()
print(processor(['', 1, 2, 3]))
```

输出结果为：

```
1
```

可以看到经过此 Processor 处理后的结果返回了第一个不为空的值。

#### Join

此方法相当于字符串的 join() 方法，可以把列表拼合成字符串，默认使用空格分隔，例如：

```python
from scrapy.loader.processors import Join
processor = Join()
print(processor(['one', 'two', 'three']))
```

输出结果为：

```
one two three
```

也可以通过参数更改默认的分隔符，例如改成逗号：

```python
from scrapy.loader.processors import Join
processor = Join(',')
print(processor(['one', 'two', 'three']))
```

运行结果：

```
one,two,three
```

#### Compose

它是用给定的多个函数的组合而构造的 Processor，每个输入值被传递到第一个函数，然后其输出再传递到第二个函数，依次类推，直到最后一个函数返回整个处理器的输出。

例如：

```python
from scrapy.loader.processors import Compose
processor = Compose(str.upper, lambda s: s.strip())
print(processor(' hello world'))
```

运行结果：

```
HELLO WORLD
```

在这里我们构造了一个 Compose Processor，传入了一个开头带有空格的字符串，然后 Compose Processor 的参数有两个，第一个是 str.upper 它可以将字母全部转为大写，第二个参数是一个匿名函数，调用了 strip() 方法去除头尾空白字符，Compose 会顺次调用两个方法，最后得到返回结果，可以看到最终的结果字符串已全部转化为大写并且去除了开头的空格。

#### MapCompose

与 Compose 类似，MapCompose 可以用于迭代处理一个列表输入值，例如：

```python
from scrapy.loader.processors import MapCompose
processor = MapCompose(str.upper, lambda s: s.strip())
print(processor(['Hello', 'World', 'Python']))
```

运行结果：

```
['HELLO', 'WORLD', 'PYTHON']
```

被处理的内容是一个可迭代对象，MapCompose 会将该对象遍历然后依次处理。

#### SelectJmes

它可以查询一个 Json，传入 Key，返回查询所得的 Value，不过这里需要先安装 Jmespath 库才可以使用，命令如下：

```
pip3 install jmespath
```

安装好 Jmespath 之后便可以使用这个 Processor 了。

例如：

```python
from scrapy.loader.processors import SelectJmes
proc = SelectJmes('foo')
processor = SelectJmes('foo')
print(processor({'foo': 'bar'}))
```

运行结果：

```
bar
```

以上便是一些常用的 Processor，在本节的实例中我们会使用 Processor 来进行数据的处理。

以上便是 Item Loader 的简单用法介绍，接下来让我们用一个实例来了解它的用法。

### 3. 本节目标

这里我们以中华网科技类新闻为例来了解一下 CrawlSpider 和 Item Loader 的用法，然后再提取其可配置信息实现可配置化，官网链接为：[http://tech.china.com/](http://tech.china.com/)，我们需要爬取它的科技类新闻内容，链接为：[http://tech.china.com/articles/](http://tech.china.com/articles/)，页面如图 13-22 所示：

![](./assets/13-22.png)

图 13-22 爬取站点

我们需要将新闻列表中的所有分页的新闻详情抓取下来，包括标题、正文、时间、来源等信息。

### 4. 新建项目

首先我们新建一个 Scrapy 项目，名称叫做  scrapyuniversal，命令如下：

```
scrapy startproject scrapyuniversal
```

接下来我们创建一个 CrawlSpider，创建 CrawlSpider 需要制定一个模板，在这里我们可以先看下有哪些可用模板，命令如下：

```
scrapy genspider -l
```

运行结果：

```
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
```

在之前我们创建 Spider 的时候实际上是默认使用了第一个模板 basic，这次如果我们要创建 CrawlSpider，就需要使用第二个模板 crawl，创建命令如下：

```
scrapy genspider -t crawl china tech.china.com
```

运行之后便会生成一个 CrawlSpider，其内容如下：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class ChinaSpider(CrawlSpider):
    name = 'china'
    allowed_domains = ['tech.china.com']
    start_urls = ['http://tech.china.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i
```

可以发现这次生成的 Spider 内容就有所不同了，多了一个 rules 属性的定义，Rule 的第一个参数是 LinkExtractor，其实就是上文所说的 LxmlLinkExtractor，只是名称不同。同时默认的回调函数也不再是 parse，而是 parse_item。

### 4. 定义Rule

要实现新闻的爬取我们需要做的就是定义好 Rule，然后实现解析函数就好了，下面我们就来一步步实现这个过程。

首先我们需要先将 start_urls 修改为起始链接，代码如下：

```python
start_urls = ['http://tech.china.com/articles/']
```

定义好了之后 Spider 就会爬取 start_urls 里面的每一个链接，所以这里第一个爬取的页面就是我们刚才所定义的链接，得到 Response 之后 Spider 就会根据每一个 Rule 来去提取这个页面内的超链接去生成进一步的Request，所以我们接下来就需要定义 Rule 来指定提取哪些链接。

当前页面如图 13-23 所示：

![](./assets/13-23.png)

图 12-23 页面内容

这是新闻的列表页，所以很自然地下一步必然是将列表中的每条新闻详情的链接提取出来，我们这里直接指定这些链接所在区域即可，审查源代码可以发现这所有链接都在 id 为 left_side 的节点内，具体来说是其内的 class 为 con_item 的节点，如图 13-24 所示：

![](./assets/13-24.jpg)

图 13-24 列表源码

所以此处我们可以用 LinkExtractor 的 restrict_xpaths 属性来指定，指定了之后 Spider 就会从这个区域提取所有的超链接并生成 Request，但是同时注意到每篇文章的导航中可能还有一些其他的超链接标签，但是我们只想把需要的新闻链接提取出来，查看源码后可以观察到真正的新闻链接路径都是以 article 开头的，我们用一个正则表达式将其匹配出来再赋值给 allow 参数即可。另外这些链接对应的页面其实就是对应的新闻详情页，而我们需要解析的就是新闻的详情信息，所以此处还需要指定一个回调函数 callback。

到现在我们分析了如上页面就可以构造出一个 Rule 了，代码如下：

```python
Rule(LinkExtractor(allow='article\/.*\.html', restrict_xpaths='//div[@id="left_side"]//div[@class="con_item"]'), callback='parse_item')
```

接下来当前页面我们还需要让它做到分页功能，所以还需要提取下一页的链接，分析网页源码之后可以发现它是在 id 为 pageStyle 的节点内，如图 13-25 所示：

![](./assets/13-25.jpg)

图 13-25 分页源码

但是这里可以发现我们所需要的下一页节点和其他的分页链接区分度不高，要取出此链接我们可以直接用 XPath 的文本匹配方式，所以这里我们直接用 LinkExtractor 的 restrict_xpaths 属性来指定提取的链接即可。另外此分页链接对应的页面我们不需要像新闻详情页一样去提取详情信息，也就是不需要生成 Item，所以不需要加 callback 参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个 follow 参数为 True，代表继续跟进匹配分析。其实这个 follow 参数可以加可不加，因为当 callback 为空的时候，follow 默认为 True，所以此处 Rule 可以定义为：

```python
Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
```

所以现在 rules 就变成了：

```python
rules = (
    Rule(LinkExtractor(allow='article\/.*\.html', restrict_xpaths='//div[@id="left_side"]//div[@class="con_item"]'), callback='parse_item'),
    Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
)
```

接着我们运行一下代码，命令如下：

```
scrapy crawl china
```

即可发现现在已经可以实现页面的翻页和详情页的抓取了，我们仅仅通过定义了两个 Rule 即实现了这样的功能，运行效果如图 13-26 所示：

![](./assets/13-26.jpg)

运行效果

### 5. 解析页面

接下来我们需要做的就是解析页面内容了，在这里我们将标题、发布时间、正文、来源提取出来即可，所以这里我们首先定义一个 Item 如下：

```python
from scrapy import Field, Item

class NewsItem(Item):
    title = Field()
    url = Field()
    text = Field()
    datetime = Field()
    source = Field()
    website = Field()
```

在这里的字段分别指新闻标题、链接、正文、发布时间、来源、站点名称，其中站点名称就直接赋值为中华网即可，因为既然是通用爬虫，肯定还有很多爬虫来爬取也来爬取同样结构的新闻内容，所以需要一个字段来区分一下站点名称。

详情页的预览图如图 13-27 所示：

![](./assets/13-27.png)

图 13-27 详情页面

如果我们像之前一样提取内容的话，就直接调用 response 变量的 xpath()、css() 等方法即可，这里 parse_item() 方法的实现如下：

```python
def parse_item(self, response):
    item = NewsItem()
    item['title'] = response.xpath('//h1[@id="chan_newsTitle"]/text()').extract_first()
    item['url'] = response.url
    item['text'] = ''.join(response.xpath('//div[@id="chan_newsDetail"]//text()').extract()).strip()
    item['datetime'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('(\d+-\d+-\d+\s\d+:\d+:\d+)')
    item['source'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('来源：(.*)').strip()
    item['website'] = '中华网'
    yield item
```

这样我们就把每条新闻的信息提取形成了一个 NewsItem 对象。

这时实际上我们就已经完成了 Item 的提取了，此时再运行一下 Spider：

```
scrapy crawl china
```

输出内容如图 13-28 所示：

![](./assets/13-28.jpg)

图 13-28 输出内容

可以看到现在我们就可以成功将每条新闻的信息提取出来了。

不过我们发现这种提取方式非常不规整，下面我们再用 Item Loader 来实现一下提取，通过 add_xpath()、add_css()、add_value() 等方式实现配置化提取，我们可以把 parse_item() 改写如下：

```python
def parse_item(self, response):
    loader = ChinaLoader(item=NewsItem(), response=response)
    loader.add_xpath('title', '//h1[@id="chan_newsTitle"]/text()')
    loader.add_value('url', response.url)
    loader.add_xpath('text', '//div[@id="chan_newsDetail"]//text()')
    loader.add_xpath('datetime', '//div[@id="chan_newsInfo"]/text()', re='(\d+-\d+-\d+\s\d+:\d+:\d+)')
    loader.add_xpath('source', '//div[@id="chan_newsInfo"]/text()', re='来源：(.*)')
    loader.add_value('website', '中华网')
    yield loader.load_item()
```

这里我们定义了一个 ItemLoader 的子类，叫做 ChinaLoader，其实现如下：

```python
from scrapy.loader import ItemLoader
from scrapy.loader.processors import TakeFirst, Join, Compose

class NewsLoader(ItemLoader):
    default_output_processor = TakeFirst()

class ChinaLoader(NewsLoader):
    text_out = Compose(Join(), lambda s: s.strip())
    source_out = Compose(Join(), lambda s: s.strip())
```

在这里 ChinaLoader 是继承了 NewsLoader 类，其内定义了一个通用的 Out Processor 为 TakeFirst，这也就相当于之前我们所定义的 extract_first() 方法相同的功能，在 ChinaLoader 中又定义了 text_out 和 source_out 字段，这里又用了一个 Compose 这个 Processor，这里有两个参数，第一个参数 Join 也是一个 Processor，它可以把列表拼合成一个字符串，第二个参数是一个匿名函数，可以将字符串的头尾空白字符去掉，经过这一系列处理之后就可以做到将列表形式的提取结果转化为去重头尾空白字符的字符串了。

重新运行一下，可以发现提取效果是完全一样的。

至此，我们可以说已经实现了爬虫的半通用化配置。

### 6. 通用配置抽取

为什么说现在做到的这一步是半通用化呢？因为如果我们需要扩展其他站点的话仍然需要创建一个新的 CrawlSpider，定义这个站点的 Rule，而且还需要单独去实现 parse_item() 方法，实际上还有很多的代码是重复的，如 CrawlSpider 的变量、方法名几乎都是一样的，那么我们可不可以把它们共用一份代码，把完全不相同的地方抽离出来，做成可配置文件呢？

当然可以。那我们可以抽离出哪些部分呢？所有的变量均可以抽取，如 name、allowed_domains、start_urls、rules 等等，我们将这些变量在 CrawlSpider 初始化的时候赋值即可，所以这时我们就可以新建一个通用的 Spider 来实现一下，命令如下：

```
scrapy genspider -t crawl universal universal
```

这样我们就新建了一个全新的 Spider，名称为 universal，接下来我们将刚才所写的 Spider 内的属性全都抽离出来放到一个配置文件内，配置成一个 Json，命名为 china.json，放到一个 configs 文件夹，和 spiders 文件夹并列，代码如下：

```json
{
  "spider": "universal",
  "website": "中华网科技",
  "type": "新闻",
  "index": "http://tech.china.com/",
  "settings": {
    "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"
  },
  "start_urls": [
    "http://tech.china.com/articles/"
  ],
  "allowed_domains": [
    "tech.china.com"
  ],
  "rules": "china"
}
```

在这里第一个字段 spider 即使用的 Spider 的名称，在这里使用 universal，然后是站点的描述比如站点名称、类型、首页等等。随后的 settings 是该 Spider 特有的 settings 配置，如果要覆盖全局项目 settings.py 内的配置可以单独为其配置，随后是 Spider 的一些属性，如 start_urls、allowed_domains、rules 等，这里 rules 我们也可以单独定义成一个 rules.py 文件，实现 Rule 的分离，做成配置文件，例如：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import Rule

rules = {
    'china': (
        Rule(LinkExtractor(allow='article\/.*\.html', restrict_xpaths='//div[@id="left_side"]//div[@class="con_item"]'),
             callback='parse_item'),
        Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
    )
}
```

这样我们就算把基本的配置抽取出来了，如果要启动爬虫只需要从该配置文件中读取然后动态加载到 Spider 中即可，所以我们需要定义一个读取该 Json 文件的方法，定义如下：

```python
from os.path import realpath, dirname
import json
def get_config(name):
    path = dirname(realpath(__file__)) + '/configs/' + name + '.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.loads(f.read())
```

定义了 get_config() 方法之后我们只需要向其传入 Json 配置文件的名称即可获取此 Json 配置信息，随后我们定义一个入口文件 run.py，放在项目根目录下，它的作用是启动 Spider，定义如下：

```python
import sys
from scrapy.utils.project import get_project_settings
from scrapyuniversal.spiders.universal import UniversalSpider
from scrapyuniversal.utils import get_config
from scrapy.crawler import CrawlerProcess

def run():
    name = sys.argv[1]
    custom_settings = get_config(name)
    # 爬取使用的Spider名称
    spider = custom_settings.get('spider', 'universal')
    project_settings = get_project_settings()
    settings = dict(project_settings.copy())
    # 合并配置
    settings.update(custom_settings.get('settings'))
    process = CrawlerProcess(settings)
    # 启动爬虫
    process.crawl(spider, **{'name': name})
    process.start()

if __name__ == '__main__':
    run()
```

运行入口为 run()，首先获取了命令行的参数并赋值为 name，name 就是 Json 文件的名称，其实也就相当于要爬取的目标网站的名称，在这里首先利用 get_config() 方法并传入该名称读取我们刚才定义的配置文件，然后获取爬取使用的 spider 的名称，随后获取了配置文件中的 settings 配置，然后和项目全局的 settings 做了合并，随后新建一个 CrawlerProcess，传入爬取使用的配置，随后调用 crawl() 和 start() 方法即可启动爬取。

在 universal 这个 Spider 中我们新建一个 __init__() 方法，进行初始化配置，实现如下：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapyuniversal.utils import get_config
from scrapyuniversal.rules import rules

class UniversalSpider(CrawlSpider):
    name = 'universal'
    def __init__(self, name, *args, **kwargs):
        config = get_config(name)
        self.config = config
        self.rules = rules.get(config.get('rules'))
        self.start_urls = config.get('start_urls')
        self.allowed_domains = config.get('allowed_domains')
        super(UniversalSpider, self).__init__(*args, **kwargs)
    
    def parse_item(self, response):
        i = {}
        return i
```

在 __init__() 方法中赋值了 start_urls、allowed_domains、rules 等属性，其中 rules 属性是另外读取了 rules.py 的配置，这样就成功实现爬虫的基础配置了。

接下来我们可以执行如下命令运行爬虫：

```
python3 run.py china
```

运行此命令后，程序会首先读取 Json 配置文件，然后将配置中的一些属性赋值给 Spider，然后启动爬取，运行效果完全相同，运行结果如图 13-29 所示：

![](./assets/13-29.jpg)

图 13-29 运行结果

现在我们已经把 Spider 的基础属性实现了可配置化了，剩下的解析部分我们同样需要实现可配置化，原来的解析函数为：

```python
def parse_item(self, response):
    loader = ChinaLoader(item=NewsItem(), response=response)
    loader.add_xpath('title', '//h1[@id="chan_newsTitle"]/text()')
    loader.add_value('url', response.url)
    loader.add_xpath('text', '//div[@id="chan_newsDetail"]//text()')
    loader.add_xpath('datetime', '//div[@id="chan_newsInfo"]/text()', re='(\d+-\d+-\d+\s\d+:\d+:\d+)')
    loader.add_xpath('source', '//div[@id="chan_newsInfo"]/text()', re='来源：(.*)')
    loader.add_value('website', '中华网')
    yield loader.load_item()
```

我们需要将这些配置也抽离出来，可以看到这里的变量主要有 Item Loader 类的选用、Item 类的选用、Item Loader 方法参数的定义，所以在这里我们可以在 Json 文件中添加如下 item 的配置：

```json
"item": {
  "class": "NewsItem",
  "loader": "ChinaLoader",
  "attrs": {
    "title": [
      {
        "method": "xpath",
        "args": [
          "//h1[@id='chan_newsTitle']/text()"
        ]
      }
    ],
    "url": [
      {
        "method": "attr",
        "args": [
          "url"
        ]
      }
    ],
    "text": [
      {
        "method": "xpath",
        "args": [
          "//div[@id='chan_newsDetail']//text()"
        ]
      }
    ],
    "datetime": [
      {
        "method": "xpath",
        "args": [
          "//div[@id='chan_newsInfo']/text()"
        ],
        "re": "(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)"
      }
    ],
    "source": [
      {
        "method": "xpath",
        "args": [
          "//div[@id='chan_newsInfo']/text()"
        ],
        "re": "来源：(.*)"
      }
    ],
    "website": [
      {
        "method": "value",
        "args": [
          "中华网"
        ]
      }
    ]
  }
}
```

在这里定义了 class 和 loader 属性分别代表 Item 和 Item Loader 所使用的类，然后定义了 attrs 属性来定义每个字段的提取规则，例如这里 title 的定义的每一项都包含一个 method 属性，它代表使用的提取方法，比如这里定义为 xpath 即代表调用 Item Loader 的 add_xpath() 方法，args 即参数，就是 add_xpath() 的第二个参数，即 XPath 表达式，另外对于 datetime 字段我们最后还用了一次正则提取，所以这里还可以定义一个 re 参数来传递提取时所使用的正则表达式。

有了这些配置之后我们还需要将它们动态加载到 parse_item() 方法里，最后最重要的就是实现 parse_item() 方法，实现如下：

```python
 def parse_item(self, response):
    item = self.config.get('item')
    if item:
        cls = eval(item.get('class'))()
        loader = eval(item.get('loader'))(cls, response=response)
        # 动态获取属性配置
        for key, value in item.get('attrs').items():
            for extractor in value:
                if extractor.get('method') == 'xpath':
                    loader.add_xpath(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'css':
                    loader.add_css(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'value':
                    loader.add_value(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'attr':
                    loader.add_value(key, getattr(response, *extractor.get('args')))
        yield loader.load_item()
```

在这里首先获取了 Item 的配置信息，然后获取了 class 的配置，将其初始化，然后又初始化了 Item Loader，随后遍历了 Item 的各个属性依次进行提取，判断了 method 字段，然后调用对应的处理方法进行处理，如 method 为 css 的时候，就调用 Item Loader 的 add_css() 方法进行提取，最后所有配置动态加载完毕之后，调用 load_item() 方法将 Item 提取出来，达到同样的提取效果。

重新运行，结果如图 13-30 所示：

![](./assets/13-30.jpg)

图  13-30 运行结果

可以看到运行结果是完全相同的。

另外最后我们再回过头看一下 start_urls 的配置，在这里 start_urls 我们只可以配置具体的链接，但如果这些链接是 100 个，1000 个呢？我们总不能将所有的链接全部列出来吧，所以在某些情况下 start_urls 也需要动态配置，所以这里我们也需要将其分成两类，一种是直接配置 URL列表，一种是调用方法生成，我们分别定义为 static 和 dynamic 类型。

本例中的 start_urls 很明显是 static 类型的，所以将 start_urls 配置改写如下：

```json
"start_urls": {
  "type": "static",
  "value": [
    "http://tech.china.com/articles/"
  ]
}
```

如果是动态生成的我们可以调用方法传参数，例如：

```json
"start_urls": {
  "type": "dynamic",
  "method": "china",
  "args": [
    5, 10
  ]
}
```

例如这里我们定义为 dynamic 类型，指定方法为 urls_china()，然后传入参数 5 和 10，来生成第 5 到 10 页的链接，然后这样我们只需要实现该方法即可，可以统一新建一个 urls.py 文件，方法实现如下：

```python
def china(start, end):
    for page in range(start, end + 1):
        yield 'http://tech.china.com/articles/index_' + str(page) + '.html'
```

如有新增的可以自行配置，如某些链接需要用到时间戳，加密参数等等，均可通过方法实现。

接下来在 Spider 的 __init__() 方法中，start_urls 的配置可以改写如下：

```python
from scrapyuniversal import urls

start_urls = config.get('start_urls')
if start_urls:
    if start_urls.get('type') == 'static':
        self.start_urls = start_urls.get('value')
    elif start_urls.get('type') == 'dynamic':
        self.start_urls = list(eval('urls.' + start_urls.get('method'))(*start_urls.get('args', [])))
```

在这里通过判定 start_urls 的类型分别进行不同的处理，这样我们就可以实现 start_urls 的配置了。

至此，Spider 的设置、起始链接、属性、提取方法都已经实现了全部的可配置化。

综上，整个项目的配置总结如下：
* spider，指定所使用的 Spider 的名称。
* settings，可以专门为 Spider 定制配置信息，会覆盖项目级别的配置。
* start_urls，指定爬虫爬取的起始链接。
* allowed_domains，允许爬取的站点。
* rules，站点的爬取规则。
* item，数据的提取规则。

以上我们就实现了 Scrapy 的通用爬虫，每个站点只需要修改 Json 文件即可实现自由配置。

### 7. 本节代码

本节代码地址为：[https://github.com/Python3WebSpider/ScrapyUniversal](https://github.com/Python3WebSpider/ScrapyUniversal)。

### 8. 结语

本节介绍了一下 Scrapy 通用爬虫的实现，我们将所有的配置全部抽离了出来，每增加一个爬虫，就只需要增加一个 Json 文件配置即可，后面我们只需要维护这些配置文件即可，另外如果要更加方便的管理的话可以将规则再存入数据库，然后再对接可视化管理页面即可。

