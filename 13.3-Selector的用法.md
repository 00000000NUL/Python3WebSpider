# 13.3 Selector的用法

我们之前介绍了利用 BeautifulSoup、PyQuery 以及正则表达式来提取网页数据，确实已经非常方便了，但是在 Scrapy 中它还提供了自己的数据提取方法，即 Selector（选择器），它是基于 LXML来构建的，支持XPath 选择器、CSS 选择器以及正则表达式，功能全面，解析速度和准确度也是非常高的。

本节介绍一下 Scrapy Selector 的用法。

### 1. 直接使用

Scrapy Selector 其实是一个可以独立使用的模块，我们可以直接利用这个类来构建一个选择器对象，然后调用它的相关方法如 xpath()、css() 等方法来提取数据。

比如有一段 HTML 代码，我们可以用如下方式构建 Selector 对象来提取数据，我们用一个实例来感受一下：

```python
from scrapy import Selector

body = '<html><head><title>Hello World</title></head><body></body></html>'
selector = Selector(text=body)
title = selector.xpath('//title/text()').extract_first()
print(title)
```

运行结果：

```
Hello World
```

我们在这里没有在 Scrapy 框架中运行，而是把 Scrapy 中的 Selector 单独拿出来使用了，构建的时候传入 text 这个参数，就生成了一个 Selector 选择器对象，然后就可以像前面我们所用的 Scrapy 中的解析方式一样，调用 xpath()、css() 等方法来提取了。

在这里我们查找的是源代码中的 title 中的文本，通过在选择器最后加 text() 方法就可以实现文本的提取了。

以上就是 Selector 的直接使用方式，其实它也是同 BeautifulSoup 等库类似的强大的网页解析库，如果觉得方便的话，也可以在其他项目中直接使用 Selector 来提取数据。

接下来我们就用实例来详细讲解下 Selector 的用法。

### 2. Scrapy Shell

由于 Selector 主要是与 Scrapy 结合使用，例如 Scrapy 的回调函数中的参数 response 直接调用 xpath() 或者 css() 方法来提取数据，所以在这里我们借助于 Scrapy Shell 来模拟 Scrapy 请求的过程，拿到 response 对象，用它来讲解相关的提取方法。

在这里我们拿官方文档的一个样例页面来做演示：[http://doc.scrapy.org/en/latest/_static/selectors-sample1.html](http://doc.scrapy.org/en/latest/_static/selectors-sample1.html)。

首先我们需要开启 Scrapy Shell，在命令行下输入

```
scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html
```

这样我们就进入到了 Scrapy Shell 模式，这个过程其实是 Scrapy 为我们发起了一次请求，请求的 URL 就是刚才命令行下输入的 URL，然后把一些可操作的变量传递给我们，如 request、response 等等，如图 13-5 所示：

![](./assets/13-5.jpg)

图 13-5 Scrapy Shell

进入到 Shell 模式之后，我们可以在下方的输入命令调用对象的一些相关方法，回车之后会实时显示结果，这与 Python 的命令行交互模式是类似的。

页面的源码是这样的，接下来演示的实例都是以它为分析目标：

```html
<html>
 <head>
  <base href='http://example.com/' />
  <title>Example website</title>
 </head>
 <body>
  <div id='images'>
   <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' /></a>
   <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' /></a>
   <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' /></a>
   <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' /></a>
   <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' /></a>
  </div>
 </body>
</html>
```

### 3. XPath选择器

进入 Scrapy Shell 之后，我们将主要操作 response 这个变量来进行解析，而且因为我们解析的是 HTML 代码， Selector 将自动使用 HTML 语法来分析。

response 有一个属性 selector，我们调用 response.selector 返回的内容就相当于用 response 的 body 构造了一个 Selector 对象，通过它我们可以调用解析方法如 xpath()、css() 等等，通过传入 XPat h或 CSS 选择器就可以实现信息的提取。

我们用一个实例感受一下：

```python
>>> result = response.selector.xpath('//a')
>>> result
[<Selector xpath='//a' data='<a href="image1.html">Name: My image 1 <'>,
 <Selector xpath='//a' data='<a href="image2.html">Name: My image 2 <'>,
 <Selector xpath='//a' data='<a href="image3.html">Name: My image 3 <'>,
 <Selector xpath='//a' data='<a href="image4.html">Name: My image 4 <'>,
 <Selector xpath='//a' data='<a href="image5.html">Name: My image 5 <'>]
>>> type(result)
scrapy.selector.unified.SelectorList
```

可以看到打印结果的形式是 Selector 组成的列表，其实它是 SelectorList 类型，SelectorList 和 Selector 都可以继续调用 xpath() 和 css() 等方法来进一步提取数据。

上面的例子我们提取了 a 节点，接下来我们尝试继续调用 xpath() 方法来提取里面包含的 img 节点，可以这么写：

```python
>>> result.xpath('./img')
[<Selector xpath='./img' data='<img src="image1_thumb.jpg">'>,
 <Selector xpath='./img' data='<img src="image2_thumb.jpg">'>,
 <Selector xpath='./img' data='<img src="image3_thumb.jpg">'>,
 <Selector xpath='./img' data='<img src="image4_thumb.jpg">'>,
 <Selector xpath='./img' data='<img src="image5_thumb.jpg">'>]
```

这样我们就获得了 a 节点里面的所有 img 节点，结果正为 5 个。

值得注意的是，选择器的最前方加 . （点）才代表提取元素内部的数据，如果没有加点，则还是从根节点开始提取。比如此处我们用了 ./img 的提取方式，则代表从 a 节点里面进行提取，如果此处我们用 //img，则还是从 html 节点里面进行提取，所以这点非常值得注意。

我们刚才使用了 response.selector.xpath() 方法对数据进行了提取，其实为了方便，Scrapy提供了两个实用的快捷方法，response.xpath() 和 response.css()，它们二者的功能完全等同于 response.selector.xpath() 和response.selector.css()，所以后面我们为了方便，统一直接调用 response 的 xpath() 和 css() 方法进行选择。

现在我们得到的是 SelectorList 类型的变量，是由 Selector 对象组成的列表，我们可以用索引单独取出其中某一个 Selector 元素，例如：

```python
>>> result[0]
<Selector xpath='//a' data='<a href="image1.html">Name: My image 1 <'>
```

可以看到我们可以像操作列表一样操作这个 SelectorList。

但是现在获取的是 Selector 或者 SelectorList 类型，并不是真正的文本内容，其实并不是我们想要的内容，那么具体的内容怎么提取呢？

比如我们现在想提取出 a 节点元素，就可以利用 extract()方法，例如：

```python
>>> result.extract()
['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg"></a>', '<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg"></a>', '<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg"></a>', '<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg"></a>', '<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg"></a>']
```

可以看到用了 extract() 方法我们就可以把真实选取的内容获取下来了。

通过 XPath 的语法我们还可以知道，要选取节点的内部文本和属性我们可以改写一下 XPath 表达式：

```python
>>> response.xpath('//a/text()').extract()
['Name: My image 1 ', 'Name: My image 2 ', 'Name: My image 3 ', 'Name: My image 4 ', 'Name: My image 5 ']
>>> response.xpath('//a/@href').extract()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
```

在这里我们只需要再加一层 /text() 就可以获取节点的内部文本，或者加一层 /@href 就可以获取节点的 href 属性，其中 @ 符号后面就是要获取的属性名称。

好，那现在我们可以用一个规则把所有符合要求的节点都获取下来了，返回的类型是列表类型。

但是这里有一个问题，如果符合要求的节点只有一个，那么返回的结果会是什么呢？我们再用一个实例来感受一下：

```python
>>> response.xpath('//a[@href="image1.html"]/text()').extract()
['Name: My image 1 ']
```

在这里我们用属性限制了一下匹配的范围，使 XPath 只可以匹配到一个元素，然后我们再用 extract() 方法提取一下结果，可以看到其结果还是一个列表形式，其文本是列表的第一个元素，但很多情况下我们其实想要的数据就是第一个元素内容，这里我们可以加一个索引来获取一下，例如：

```python
>>> response.xpath('//a[@href="image1.html"]/text()').extract()[0]
'Name: My image 1 '
```

但这个写法很明显是有风险的，一旦 XPath 有问题，那么 extract() 后的结果可能是一个空列表，这里如果我们再用索引来取那不就会报越界错误了吗？

所以这里还有另外一个方法专门来提取单个元素，叫做 extract_first()，所以上面的例子我们可以改写如下：

```python
>>> response.xpath('//a[@href="image1.html"]/text()').extract_first()
'Name: My image 1 '
```

这样我们就可以直接利用这个方法将匹配的第一个结果提取出来了，非常简单易用。

另外我们也不用担心越界的问题，也可以设置默认值，例如将 XPath 改成一个不存在的规则，重新执行一下代码：

```python
>>> response.xpath('//a[@href="image1"]/text()').extract_first()
>>> response.xpath('//a[@href="image1"]/text()').extract_first('Default Image')
'Default Image'
```

这里我们可以看到如果 XPath 匹配不到任何元素，调用 extract_first() 会返回空，也不会报错。

另外该方法我们还可以传一个参数当做默认值，比如Default Image，这样如果匹配不到元素的话，返回值会使用这个参数来代替，可以看到输出正是如此。

好，那现在为止我们就了解了 Scrapy 中的 XPath 的相关用法，包括嵌套查询、提取内容、提取单个内容、获取文本和属性等等。

### 5. CSS选择器

下面我们再看下 CSS 选择器的用法。

Scrapy 的选择器同时还对接了 CSS 选择器，使用response.css() 可以利用 CSS 选择器来选择对应的元素。

例如在上文我们选取了所有的 a 节点，那么用 CSS 选择器同样可以做到，写法如下：

```python
>>> response.css('a')
[<Selector xpath='descendant-or-self::a' data='<a href="image1.html">Name: My image 1 <'>, 
<Selector xpath='descendant-or-self::a' data='<a href="image2.html">Name: My image 2 <'>, 
<Selector xpath='descendant-or-self::a' data='<a href="image3.html">Name: My image 3 <'>, 
<Selector xpath='descendant-or-self::a' data='<a href="image4.html">Name: My image 4 <'>, 
<Selector xpath='descendant-or-self::a' data='<a href="image5.html">Name: My image 5 <'>]
```

然后同样调用 extract() 方法就可以提取出节点，写法如下：

```python
>>> response.css('a').extract()
['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg"></a>', '<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg"></a>', '<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg"></a>', '<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg"></a>', '<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg"></a>']
```

原理和 XPath 选择是完全一样的。

另外我们也可以进行属性选择和嵌套选择，例如：

```python
>>> response.css('a[href="image1.html"]').extract()
['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg"></a>']
>>> response.css('a[href="image1.html"] img').extract()
['<img src="image1_thumb.jpg">']
```

比如在这里用 [href="image.html"] 就限定了 href 属性，可以看到结果就只有一个匹配结果了，另外如果想查找 a 节点内的 img 节点，只需要再加一个空格，再加 img 即可，写法和 CSS 选择器如出一辙。

另外我们也可以使用 extract_first() 方法提取列表的第一个元素：

```python
>>> response.css('a[href="image1.html"] img').extract_first()
'<img src="image1_thumb.jpg">'
```

但是接下来的两个用法不太一样，当获取节点的内部文本和属性的时候是这样实现的：

```python
>>> response.css('a[href="image1.html"]::text').extract_first()
'Name: My image 1 '
>>> response.css('a[href="image1.html"] img::attr(src)').extract_first()
'image1_thumb.jpg'
```

在这里获取文本和属性需要用 ::text 和 ::attr() 这样的写法，而在其他的库如 Beautiful Soup 或 PyQuery 中都是有单独的方法的，所以这两种写法值得注意一下。

另外 CSS 选择器和 XPath 选择器一样可以嵌套选择，例如我们可以先用XPath选择器选中所有 a 节点，再利用 CSS 选择器选中 img 节点，再用 XPath 选择器获取属性，我们用一个实例来感受一下：

```python
>>> response.xpath('//a').css('img').xpath('@src').extract()
['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']
```

可以看到我们成功获取了所有 img 节点的 src 属性。

因此在选择过程中我们可以随意使用 xpath() 和 css() 方法进行组合实现嵌套查询，二者是完全兼容的。

### 6. 正则匹配

另外 Scrapy 的选择器还支持正则匹配，比如在示例的 a 节点中的文本类似于 Name: My image 1，现在我们只想把Name: 后面的内容提取出来，这时就可以借助于 re() 方法，实现如下：

```python
>>> response.xpath('//a/text()').re('Name:\s(.*)')
['My image 1 ', 'My image 2 ', 'My image 3 ', 'My image 4 ', 'My image 5 ']
```

在这里我们给 re() 方法传了一个正则表达式，其中 (.*) 就是要匹配的内容，输出的结果就是正则表达式匹配的分组，会依次输出。

如果同时存在两个分组，那么依然会按序输出：

```python
>>> response.xpath('//a/text()').re('(.*?):\s(.*)')
['Name', 'My image 1 ', 'Name', 'My image 2 ', 'Name', 'My image 3 ', 'Name', 'My image 4 ', 'Name', 'My image 5 ']
```

另外类似 extract_first() 方法，也有一个 re_first() 方法来选取列表的第一个元素，用法如下：

```python
>>> response.xpath('//a/text()').re_first('(.*?):\s(.*)')
'Name'
>>> response.xpath('//a/text()').re_first('Name:\s(.*)')
'My image 1 '
```

可以看到不论正则匹配了几个分组，都会只输出列表的第一个元素。

另外值得注意的地方是，response 对象是不能直接调用 re() 和 re_first() 方法的，如果想要对全文进行正则匹配可以先调用一次 xpath() 方法再正则匹配：

```python
>>> response.re('Name:\s(.*)')
Traceback (most recent call last):
  File "<console>", line 1, in <module>
AttributeError: 'HtmlResponse' object has no attribute 're'
>>> response.xpath('.').re('Name:\s(.*)<br>')
['My image 1 ', 'My image 2 ', 'My image 3 ', 'My image 4 ', 'My image 5 ']
>>> response.xpath('.').re_first('Name:\s(.*)<br>')
'My image 1 '
```

通过上面的例子我们可以看到直接调用 re() 方法会提示没有该属性，但是这里首先调用了一下 xpath('.') 选中全文，然后再调用 re() 和 re_first() 方法就可以匹配了。

### 7. 结语

以上便是 Scrapy 选择器的用法，它包括两个常用选择器和正则匹配功能，熟练掌握 XPath 语法、CSS 选择器语法、正则表达式语法，在解析的时候灵活搭配使用可以大大提高数据提取效率。


