# Scrapyd分布式部署

在上一节我们已经完成了分布式爬虫的部署并可以成功运行了，但是有个环节非常繁琐，那就是代码部署。

我们设想下面的几个场景：
* 如果采用上传文件的方式部署代码，那么得首先将代码压缩，然后采用SFTP协议或FTP的方式将文件上传到服务器，上传之后再连接服务器将文件解压，每个服务器都需要这样配置。
* 如果采用Git同步的方式部署代码，我们可以先把代码Push到某个Git仓库里，然后再远程连接上各台主机然后执行Pull操作，同步代码，每个服务器同样需要做一次操作。

假设现在代码突然有更新，那我们还是不得不挨个服务器更新一遍，而且如果万一有哪台主机的版本没控制好，可能会影响整体的分布式爬取状况。

所以在这里我们需要一个更方便的工具来部署Scrapy项目，如果可以省去一遍遍逐个登录服务器部署的操作，那将会方便太多。

在本节我们就来看下一个提供分布式部署的工具，Scrapyd。

## Scrapyd是什么

Scrapyd是一个运行Scrapy爬虫的服务程序，它提供一系列HTTP接口来帮助我们部署、启动、停止、删除爬虫程序，而且Scrapyd还支持版本管理，同时还可以管理多个爬虫任务，利用它我们可以非常方便地完成Scrapy爬虫项目的部署任务调度。

那么接下来我们就利用它来实现爬虫的部署，来感受一下它的强大和方便之处。

那么在这里首先需要在服务器上安装好Scrapyd，安装和配置的方法可以参见第一章的内容。

安装并运行了Scrapyd之后，我们就可以访问服务器的6800端口看到一个WebUI页面了，例如我的服务器地址为120.27.34.25，在上面安装好了Scrapyd并成功运行，那么我就可以在本地的浏览器中打开[http://120.27.34.25:6800](http://120.27.34.25:6800)，就可以看到Scrapyd的首页。

![](./assets/2017-08-02-20-43-35.png)

如果可以成功访问到此页面，那么证明Scrapyd配置就没有问题了。

## Scrapyd的功能

Scrapyd提供了一系列HTTP接口来实现各种操作，在这里我们可以将接口的功能梳理一下，Scrapyd所在的IP以120.27.34.25为例：

### daemonstatus.json

这个接口负责查看Scrapyd当前的服务和任务状态，我们可以用curl命令来请求这个接口，命令如下：

```
curl http://139.217.26.30:6800/daemonstatus.json
```

这样我们就会得到如下结果：

```json
{"status": "ok", "finished": 90, "running": 9, "node_name": "datacrawl-vm", "pending": 0}
```

返回结果是Json字符串，status是当前运行状态，finished代表当前已经完成的Scrapy任务，running代表正在运行的Scrapy任务，pending代表等待被调度的Scrapyd任务，node_name就是主机的名称。

### addversion.json

这个接口主要是用来部署Scrapy项目用的，在部署的时候我们需要首先将项目打包成Egg文件，然后传入项目名称和部署版本。

我们可以用如下的方式实现项目部署：

```
curl http://120.27.34.25:6800/addversion.json -F project=wenbo -F version=first -F egg=@weibo.egg
```

在这里-F即代表添加一个参数，同时我们还需要将项目打包成Egg文件放到本地。

这样发出请求之后我们可以得到如下结果：

```json
{"status": "ok", "spiders": 3}
```

这个结果表明部署成功，并且其中包含的Spider的数量为3。

此方法部署可能比较繁琐，在后文会介绍更方便的工具来实现项目的部署。

### schedule.json

这个接口负责调度已部署好的Scrapy项目运行。

我们可以用如下接口实现任务调度：

```
curl http://120.27.34.25:6800/schedule.json -d project=weibo -d spider=weibocn
```

在这里需要传入两个参数，project即Scrapy项目名称，spider即Spider名称。

返回结果如下：

```json
{"status": "ok", "jobid": "6487ec79947edab326d6db28a2d86511e8247444"}
```

status代表Scrapy项目启动情况，jobid代表当前正在运行的爬取任务代号。

### cancel.json

这个接口可以用来取消某个爬取任务，如果这个任务是pending状态，那么它将会被移除，如果这个任务是running状态，那么它将会被终止。

我们可以用下面的命令来取消任务的运行：

```
curl http://120.27.34.25:6800/cancel.json -d project=weibo -d job=6487ec79947edab326d6db28a2d86511e8247444
```

在这里需要传入两个参数，project即项目名称，job即爬取任务代号。

返回结果如下：

```json
{"status": "ok", "prevstate": "running"}
```

status代表请求执行情况，prevstate代表之前的运行状态。

### listprojects.json

这个接口用来列出部署到Scrapyd服务上的所有项目描述。

我们可以用下面的命令来获取Scrapyd服务器上的所有项目描述：

```
curl http://120.27.34.25:6800/listprojects.json
```

这里不需要传入任何参数。

返回结果如下：

```json
{"status": "ok", "projects": ["weibo", "zhihu"]}
```

status代表请求执行情况，projects是项目名称列表。

### listversions.json

这个接口用来获取某个项目的所有版本号，版本号是按序排列的，最后一个条目是最新的版本号。

我们可以用如下命令来获取项目的版本号：

```
curl http://120.27.34.25:6800/listversions.json?project=weibo
```

在这里需要一个参数project，就是项目的名称。

返回结果如下：

```json
{"status": "ok", "versions": ["v1", "v2"]}
```

status代表请求执行情况，versions是版本号列表。

### listspiders.json

这个接口用来获取某个项目最新的一个版本的所有Spider名称。

我们可以用如下命令来获取项目的Spider名称：

```
curl http://120.27.34.25:6800/listspiders.json?project=weibo
```

在这里需要一个参数project，就是项目的名称。

返回结果如下：

```json
{"status": "ok", "spiders": ["weibocn"]}
```

status代表请求执行情况，spiders是Spider名称列表。


### listjobs.json

这个接口用来获取某个项目当前运行的所有任务详情。

我们可以用如下命令来获取所有任务详情：

```
curl http://120.27.34.25:6800/listjobs.json?project=weibo
```

在这里需要一个参数project，就是项目的名称。

返回结果如下：

```json
{"status": "ok",
 "pending": [{"id": "78391cc0fcaf11e1b0090800272a6d06", "spider": "weibocn"}],
 "running": [{"id": "422e608f9f28cef127b3d5ef93fe9399", "spider": "weibocn", "start_time": "2017-07-12 10:14:03.594664"}],
 "finished": [{"id": "2f16646cfcaf11e1b0090800272a6d06", "spider": "weibocn", "start_time": "2017-07-12 10:14:03.594664", "end_time": "2017-07-12 10:24:03.594664"}]}
```

status代表请求执行情况，pendings代表当前正在等待的任务，running代表当前正在运行的任务，finished代表已经完成的任务。

### delversion.json

这个接口用来删除项目的某个版本。

我们可以用如下命令来删除项目版本：

```
curl http://120.27.34.25:6800/delversion.json -d project=weibo -d version=v1
```

在这里需要一个参数project，就是项目的名称，还需要一个参数version，就是项目的版本。

返回结果如下：

```json
{"status": "ok"}
```

status代表请求执行情况，这样就代表删除成功了。

### delproject.json

这个接口用来删除某个项目。

我们可以用如下命令来删除某个项目：

```
curl http://120.27.34.25:6800/delproject.json -d project=weibo
```

在这里需要一个参数project，就是项目的名称。

返回结果如下：

```json
{"status": "ok"}
```

status代表请求执行情况，这样就代表删除成功了。

以上就是Scrapyd所有的接口，我们可以直接请求HTTP接口即可控制项目的部署、启动、运行等操作。

## ScrapydAPI的使用

以上的这些接口可能使用起来还不是很方便，没关系，还有一个ScrapydAPI库对这些接口又做了一层封装，其安装方式也可以参考第一章的内容。

下面我们来看下ScrapydAPI的使用方法，其实核心原理和HTTP接口请求方式并无二致，只不过用Python封装后使用更加便捷。

我们可以用如下方式建立一个ScrapydAPI对象：

```python
from scrapyd_api import ScrapydAPI
scrapyd = ScrapydAPI('http://120.27.34.25:6800')
```

然后就可以调用它的方法来实现对应接口的操作了，例如部署的操作可以使用如下方式：

```python
egg = open('weibo.egg', 'rb')
scrapyd.add_version('weibo', 'v1', egg)
```

这样我们就可以将项目打包为Egg文件，然后把本地打包的的Egg项目部署到远程Scrapyd了。

另外ScrapydAPI还实现了所有Scrapyd提供的API接口，名称都是相同的，参数也是相同的。

例如我们可以调用list_projects()方法即可列出Scrapyd中所有已部署的项目：

```python
scrapyd.list_projects()
['weibo', 'zhihu']
```

另外还有其他的方法在此不再一一列举了，名称和参数都是相同的，更加详细的操作可以参考其官方文档：[http://python-scrapyd-api.readthedocs.io/](http://python-scrapyd-api.readthedocs.io/)

