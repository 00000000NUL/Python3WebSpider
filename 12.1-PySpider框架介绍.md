#12.1 PySpider框架介绍

PySpider 是国人 binux 编写的强大的网络爬虫系统，其 GitHub 地址为：[https://github.com/binux/pyspider](https://github.com/binux/pyspider)，官方文档地址为：[http://docs.pyspider.org/](http://docs.pyspider.org/)。

它带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，同时它支持多种数据库后端、多种消息队列，另外它还支持 JavaScript 渲染页面的爬取，使用起来非常方便。

本章我们来了解一下它的相关用法。

### 1. PySpider基本功能

那么它具体有什么功能呢？我们在这里总结如下：
* 提供方便易用的 WebUI 系统，可以可视化地编写和调试爬虫。
* 提供爬取进度监控、爬取结果查看、爬虫项目管理等功能。
* 支持多种后端数据库，如 MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL。
* 支持多种消息队列，如 RabbitMQ、Beanstalk、Redis、Kombu。
* 提供优先级控制、失败重试、定时抓取等功能。
* 对接了 PhantomJS，可以抓取 JavaScript 渲染的页面。
* 支持单机和分布式部署，支持 Docker 部署。

如果我们想要快速方便地实现一个页面的抓取的话，使用 PySpider 不失为一个好的选择。

### 2. 与Scrapy的比较

在后文我们还会介绍另外一个爬虫框架 Scrapy，在学习完 Scrapy 之后可以再回来了解此部分内容会更加容易理解，下面大致列一下 PySpider 与 Scrapy 相比有什么不同：
* PySpider 提供了 WebUI，爬虫的编写、调试都是在 WebUI 中进行的，而 Scrapy 原生是不具备这个功能的，采用的是代码和命令行操作，但可以通过对接 Portia 实现可视化配置。
* PySpider 调试非常方便，WebUI 操作便捷直观，在Scrapy 中则是使用 parse 命令进行调试，论方便程度不及 PySpider。
* PySpider 支持 PhantomJS 来进行 JavaScript 渲染页面的采集，在 Scrapy 中可以对接 ScrapySplash 组件，需要额外配置。
* PySpide r中内置了 PyQuery 作为选择器，在 Scrapy 中对接了 XPath、CSS 选择器和正则匹配。
* PySpider 的可扩展程度不足，可配制化程度不高，在 Scrapy 中可以通过对接 Middleware、Pipeline、Extension 等组件实现非常强大的功能，模块之间的耦合程度低，可扩展程度极高。

因此如果我们想要快速实现一个页面的抓取的话，推荐使用 PySpider，开发更加便捷，如快速抓取某个普通新闻网站的新闻内容。但如果要应对反爬程度很强、超大规模的抓取的话，推荐使用 Scrapy，如抓取封 IP、封账号、高频验证的网站的大规模数据采集。

### 3. PySpider的架构

首先我们来看下 PySpider 的架构，PySpider 的架构主要分为 Scheduler（调度器）、Fetcher（抓取器）、Processer（处理器）三个部分，同时整个过程受到Monitor（监控器）的监控，抓取的结果被 Result Worker（结果处理器）处理，如图所示：

![](./assets/2017-08-13-15-52-37.jpg)

任务是 Scheduler 发起调度，Fetcher 负责抓取网页内容，Processer 负责解析网页内容，然后将新生成的Request 在此发给 Scheduler 进行调度，将生成的提取结果输出保存。

数据流过程如下：
* 每一个 PySpider 的项目对应一个 Python 脚本，是一个 Handler 类，它有一个 on_start() 方法，在爬取开始时会首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler 进行调度。
* Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行并得到 Response，随后将 Response 发送给Processer。
* Processer 处理 Response 并提取出新的 URL 生成新的抓取任务，然后通过消息队列的方式通知 Schduler 当前抓取任务执行情况并将新生成的抓取任务发送给 Scheduler，同时如果生成了新的提取结果则将其发送到结果队列等待 Result Worker 处理。
* Scheduler 接收到了新的抓取任务，然后查询数据库，判断这个如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其再发送回 Fetcher 进行抓取。
* 不断重复以上工作，直到所有的任务都被执行完毕，抓取结束。
* 抓取结束后会回调 on_finished() 方法，在这里可以定义后处理过程。

以上便是 PySpider 的任务执行流程，整体的逻辑还是比较清晰的。

### 4. 结语

本节我们主要了解了一下 PySpider 的基本功能和架构，在后面我们会先用一个实例来体验一下 PySpider 的抓取操作，然后再总结一下它的各种用法。
