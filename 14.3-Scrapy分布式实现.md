# Scrapy分布式实现

要实现Scrapy分布式其实配置非常简单，我们首先需要确保ScrapyRedis这个库已经正确安装，如果还没安装，请参考第一章中的安装说明，安装完成之后，我们就可以进行分布式的配置了。

## 搭建Redis服务器

由于我们要实现分布式部署，所以多台主机需要共享爬取队列和去重集合，而这两部分内容都是存在Redis数据库中的，所以要使得多台主机都可以访问到，我们需要搭建一个可公网访问的Redis服务器。

推荐使用Linux服务器搭建，该Linux服务器需要可以公网访问，比如可以购买阿里云、腾讯云、Azure等提供的云主机，一般都会配有公网IP，具体的搭建方式可以参考第一章中Redis数据库的安装方式。

安装完成之后应该就可以远程连接Redis了，注意部分商家的服务器需要配置一下安全组放通Redis运行端口才可以远程访问，如果遇到不能远程连接的问题可以排查下安全组的设置。

如果Redis可以远程访问了，那么在这里需要记录一下Redis的运行IP、端口、地址，供后面配置分布式爬虫使用。

如我当前配置好的Redis的IP为服务器的IP，120.27.34.25，端口为默认的6379，密码为foobared。

## 部署代理池和Cookies池

因为当前的新浪微博项目需要用到代理池和Cookies池，而之前我们的代理池和Cookies池都是在本地运行的，所以我们还需要将二者放到可以被公网访问的服务器上运行，将代码上传到服务器上即可，然后将Redis的连接信息配置修改一下，然后用同样的方式运行代理池和Cookies池。

运行之后我们就可以远程访问代理池和Cookies池提供的接口来获取随机代理和Cookies了，如果不能远程访问请检查一下部署有没有错误，确保其在0.0.0.0这个Host上运行，如果没有错误，再检查一下安全组的配置。

如我当前配置好的代理池和Cookies池的运行IP都是服务器的IP，120.27.34.25，端口分别为5555和5556，这样我们就可以通过远程接口来获取随机代理和Cookies了，如图所示：

![](./assets/2017-08-02-15-25-45.jpg)

![](./assets/2017-08-02-15-27-36.jpg)

所以接下来我们就需要把Scrapy新浪微博项目中的访问链接修改如下：

```python
PROXY_URL = 'http://120.27.34.25:5555/random'
COOKIES_URL = 'http://120.27.34.25:5556/weibo/random'
```

具体的修改方式根据你实际配置的IP和端口做相应调整。

## 配置ScrapyRedis

配置ScrapyRedis非常简单，只需要修改一下settings.py配置文件即可。

### 核心配置

首先最主要的，需要将调度器的类和去重的类替换，更改为ScrapyRedis提供的类，在settings.py里面添加如下配置即可：

```python
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
```

### Redis连接配置

接下来我们还需要配置Redis的连接信息，只有配置了这个，爬虫才能远程连接Redis，才能使用Redis来维护共享的爬取队列和去重队列，在这里有两种配置方式。

#### 连接字符串配置

我们可以用Redis的地址、端口、密码来构造一个Redis连接字符串，支持的连接形式是：

```python
redis://[:password]@host:port/db
rediss://[:password]@host:port/db
unix://[:password]@/path/to/socket.sock?db=db
```

password就是密码，比如要以冒号开头，外面的中括号代表此选项可有可无，host代表Redis的地址，port是运行端口，db是数据库代号，默认是0。

所以根据上文中提到我的Redis连接信息，构造的这个Redis连接字符串就是：

```python
redis://:foobared@120.27.34.25:6379
```

那么我们直接在settings.py里面配置为REDIS_URL这个变量即可：

```python
REDIS_URL = 'redis://:foobared@120.27.34.25:6379'
```

#### 分项单独配置

如果不想用连接字符串的配置方法，也可以使用下面的分项配置，这个配置就更加直观明了，如根据我的Redis连接信息，可以在settings.py中配置如下：

```python
REDIS_HOST = '120.27.34.25'
REDIS_PORT = 6379
REDIS_PASSWORD = 'foobared'
```

即分开配置了Redis的地址、端口和密码。

值得注意的是，如果配置了REDIS_URL，那么ScrapyRedis将优先使用REDIS_URL连接，会覆盖上面的三项配置，如果想要分项单独配置的话，请不要配置REDIS_URL。

在本项目中我选择的是配置REDIS_URL。

### 配置调度队列

此项配置是可选的，默认会使用PriorityQueue。如果想要更改配置，可以配置一下SCHEDULER_QUEUE_CLASS变量，配置如下：

```python
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'
```

以上三行任选其一配置即可，配置完了之后即切换了爬取队列的存储方式。

在本项目中不进行任何配置，使用默认配置即可。

### 配置持久化

此配置是可选的，默认False。ScrapyRedis默认会在爬取全部完成后清空爬取队列和去重指纹集合。

如果不想在爬取完成之后自动清空爬取队列和指纹集合，可以增加如下配置：

```python
SCHEDULER_PERSIST = True
```

将SCHEDULER_PERSIST设置为True之后，爬取队列和去重指纹集合不会在爬取完成后自动清空，如果不配置，默认是False，即自动清空。

值得注意的是，如果强制中断爬虫是不会自动清空的。

在本项目中不进行任何配置，使用默认配置即可。

### 配置重爬

此配置是可选的，默认False。如果配置了持久化或者强制中断了爬虫，那么爬取队列和指纹集合是不会被清空的，重新启动之后就会接着上次爬取，所以此时如果想重新爬取的话，我们可以配置重爬的选项，可以增加如下配置：

```python
SCHEDULER_FLUSH_ON_START = True
```

这样将SCHEDULER_FLUSH_ON_START设置为True之后，每次启动爬虫时都会清空爬取队列和指纹集合，所以要做分布式爬取，我们必须保证只能清空一次，否则每个爬虫任务在启动时都清空一次，就会把之前的爬取队列清空，势必会影响分布式爬取。

所以此配置在单机爬取的时候比较方便，分布式爬取不常用此配置。

在本项目中不进行任何配置，使用默认配置即可。

### Pipeline配置

此配置是可选的，默认不启动此Pipeline。ScrapyRedis实现了一个存储到Redis的Item Pipeline，启用之后会把生成的Item存储到Redis数据库中，在数据量比较大的情况下我们一般不会这么做，因为Redis是基于内存的，我们利用的是它处理速度快的特性，用它来做存储未免太浪费了，当然如果数据量小的话倒无所谓，配置如下：

```python
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300
}
```

在本项目中不进行任何配置，即不启动此Pipeline。

到此位置，ScrapyRedis的配置就完成了，有的选项我们没有配置，但是这些配置可能在其他Scrapy项目中可能用到，视具体情况而定。

## 配置存储目标

我们之前的Scrapy新浪微博爬虫项目中使用的存储是MongoDB，而且此MongoDB是本地运行的，如果我们仍然使用此配置也不是不可以，这样将爬虫程序分发到各台主机运行的时候它就会连接各台主机各自的MongoDB了，所以我们需要在各台主机上都安装MongoDB，这样一是搭建MongoDB环境比较繁琐，二是这样各台主机的爬虫会把爬取结果分散存到各台主机上，不方便统一管理。

所以这里我们最好可以将存储目标存到同一个地方，例如都存到同一个MongoDB数据库中。

所以在这里我们可以在服务器上搭建一个MongoDB服务，或者直接购买MongoDB数据存储服务。

那么我在这里使用的就是服务器自己搭建的的MongoDB服务，IP仍然为120.27.34.25，用户名为admin，密码为admin123。

那么在这里就配置MONGO_URI修改为如下配置：

```python
MONGO_URI = 'mongodb://admin:admin123@120.27.34.25:27017'
```

到此为止，我们就成功完成了Scrapy分布式爬虫的配置了。

## 运行

接下来我们可以将代码部署到各台主机上，记得每台主机都需要配好对应的Python环境。

接下来在每台主机上都执行如下命令即可启动爬取：

```
scrapy crawl weibocn
```

每台主机分别启动了此命令之后，就都会将配置的Redis数据库中存取Request，做到爬取队列共享和指纹集合共享，同时每台主机各自占用各自的带宽和处理器，不会互相影响，爬取效率成倍提高。

## 结果

爬取一段时间后，我们可以用RedisDesktop观察一下远程Redis数据库的信息，可以看到这里即会出现两个Key，一个叫做weibocn:dupefilter，用来储存指纹，另一个叫做weibocn:requests，即爬取队列，如图所示：


![](./assets/2017-08-02-17-31-39.jpg)

![](./assets/2017-08-02-17-24-44.jpg)

随着时间的推移，指纹集合的大小会不断增长，爬取队列会动态变化，爬取的数据也会被储存到MongoDB数据库中。

至此Scrapy分布式的配置已全部完成，本节代码为：
