# Spider详解

## Spider运行流程

在实现Scrapy爬虫项目时，最核心的类便是Spider类了，它定义了如何爬取某个网站的流程和解析方式。简单来讲，Spider要做的事就是两类：

* 定义爬取网站的动作
* 分析爬取下来的网页

对于Spider类来说，整个爬取循环如下所述：

* 以初始的URL初始化Request，并设置回调函数。 当该Request成功请求并返回时，将生成Response，并作为参数传给该回调函数。
* 在回调函数内分析返回的网页内容。返回结果可以有两种形式，一种是解析到的有效结果返回字典或Item 对象。下一步可经过处理后(或直接)保存，另一种是解析得下一个(如下一页)链接，可以利用此链接构造Request并设置新的回调函数，返回Request。
* 如果返回的是字典或Item对象，可通过Feed Exports等形式存入到文件，如果设置了Pipeline的话，可以经由Pipeline处理（如过滤、修正等）并保存。
* 如果返回的是Reqeust，那么Request中设置的回调函数会得到响应结果Response，可以再次使用选择器(Selectors) 来分析新得到的网页内容，并根据分析的数据生成item。

通过以上几步循环往复进行，便完成了站点的爬取。

## Spider类分析

在上一节的例子中我们定义的Spider是继承自`scrapy.spiders.Spider`，这个类是最简单最基本的Spider类，每个其他的Spider必须继承这个类，还有后文要说明的一些特殊Spider类也都是继承自它。

这个类里提供了start_requests方法的默认实现，读取并请求start_urls属性, 并根据返回的结果调用parse方法解析结果。另外它还有一些基础属性，下面对其进行讲解：

* name，爬虫名称，是定义spider名字的字符串。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。 不过您可以生成多个相同的spider实例(instance)，这没有任何限制。 name是spider最重要的属性，而且是必须的。如果该spider爬取单个网站(single domain)，一个常见的做法是以该网站(domain)(加或不加 后缀 )来命名spider。 例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite 。
* allowed_domains，允许爬取的域名，是可选配置，不在此范围的链接不会被跟进爬取。
* start_urls，起始URL列表，当我们没有实现start_requests()方法时，默认会从这个列表开始抓取。
* custom_settings，这是一个字典，是专属于本Spider的配置，此设置会覆盖项目全局的设置，而且此设置必须在初始化前被更新，所以它必须定义成类变量。
* crawler，此属性是由from_crawler()方法设置的，代表的是本Spider类对应的Crawler对象，Crawler对象中包含了很多项目组件，利用它我们可以获取项目的一些配置信息，如最常见的就是获取项目的设置信息，即Settings。
* settings，是一个Settings对象，利用它我们可以直接获取项目的全局设置变量。

除了一些基础属性，Spider还有一些常用的方法，在此介绍如下：

* start_requests()，此方法用于生成初始请求，它必须返回一个可迭代对象，此方法会默认使用start_urls里面的URL来构造Request，而且Request是GET请求方式。如果我们想在启动时以POST方式访问某个站点，可以直接重写这个方法，发送POST请求时我们使用FormRequest即可。
* parse()，当Response没有指定回调函数时，该方法会默认被调用，它负责处理Response，处理返回结果，并从中提取出想要的数据和下一步的请求，然后返回。该方法需要返回一个包含Request或Item的可迭代对象。
* closed()，当Spider关闭时，该方法会被调用，在这里一般会定义释放资源的一些操作或其他收尾操作。

以上的属性和方法可能初看起来有点摸不清头脑，不用担心，后面我们会有很多实例来使用这些属性和方法，慢慢会熟练掌握的。
