# 爬虫基本原理

爬虫，即网络爬虫，我们可以把互联网就比作一张大网，而爬虫便是在网上爬行的蜘蛛，我们可以把网的节点比做一个个网页，爬虫爬到这就相当于访问了该页面获取了其信息，节点间的连线可以比做网页与网页之间的链接关系，这样蜘蛛通过一个节点后可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，这样网站的数据就可以被抓取下来了。

## 爬虫要点

可能上面的说明还是难以具体地描述爬虫究竟是个什么，简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。

### 获取网页

爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。

在前面我们讲到了Request和Response的概念，我们向网站的服务器发送一个Request，返回的Response的Body便是网页源代码。所以最关键的部分就是构造一个Request并发送给服务器，然后接收到Response并将其解析出来，那这个流程可以怎样来实现呢？总不能手工去截取网页源码把？

不用担心，Python里面提供了许多库来帮助我们实现这个操作，如Urllib、Requests等，我们可以用这些库来帮助我们实现HTTP请求操作，Request和Response都可以用类库提供的数据结构来表示，得到Response之后只需要解析数据结构中的Body部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。

### 提取信息

我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如BeautifulSoup、PyQuery、lxml等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。

### 保存数据

提取信息之后我们一般会将提取到的数据保存到某处以便后续数据处理使用。保存形式有多种多样，如可以简单保存为TXT文本或Json文本，也可以保存址数据库，如MySQL、MongoDB等，也可保存至远程服务器，如借助Sftp进行操作等。

提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。

### 自动化程序

说到自动化程序，意思即是说爬虫可以代替人来完成这些操作。首先我们手工当然是可以提取这些信息的，但是当量特别大或者想快速获取大量数据的话，肯定还是借助于程序。所以爬虫就是代替我们来完成这份爬取数据的工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。

## 能抓怎样的数据

在网页中我们能看到各种各样的信息，最常见的便是常规网页，其都对应着HTML代码，而最常见的抓取便是抓取HTML源代码。另外可能有些网页返回的不是HTML代码，而是返回一个Json字符串，网页接口大多采用这样的形式，方便数据的传输和解析，这种同样可以抓取，而且数据提取更加方便。此外我们还可以看到各种二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。另外我们还可以看到各种扩展名的文件，如CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。

## 抓取的网页源码和浏览器的不一样

这个问题是一个非常常见的问题，现在网页越来越多地采用Ajax、前端模块化工具来构建网页，整个网页可能都是由JavaScript渲染出来的，意思就是说原始的HTML代码就是一个空壳，例如：

```html
<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>This is a Demo</title>
    </head>
    <body>
        <div id="container">
        </div>
    </body>
    <script src="app.js"></script>
</html>
```

`<body>`里面只有一个id为container的节点，但是注意到在`</body>`后引入了一个app.js，这个便负责了整个网站的渲染。

实际上在浏览器中我们看到的网页都是经过JavaScript渲染而成的，也就是说我们在浏览器开发者模式看到的网页源码已经不是最原始的HTML代码了，是执行完引入的JavaScript后所得到的结果。

然而对于程序来说，使用Urllib、Requests等库直接请求某个链接，得到的其实是最原始的HTML代码，而不是执行完JavaScript之后的源代码，这点一定要注意。

所以使用基本HTTP请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染，这样我们便可以爬取JavaScript渲染的网页的内容了。

在后文我们会详细介绍对于JavaScript渲染的网页的采集方法。





