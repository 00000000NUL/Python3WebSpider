# Scrapyd的安装

Scrapyd是一个用于部署和运行Scrapy项目的工具。有了它，你可以将写好的Scrapy项目上传到云主机并通过API来控制它的运行。

既然是Scrapy项目部署，所以基本上都使用Linux主机，所以本节的安装是针对于Linux主机的。

## 安装

利用Pip安装Scrapyd即可，命令如下：

```
pip3 install scrapyd
```

## 配置

安装完毕之后需要新建一个配置文件/etc/scrapyd/scrapyd.conf，Scrapyd在运行的时候会读取此配置文件。

在Scrapyd 1.2版本之后，不会自动创建该文件，需要我们自行添加。

执行命令新建文件：

```
sudo mkdir /etc/scrapyd
sudo vi /etc/scrapyd/scrapyd.conf
```

写入如下内容：

```
[scrapyd]
eggs_dir    = eggs
logs_dir    = logs
items_dir   =
jobs_to_keep = 5
dbs_dir     = dbs
max_proc    = 0
max_proc_per_cpu = 10
finished_to_keep = 100
poll_interval = 5.0
bind_address = 0.0.0.0
http_port   = 6800
debug       = off
runner      = scrapyd.runner
application = scrapyd.app.application
launcher    = scrapyd.launcher.Launcher
webroot     = scrapyd.website.Root

[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus
```

在这里的配置文件我有所修改，其中之一是max_proc_per_cpu官方默认为4，即一台主机每个CPU最多运行几个Scrapy Job，另外一个是bind_address，默认为本地127.0.0.1，在此修改为0.0.0.0，以使外网可以访问。

## 后台运行

由于Scrapyd使一个纯Python项目，在这里可以直接调用scrapyd来运行，为了使程序一直在后台运行，可以使用如下命令：

```
(scrapyd > /dev/null &)
```

这样Scrapyd就会在后台持续运行了，控制台输出直接忽略，当然如果想记录输出日志可以修改输出目标，如：

```
(scrapyd > ~/scrapyd.log &)
```

则会输出Scrapyd运行输出到~/scrapyd.log文件中。

运行之后便可以在浏览器的6800访问WebUI了，可以简略看到当前Scrapyd的运行Job、日志等内容。

![](./assets/2017-06-06-00-12-28.png)

在后文我们会详细讲解Scrapy项目的部署及项目运行状态监控方法。

