# 12.3 pyspider用法详解

前面我们了解了 pyspider 的基本用法，我们通过非常少的代码和便捷的可视化操作就完成了一个爬虫的编写，本节我们来总结一下它的详细用法。

### 1. 命令行

上面的实例通过如下命令启动pyspider：

```
pyspider all
```

命令行还有很多可配制参数，完整的命令行结构如下所示：

```
pyspider [OPTIONS] COMMAND [ARGS]
```

其中，OPTIONS为可选参数，它可以指定如下参数。

```
Options:
  -c, --config FILENAME    指定配置文件名称
  --logging-config TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf
  --debug                  开启调试模式
  --queue-maxsize INTEGER  队列的最大长度
  --taskdb TEXT            taskdb的数据库连接字符串, 默认: sqlite
  --projectdb TEXT         projectdb的数据库连接字符串, 默认: sqlite
  --resultdb TEXT          resultdb的数据库连接字符串, 默认: sqlite
  --message-queue TEXT     消息队列连接字符串，默认: multiprocessing.Queue
  --phantomjs-proxy TEXT   PhantomJS使用的代理，ip:port 的形式
  --data-path TEXT         数据库存放的路径
  --version                pyspider的版本
  --help                   显示帮助信息
```

例如，-c可以指定配置文件的名称，这是一个常用的配置，配置文件的样例结构如下所示：

```json
{
  "taskdb": "mysql+taskdb://username:password@host:port/taskdb",
  "projectdb": "mysql+projectdb://username:password@host:port/projectdb",
  "resultdb": "mysql+resultdb://username:password@host:port/resultdb",
  "message_queue": "amqp://username:password@host:port/%2F",
  "webui": {
    "username": "some_name",
    "password": "some_passwd",
    "need-auth": true
  }
}
```

如果要配置pyspider WebUI的访问认证，可以新建一个pyspider.json，内容如下所示：

```json
{
  "webui": {
    "username": "root",
    "password": "123456",
    "need-auth": true
  }
}
```

这样我们通过在启动时指定配置文件来配置pyspider WebUI的访问认证，用户名为root，密码为123456，命令如下所示：

```
pyspider -c pyspider.json all
```

运行之后打开：[http://localhost:5000/](http://localhost:5000/)，页面如 12-26 所示：

![](./assets/12-26.png)

图 12-26 运行页面

也可以单独运行pyspider的某一个组件。

运行Scheduler的命令如下所示：

```
pyspider scheduler [OPTIONS]
```

运行时也可以指定各种配置，参数如下所示：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --inqueue-limit INTEGER  任务队列的最大长度，如果满了则新的任务会被忽略
  --delete-time INTEGER    设置为delete标记之前的删除时间
  --active-tasks INTEGER   当前活跃任务数量配置
  --loop-limit INTEGER     单轮最多调度的任务数量
  --scheduler-cls TEXT     Scheduler使用的类
  --help                   显示帮助信息
```

运行Fetcher的命令如下所示：

```
pyspider fetcher [OPTIONS]
```

参数配置如下所示：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --poolsize INTEGER      同时请求的个数
  --proxy TEXT            使用的代理
  --user-agent TEXT       使用的User-Agent
  --timeout TEXT          超时时间
  --fetcher-cls TEXT      Fetcher使用的类
  --help                  显示帮助信息
```


运行 Processer 的命令如下所示：

```
pyspider processor [OPTIONS]
```

参数配置如下所示：

```
Options:
  --processor-cls TEXT  Processor使用的类
  --help                显示帮助信息
```

运行 WebUI 的命令如下所示：

```
pyspider webui [OPTIONS]
```

参数配置如下所示：

```
Options:
  --host TEXT            运行地址
  --port INTEGER         运行端口
  --cdn TEXT             JS和CSS的CDN服务器
  --scheduler-rpc TEXT   Scheduler的xmlrpc路径
  --fetcher-rpc TEXT     Fetcher的xmlrpc路径
  --max-rate FLOAT       每个项目最大的rate值
  --max-burst FLOAT      每个项目最大的burst值
  --username TEXT        Auth验证的用户名
  --password TEXT        Auth验证的密码
  --need-auth            是否需要验证
  --webui-instance TEXT  运行时使用的Flask应用
  --help                 显示帮助信息
```

这里的配置和前面提到的配置文件参数是相同的。如果想要改变WebUI的端口为5001，单独运行如下命令：

```
pyspider webui --port 5001
```

或者可以将端口配置到JSON文件中，配置如下所示：

```json
{
  "webui": {
    "port": 5001
  }
}
```

使用如下命令启动同样可以达到相同的效果：

```
pyspider -c pyspider.json webui
```

这样就可以在5001端口上运行WebUI了。

### 2. crawl()方法

在前面的例子中，我们使用crawl()方法实现了新请求的生成，但是只指定了URL和Callback。这里将详细介绍一下crawl()方法的参数配置。

#### url

url是爬取时的URL，可以定义为单个URL字符串，也可以定义成URL列表。

#### callback

callback是回调函数，指定了该URL对应的响应内容用哪个方法来解析，如下所示：

```python
def on_start(self):
    self.crawl('http://scrapy.org/', callback=self.index_page)
```

这里指定了callback为index_page，就代表爬取http://scrapy.org/链接得到的响应会用index_page()方法来解析。

index_page()方法的第一个参数是响应对象，如下所示：

```python
def index_page(self, response):
    pass
```

方法中的response参数就是请求上述URL得到的响应对象，我们可以直接在index_page()方法中实现页面的解析。

#### age

age是任务的有效时间。如果某个任务在有效时间内且已经被执行，则它不会重复执行，如下所示：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=10*24*60*60)
```

或者可以这样设置：

```python
@config(age=10 * 24 * 60 * 60)
def callback(self):
    pass
```

默认的有效时间为 10 天。

#### priority

priority是爬取任务的优先级，其值默认是0，priority的数值越大，对应的请求会越优先被调度，如下所示：

```python
def index_page(self):
    self.crawl('http://www.example.org/page.html', callback=self.index_page)
    self.crawl('http://www.example.org/233.html', callback=self.detail_page,
               priority=1)
```

第二个任务会优先调用，233.html这个链接优先爬取。

#### exetime

exetime参数可以设置定时任务，其值是时间戳，默认是0，即代表立即执行，如下所示：

```python
import time
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               exetime=time.time()+30*60)
```

这样该任务会在30分钟之后执行。

#### retries

retries可以定义重试次数，其值默认是3。

#### itag

itag参数设置判定网页是否发生变化的节点值，在爬取时会判定次当前节点是否和上次爬取到的节点相同。如果节点相同，则证明页面没有更新，就不会重复爬取，如下所示：

```python
def index_page(self, response):
    for item in response.doc('.item').items():
        self.crawl(item.find('a').attr.url, callback=self.detail_page,
                   itag=item.find('.update-time').text())
```

在这里设置了更新时间这个节点的值为 itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。

#### auto_recrawl

当开启时，爬取任务在过期后会重新执行，循环时间即定义的age时间长度，如下所示：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=5*60*60, auto_recrawl=True)
```

这里定义了age有效期为5小时，设置了auto_recrawl为True，这样任务就会每5小时执行一次。

#### method

method是HTTP请求方式，它默认是GET。如果想发起POST请求，可以将method设置为POST。

#### params

我们可以方便地使用params来定义GET请求参数，如下所示：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback,
               params={'a': 123, 'b': 'c'})
    self.crawl('http://httpbin.org/get?a=123&b=c', callback=self.callback)
```

这里两个爬取任务是等价的。

#### data

data是POST表单数据。当请求方式为POST时，我们可以通过此参数传递表单数据，如下所示：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', data={'a': 123, 'b': 'c'})
```

#### files

files是上传的文件，需要指定文件名，如下所示：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', files={field: {filename: 'content'}})
```

#### user_agent

user_agent是爬取使用的User-Agent。

#### headers

headers是爬取时使用的Headers，即Request Headers。

#### cookies

cookies是爬取时使用的Cookies，为字典格式。

#### connect_timeout

connect_timeout是在初始化连接时的最长等待时间，它默认是20秒。

#### timeout

timeout是抓取网页时的最长等待时间，它默认是120秒。

#### allow_redirects

allow_redirects确定是否自动处理重定向，它默认是True。

#### validate_cert

validate_cert确定是否验证证书，此选项对HTTPS请求有效，默认是True。

#### proxy

proxy是爬取时使用的代理，它支持用户名密码的配置，格式为username:password@hostname:port，如下所示：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback, proxy='127.0.0.1:9743')
```

也可以设置craw_config来实现全局配置，如下所示：

```python
class Handler(BaseHandler):
    crawl_config = {
        'proxy': '127.0.0.1:9743'
    }
```

#### fetch_type

fetch_type开启PhantomJS渲染。如果遇到JavaScript渲染的页面，指定此字段即可实现PhantomJS的对接，pyspider将会使用PhantomJS进行网页的抓取，如下所示：

```python
def on_start(self):
    self.crawl('https://www.taobao.com', callback=self.index_page, fetch_type='js')
```

这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。

#### js_script

js_script是页面加载完毕后执行的JavaScript脚本，如下所示：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               fetch_type='js', js_script='''
               function() {
                   window.scrollTo(0,document.body.scrollHeight);
                   return 123;
               }
               ''')
```

页面加载成功后将执行页面混动的JavaScript代码，页面会下拉到最底部。

#### js_run_at

js_run_at 代表 JavaScript脚本运行的位置，是在页面节点开头还是结尾，默认是结尾，即document-end。

#### js_viewport_width/js_viewport_height

js_viewport_width/js_viewport_height是JavaScript渲染页面时的窗口大小。

#### load_images

load_images在加载JavaScript页面时确定是否加载图片，它默认是否。

#### save

save参数非常有用，可以在不同的方法之间传递参数，如下所示：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               save={'page': 1})

def callback(self, response):
    return response.save['page']
```

这样，在on_start()方法中生成Request并传递额外的参数page，在回调函数里可以通过response变量的save字段接收到这些参数值。

#### cancel

cancel是取消任务，如果一个任务是ACTIVE状态的，则需要将force_update设置为True。

#### force_update

即使任务处于 ACTIVE 状态，那也会强制更新状态。

以上便是 crawl() 方法的参数介绍，更加详细的描述可以参考：[http://docs.pyspider.org/en/latest/apis/self.crawl/](http://docs.pyspider.org/en/latest/apis/self.crawl/)。


### 3. 任务区分

在pyspider判断两个任务是否是重复的是使用的是该任务对应的URL的MD5值作为任务的唯一ID，如果ID相同，那么两个任务就会判定为相同，其中一个就不会爬取了。很多情况下请求的链接可能是同一个，但是POST的参数不同。这时可以重写task_id()方法，改变这个ID的计算方式来实现不同任务的区分，如下所示：

```python
import json
from pyspider.libs.utils import md5string
def get_taskid(self, task):
    return md5string(task['url']+json.dumps(task['fetch'].get('data', '')))
```

这里重写了get_taskid()方法，利用URL和POST的参数来生成ID。这样一来，即使URL相同，但是POST的参数不同，两个任务的ID就不同，它们就不会被识别成重复任务。

### 4. 全局配置

pyspider可以使用crawl_config来指定全局的配置，配置中的参数会和crawl()方法创建任务时的参数合并。如要全局配置一个Headers，可以定义如下代码：

```python
class Handler(BaseHandler):
    crawl_config = {
        'headers': {
            'User-Agent': 'GoogleBot',
        }
    }
```

### 5. 定时爬取

我们可以通过every属性来设置爬取的时间间隔，如下所示：

```python
@every(minutes=24 * 60)
def on_start(self):
    for url in urllist:
        self.crawl(url, callback=self.index_page)
```

这里设置了每天执行一次爬取。

在上文中我们提到了任务的有效时间，在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，这样才可以实现定时爬取。

例如，下面的代码就无法做到每天爬取：

```python
@every(minutes=24 * 60)
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.index_page)

@config(age=10 * 24 * 60 * 60)
def index_page(self):
    pass
```

这里任务的过期时间为10天，而自动爬取的时间间隔为1天。当第二次尝试重新爬取的时候，pyspider会监测到此任务尚未过期，便不会执行爬取，所以我们需要将age设置得小于定时时间。

### 6. 项目状态

每个项目都有6个状态，分别是TODO、STOP、CHECKING、DEBUG、RUNNING、PAUSE。

- TODO：它是项目刚刚被创建还未实现时的状态。

- STOP：如果想停止某项目的抓取，可以将项目的状态设置为STOP。

- CHECKING：正在运行的项目被修改后就会变成CHECKING状态，项目在中途出错需要调整的时候会遇到这种情况。

- DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。

- PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为PAUSE状态，并等待一定时间后继续爬取。

### 7. 抓取进度

在抓取时，可以看到抓取的进度，progress部分会显示4个进度条，如图12-27所示。

![](./assets/12-27.jpg)

图 12-27 抓取进度

progress中的5m、1h、1d指的是最近5分、1小时、1天内的请求情况，all代表所有的请求情况。

蓝色的请求代表等待被执行的任务，绿色的代表成功的任务，黄色的代表请求失败后等待重试的任务，红色的代表失败次数过多而被忽略的任务，从这里我们可以直观看到爬取的进度和请求情况。

### 8. 删除项目

pyspider中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为STOP，将分组的名称设置为delete，等待24小时，则项目会自动删除。

### 9. 结语

以上内容便是pyspider的常用用法。如要了解更多，可以参考pyspider的官方文档：[http://docs.pyspider.org/](http://docs.pyspider.org/)。


