## 3.2　使用requests

上一节中，我们了解了urllib的基本用法，但是其中确实有不方便的地方，比如处理网页验证和Cookies时，需要写Opener和Handler来处理。为了更加方便地实现这些操作，就有了更为强大的库requests，有了它，Cookies、登录验证、代理设置等操作都不是事儿。

接下来，让我们领略一下它的强大之处吧。

### 3.2.1　基本用法

#### 1. 准备工作
在开始之前，请确保已经正确安装好了requests库。如果没有安装，可以参考1.2.1节安装。

#### 2. 实例引入
urllib库中的urlopen 方法实际上是以GET方式请求网页，而requests中相应的方法就是get方法，是不是感觉表达更明确一些？下面通过实例来看一下：

```python
import requests  

r = requests.get('https://www.baidu.com/')  
print(type(r))  
print(r.status_code)  
print(type(r.text))  
print(r.text)  
print(r.cookies)
```

运行结果如下：

```python
<class 'requests.models.Response'>
200
<class 'str'>
<html>
<head>
<script>
location.replace(location.href.replace("https://","http://"));
</script>
</head>
<body>
<noscript><meta http-equiv="refresh" content="0;url=http://www.baidu.com/"></noscript>
</body>
</html>
<RequestsCookieJar[<Cookie BIDUPSID=992C3B26F4C4D09505C5E959D5FBC005 for .baidu.com/>, <Cookie
PSTM=1472227535 for .baidu.com/>, <Cookie __bsi=15304754498609545148_00_40_N_N_2_0303_C02F_N_N_N_0
for .www.baidu.com/>, <Cookie BD_NOT_HTTPS=1 for www.baidu.com/>]>
```

这里我们调用get方法实现与urlopen相同的操作，得到一个Response对象，然后分别输出了Response的类型、状态码、响应体的类型、内容以及Cookies。

通过运行结果可以发现，它的返回类型是requests.models.Response，响应体的类型是字符串str，Cookies的类型是RequestsCookieJar。

使用get方法成功实现一个GET请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成，示例如下：

```
r = requests.post('http://httpbin.org/post')  
r = requests.put('http://httpbin.org/put')  
r = requests.delete('http://httpbin.org/delete')  
r = requests.head('http://httpbin.org/get')  
r = requests.options('http://httpbin.org/get')
```

这里分别用post、put、delete等方法实现了POST、PUT、DELETE等请求。是不是比urllib简单太多了？

其实这只是冰山一角，更多的还在后面。

#### 3. GET请求
HTTP中最常见的请求之一就是GET请求，下面首先来详细了解一下利用requests构建GET请求的方法。

##### 基本实例
首先，构建一个最简单的GET请求，请求的链接为[http://httpbin.org/get]([http://httpbin.org/get)，该网站会判断如果客户端发起的是GET请求的话，它返回相应的请求信息：

```
import requests  

r = requests.get('http://httpbin.org/get')  
print(r.text)

运行结果如下：

{  
  "args": {},   
  "headers": {  
    "Accept": "*/*",   
    "Accept-Encoding": "gzip, deflate",   
    "Host": "httpbin.org",   
    "User-Agent": "python-requests/2.10.0"  
  },   
  "origin": "122.4.215.33",   
  "url": "http://httpbin.org/get"  
}
```

可以发现，我们成功发起了GET请求，返回结果中包含请求头、URL、IP等信息。

那么，对于GET请求，如果要附加额外的信息，一般怎样添加呢？比如现在想添加两个参数，其中name是germey，age是22。要构造这个请求链接，是不是要直接写成：

```python
r = requests.get('http://httpbin.org/get?name=germey&age=22')
```

这样也可以，但是是不是有点不人性化呢？一般情况下，这种信息数据会用字典来存储。那么，怎样来构造这个链接呢？

这同样很简单，利用params这个参数就好了，示例如下：

```python
import requests  

data = {  
    'name': 'germey',  
    'age': 22  
}  
r = requests.get("http://httpbin.org/get", params=data)  
print(r.text)
```

运行结果如下：

```json
{  
  "args": {  
    "age": "22",   
    "name": "germey"  
  },   
  "headers": {  
    "Accept": "*/*",   
    "Accept-Encoding": "gzip, deflate",   
    "Host": "httpbin.org",   
    "User-Agent": "python-requests/2.10.0"  
  },   
  "origin": "122.4.215.33",   
  "url": "http://httpbin.org/get?age=22&name=germey"  
}
```

通过返回信息我们可以判断，请求的链接自动被构造成了：[http://httpbin.org/get?age=22&name=germey](http://httpbin.org/get?age=22&name=germey)。

另外，网页的返回类型实际上是str类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个字典格式的话，可以直接调用json方法。示例如下：

```
import requests  

r = requests.get("http://httpbin.org/get")  
print(type(r.text))  
print(r.json())  
print(type(r.json()))
```

运行结果如下：

```python
<class 'str'>
{'headers': {'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.10.0'}, 'url': 'http://httpbin.org/get', 'args': {}, 'origin': '182.33.248.131'}
<class 'dict'>
```

可以发现，调用json方法，就可以将返回结果是JSON格式的字符串转化为字典。

但需要注意的是，如果返回结果不是JSON格式，便会出现解析错误，抛出json.decoder.JSONDecodeError异常。

##### 抓取网页
上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以“知乎”→“发现”页面为例来看一下：

```
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

这里我们加入了headers信息，其中包含了User-Agent字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。

接下来我们用到了最基础的正则表达式来匹配出所有的问题内容。关于正则表达式的相关内容，我们会在3.3节中详细介绍，这里作为实例来配合讲解。

运行结果如下：

```
['\n为什么很多人喜欢提及「拉丁语系」这个词？\n', '\n在没有水的情况下水系宝可梦如何战斗？\n', '\n有哪些经验可以送给 Kindle 新人？\n', '\n谷歌的广告业务是如何赚钱的？\n', '\n程序员该学习什么，能在上学期间挣钱？\n', '\n有哪些原本只是一个小消息，但回看发现是个惊天大新闻的例子？\n', '\n如何评价今敏？\n', '\n源氏是怎么把那么长的刀从背后拔出来的？\n', '\n年轻时得了绝症或大病是怎样的感受？\n', '\n年轻时得了绝症或大病是怎样的感受？\n']
```

我们发现，这里成功提取出了所有的问题内容。

##### 抓取二进制数据

在上面的例子中，我们抓取的是知乎的一个页面，实际上它返回的是一个HTML文档。如果想抓取图片、音频、视频等文件，应该怎么办呢？

图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制码。

下面以GitHub的站点图标为例来看一下：

```python
import requests

r = requests.get("https://github.com/favicon.ico")
print(r.text)
print(r.content)
```

这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标，如图3-3所示。

![](/assets/3-3.png)

图3-3　站点图标

这里打印了Response对象的两个属性，一个是text，另一个是content。

运行结果如图3-4所示，其中前两行是r.text的结果，最后一行是r.content的结果。

![](/assets/3-4.png)

图3-4　运行结果

可以注意到，前者出现了乱码，后者结果前带有一个b，这代表是bytes类型的数据。由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，这理所当然会出现乱码。

接着，我们将刚才提取到的图片保存下来：

```python
import requests

r = requests.get("https://github.com/favicon.ico")
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

这里用了open方法，它的第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据。

运行结束之后，可以发现在文件夹中出现了名为favicon.ico的图标，如图3-5所示。

![](/assets/3-5.ico)

图3-5　图标

同样地，音频和视频文件也可以用这种方法获取。

##### 添加headers
与urllib.request一样，我们也可以通过headers参数来传递头信息。

比如，在上面“知乎”的例子中，如果不传递headers，就不能正常请求：

```python
import requests

r = requests.get("https://www.zhihu.com/explore")
print(r.text)
```

运行结果如下：

```html
<html><body><h1>500 Server Error</h1>
An internal server error occured.
</body></html>
```

但如果加上headers并加上User-Agent信息，那就没问题了：

```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
print(r.text)
```

当然，我们可以在headers这个参数中任意添加其他的字段信息。

#### 4. POST请求

前面我们了解了最基本的GET请求，另外一种比较常见的请求方式是POST。使用requests实现POST请求同样非常简单，示例如下：

```python
import requests

data = {'name': 'germey', 'age': '22'}
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

这里还是请求 [http://httpbin.org/post](http://httpbin.org/post)，该网站可以判断如果请求是POST方式，就把相关请求信息返回。

运行结果如下：

```json
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "22", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "json": null, 
  "origin": "182.33.248.131", 
  "url": "http://httpbin.org/post"
}
```

可以发现，我们成功获得了返回结果，其中form部分就是提交的数据，这就证明POST请求成功发送了。

#### 5. 响应

发送请求后，得到的自然就是响应。在上面的实例中，我们使用text和content获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下：

```python
import requests

r = requests.get('http://www.jianshu.com')
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

这里分别打印输出status_code属性得到状态码，输出headers属性得到响应头，输出cookies属性得到Cookies，输出url属性得到URL，输出history属性得到请求历史。

运行结果如下：

```python
<class 'int'> 200
<class 'requests.structures.CaseInsensitiveDict'> {'X-Runtime': '0.006363', 'Connection': 'keep-alive', 'Content-Type': 'text/html; charset=utf-8', 'X-Content-Type-Options': 'nosniff', 'Date': 'Sat, 27 Aug 2016 17:18:51 GMT', 'Server': 'nginx', 'X-Frame-Options': 'DENY', 'Content-Encoding': 'gzip', 'Vary': 'Accept-Encoding', 'ETag': 'W/"3abda885e0e123bfde06d9b61e696159"', 'X-XSS-Protection': '1; mode=block', 'X-Request-Id': 'a8a3c4d5-f660-422f-8df9-49719dd9b5d4', 'Transfer-Encoding': 'chunked', 'Set-Cookie': 'read_mode=day; path=/, default_font=font2; path=/, _session_id=xxx; path=/; HttpOnly', 'Cache-Control': 'max-age=0, private, must-revalidate'}
<class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[<Cookie _session_id=xxx for www.jianshu.com/>, <Cookie default_font=font2 for www.jianshu.com/>, <Cookie read_mode=day for www.jianshu.com/>]>
<class 'str'> http://www.jianshu.com/
<class 'list'> []
```

因为session_id过长，在此简写。可以看到，headers和cookies这两个属性得到的结果分别是CaseInsensitiveDict和RequestsCookieJar类型。

状态码常用来判断请求是否成功，而requests还提供了一个内置的状态码查询对象requests.codes，示例如下：

```python
import requests

r = requests.get('http://www.jianshu.com')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用requests.codes.ok得到的是成功的状态码200。

那么，肯定不能只有ok这个条件码。下面列出了返回码和相应的查询条件：

```
# 信息性状态码  
100: ('continue',),  
101: ('switching_protocols',),  
102: ('processing',),  
103: ('checkpoint',),  
122: ('uri_too_long', 'request_uri_too_long'),  

# 成功状态码  
200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),  
201: ('created',),  
202: ('accepted',),  
203: ('non_authoritative_info', 'non_authoritative_information'),  
204: ('no_content',),  
205: ('reset_content', 'reset'),  
206: ('partial_content', 'partial'),  
207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),  
208: ('already_reported',),  
226: ('im_used',),  

# 重定向状态码  
300: ('multiple_choices',),  
301: ('moved_permanently', 'moved', '\\o-'),  
302: ('found',),  
303: ('see_other', 'other'),  
304: ('not_modified',),  
305: ('use_proxy',),  
306: ('switch_proxy',),  
307: ('temporary_redirect', 'temporary_moved', 'temporary'),  
308: ('permanent_redirect',  
      'resume_incomplete', 'resume',), # These 2 to be removed in 3.0  

# 客户端错误状态码  
400: ('bad_request', 'bad'),  
401: ('unauthorized',),  
402: ('payment_required', 'payment'),  
403: ('forbidden',),  
404: ('not_found', '-o-'),  
405: ('method_not_allowed', 'not_allowed'),  
406: ('not_acceptable',),  
407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),  
408: ('request_timeout', 'timeout'),  
409: ('conflict',),  
410: ('gone',),  
411: ('length_required',),  
412: ('precondition_failed', 'precondition'),  
413: ('request_entity_too_large',),  
414: ('request_uri_too_large',),  
415: ('unsupported_media_type', 'unsupported_media', 'media_type'),  
416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),  
417: ('expectation_failed',),  
418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),  
421: ('misdirected_request',),  
422: ('unprocessable_entity', 'unprocessable'),  
423: ('locked',),  
424: ('failed_dependency', 'dependency'),  
425: ('unordered_collection', 'unordered'),  
426: ('upgrade_required', 'upgrade'),  
428: ('precondition_required', 'precondition'),  
429: ('too_many_requests', 'too_many'),  
431: ('header_fields_too_large', 'fields_too_large'),  
444: ('no_response', 'none'),  
449: ('retry_with', 'retry'),  
450: ('blocked_by_windows_parental_controls', 'parental_controls'),  
451: ('unavailable_for_legal_reasons', 'legal_reasons'),  
499: ('client_closed_request',),  

# 服务端错误状态码  
500: ('internal_server_error', 'server_error', '/o\\', '✗'),  
501: ('not_implemented',),  
502: ('bad_gateway',),  
503: ('service_unavailable', 'unavailable'),  
504: ('gateway_timeout',),  
505: ('http_version_not_supported', 'http_version'),  
506: ('variant_also_negotiates',),  
507: ('insufficient_storage',),  
509: ('bandwidth_limit_exceeded', 'bandwidth'),  
510: ('not_extended',),  
511: ('network_authentication_required', 'network_auth', 'network_authentication')
```

比如，如果想判断结果是不是404状态，可以用requests.codes.not_found来比对。

### 3.2.2　高级用法

在前一节中，我们了解了requests的基本用法，如基本的GET、POST请求以及Response对象。本节中，我们再来了解下requests的一些高级用法，如文件上传、Cookies设置、代理设置等。

#### 1. 文件上传

我们知道requests可以模拟提交一些数据。假如有的网站需要上传文件，我们也可以用它来实现，这非常简单，示例如下：

```python
import requests

files = {'file': open('favicon.ico', 'rb')}
r = requests.post('http://httpbin.org/post', files=files)
print(r.text)
```

在前一节中我们保存了一个文件favicon.ico，这次用它来模拟文件上传的过程。需要注意的是，favicon.ico需要和当前脚本在同一目录下。如果有其他文件，当然也可以使用其他文件来上传，更改下代码即可。

运行结果如下：

```json
{
  "args": {}, 
  "data": "", 
  "files": {
    "file": "data:application/octet-stream;base64,AAAAAA...="
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "6665", 
    "Content-Type": "multipart/form-data; boundary=809f80b1a2974132b133ade1a8e8e058", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "json": null, 
  "origin": "60.207.237.16", 
  "url": "http://httpbin.org/post"
}
```

以上省略部分内容，这个网站会返回响应，里面包含files这个字段，而form字段是空的，这证明文件上传部分会单独有一个files字段来标识。

#### 2. Cookies
前面我们使用urllib处理过Cookies，写法比较复杂，而有了requests，获取和设置Cookies只需一步即可完成。

我们先用一个实例看一下获取Cookies的过程：

```python
import requests

r = requests.get('https://www.baidu.com')
print(r.cookies)
for key, value in r.cookies.items():
    print(key + '=' + value)
```

运行结果如下：

```python
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>, <Cookie __bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/>]>
BDORZ=27315
__bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0
```

这里我们首先调用cookies属性即可成功得到Cookies，可以发现它是RequestCookieJar类型。然后用items方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析。

当然，我们也可以直接用Cookie来维持登录状态，下面以知乎为例来说明。首先登录知乎，将Headers中的Cookie内容复制下来，如图3-6所示。

![](./assets/3-6.png)
图3-6　Cookie

这里可以替换成你自己的Cookie，将其设置到Headers里面，然后发送请求，示例如下：

```python
import requests

headers = {
    'Cookie': 'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0',
    'Host': 'www.zhihu.com',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36',
}
r = requests.get('https://www.zhihu.com', headers=headers)
print(r.text)
```

我们发现，结果中包含了登录后的结果，如图3-7所示，这证明登录成功。

![](/assets/3-7.jpg)

图3-7　运行结果

当然，你也可以通过cookies参数来设置，不过这样就需要构造RequestsCookieJar对象，而且需要分割一下cookies。这相对烦琐，不过效果是相同的，示例如下：

```python
import requests

cookies = 'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'
jar = requests.cookies.RequestsCookieJar()
headers = {
    'Host': 'www.zhihu.com',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
for cookie in cookies.split(';'):
    key, value = cookie.split('=', 1)
    jar.set(key, value)
r = requests.get('http://www.zhihu.com', cookies=jar, headers=headers)
print(r.text)
```

这里我们首先新建了一个RequestCookieJar对象，然后将复制下来的cookies利用split方法分割，接着利用set方法设置好每个Cookie的key和value，然后通过调用requests的get()方法并传递给cookies参数即可。当然，由于知乎本身的限制，headers参数也不能少，只不过不需要在原来的headers参数里面设置cookie字段了。

测试后，发现同样可以正常登录知乎。

#### 3. 会话维持
在requests中，如果直接利用get或post等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的会话，也就是说相当于你用了两个浏览器打开了不同的页面。

设想这样一个场景，第一个请求利用post方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，你又用了一次get方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的会话，能成功获取个人信息吗？那当然不能。

有小伙伴可能说了，我在两次请求时设置一样的cookies不就行了？可以，但这样做起来显得很烦琐，我们有更简单的解决方法。

其实解决这个问题的主要方法就是维持同一个会话，也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但是我又不想每次设置cookies，那该怎么办呢？这时候就有了新的利器——Session对象。

利用它，我们可以方便地维护一个会话，而且不用担心cookies的问题，它会帮我们自动处理好。示例如下：

```python
import requests

requests.get('http://httpbin.org/cookies/set/number/123456789')
r = requests.get('http://httpbin.org/cookies')
print(r.text)
```

这里我们请求了一个测试网址 [http://httpbin.org/cookies/set/number/123456789](http://httpbin.org/cookies/set/number/123456789)。请求这个网址时，可以设置一个cookie，名称叫作number，内容是123456789，随后又请求了[http://httpbin.org/cookies](http://httpbin.org/cookies)，此网址可以获取当前的Cookies。

这样能成功获取到设置的Cookies吗？试试看。

运行结果如下：

{  
  "cookies": {}  
}

这并不行。我们再用Session试试看：

```python
import requests

s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

再看下运行结果：

```python
{
  "cookies": {
    "number": "123456789"
  }
}
```

成功获取！这下能体会到同一个会话和不同会话的区别了吧！

所以，利用Session，可以做到模拟同一个会话而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作。

Session在平常用得非常广泛，可以用于模拟在一个浏览器中打开同一站点的不同页面，后面会有专门的章节来讲解这部分内容。

#### 4. SSL证书验证

此外，requests还提供了证书验证的功能。当发送HTTP请求的时候，它会检查SSL证书，我们可以使用verify参数控制是否检查此证书。其实如果不加verify参数的话，默认是True，会自动验证。

前面我们提到过，12306的证书没有被官方CA机构信任，会出现证书验证错误的结果。我们现在访问它，都可以看到一个证书问题的页面，如图3-8所示。

![](/assets/3-8.png)

图3-8 错误页面

现在我们用requests来测试一下：

```python
import requests

response = requests.get('https://www.12306.cn')
print(response.status_code)
```

运行结果如下：
```python
requests.exceptions.SSLError: ("bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)",)
```

这里提示一个错误SSLError，表示证书验证错误。所以，如果请求一个HTTPS站点，但是证书验证错误的页面时，就会报这样的错误，那么如何避免这个错误呢？很简单，把verify参数设置为False即可。相关代码如下：

```python
import requests

response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

这样就会打印出请求成功的状态码：

```python
/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
200
```

不过我们发现报了一个警告，它建议我们给它指定证书。我们可以通过设置忽略警告的方式来屏蔽这个警告：

```python
import requests
from requests.packages import urllib3

urllib3.disable_warnings()
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

或者通过捕获警告到日志的方式忽略警告：

```python
import logging
import requests
logging.captureWarnings(True)
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

当然，我们也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组：

```python
import requests

response = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))
print(response.status_code)
```

当然，上面的代码是演示实例，我们需要有crt和key文件，并且指定它们的路径。注意，本地私有证书的key必须是解密状态，加密状态的key是不支持的。

#### 5. 代理设置

对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的IP，导致一定时间段内无法访问。

那么，为了防止这种情况发生，我们需要设置代理来解决这个问题，这就需要用到proxies参数。可以用这样的方式设置：

```python
import requests

proxies = {
  'http': 'http://10.10.1.10:3128',
  'https': 'http://10.10.1.10:1080',
}

requests.get('https://www.taobao.com', proxies=proxies)
```

当然，直接运行这个实例可能不行，因为这个代理可能是无效的，请换成自己的有效代理试验一下。

若代理需要使用HTTP Basic Auth，可以使用类似http://user:password@host:port这样的语法来设置代理，示例如下：

```python
import requests

proxies = {
    'https': 'http://user:password@10.10.1.10:3128/',
}
requests.get('https://www.taobao.com', proxies=proxies)
```

除了基本的HTTP代理外，requests还支持SOCKS协议的代理。

首先，需要安装socks这个库：

```
pip3 install "requests[socks]"
```

然后就可以使用SOCKS协议代理了，示例如下：

```python
import requests

proxies = {
    'http': 'socks5://user:password@host:port',
    'https': 'socks5://user:password@host:port'
}
requests.get('https://www.taobao.com', proxies=proxies)
```

#### 6. 超时设置

在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到timeout参数。这个时间的计算是发出请求到服务器返回响应的时间。示例如下：

```python
import requests

r = requests.get('https://www.taobao.com', timeout=1)
print(r.status_code)
```

通过这样的方式，我们可以将超时时间设置为1秒，如果1秒内没有响应，那就抛出异常。

实际上，请求分为两个阶段，即连接（connect）和读取（read）。

上面设置的timeout将用作连接和读取这二者的timeout总和。

如果要分别指定，就可以传入一个元组：

```python
r = requests.get('https://www.taobao.com', timeout=(5, 30))
```

如果想永久等待，可以直接将timeout设置为None，或者不设置直接留空，因为默认是None。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。其用法如下：

```python
r = requests.get('https://www.taobao.com', timeout=None)
```

或直接不加参数：

```python
r = requests.get('https://www.taobao.com')
```

#### 7. 身份认证

在访问网站时，我们可能会遇到这样的认证页面，如图3-9所示。

![](./assets/3-9.jpg)

图3-9　认证页面

此时可以使用requests自带的身份认证功能，示例如下：

```python
import requests  
from requests.auth import HTTPBasicAuth  

r = requests.get('http://localhost:5000', auth=HTTPBasicAuth('username', 'password'))  
print(r.status_code)
```

如果用户名和密码正确的话，请求时就会自动认证成功，会返回200状态码；如果认证失败，则返回401状态码。

当然，如果参数都传一个HTTPBasicAuth类，就显得有点烦琐了，所以requests提供了一个更简单的写法，可以直接传一个元组，它会默认使用HTTPBasicAuth这个类来认证。

所以上面的代码可以直接简写如下：

```python
import requests

r = requests.get('http://localhost:5000', auth=('username', 'password'))
print(r.status_code)
```

此外，requests还提供了其他认证方式，如OAuth认证，不过此时需要安装oauth包，安装命令如下：

```
pip3 install requests_oauthlib
```

使用OAuth1认证的方法如下：

```python
import requests
from requests_oauthlib import OAuth1

url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET',
              'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
requests.get(url, auth=auth)
```

更多详细的功能就可以参考 requests_oauthlib 的官方文档：[https://requests-oauthlib.readthedocs.org/](https://requests-oauthlib.readthedocs.org/)，在此就不再赘述了。

#### 8. Prepared Request

前面介绍urllib时，我们可以将请求表示为数据结构，其中各个参数都可以通过一个Request对象来表示。这在requests里同样可以做到，这个数据结构就叫Prepared Request。我们用实例看一下：

```python
from requests import Request, Session

url = 'http://httpbin.org/post'
data = {
    'name': 'germey'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```

这里我们引入了Request，然后用url、data和headers参数构造了一个Request对象，这时需要再调用Session的prepare_request方法将其转换为一个Prepared Request对象，然后调用send方法发送即可，运行结果如下：

```json
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Connection": "close", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36"
  }, 
  "json": null, 
  "origin": "182.32.203.166", 
  "url": "http://httpbin.org/post"
}
```

可以看到，我们达到了同样的POST请求效果。

有了Request这个对象，就可以将请求当作独立的对象来看待，这样在进行队列调度时会非常方便。后面我们会用它来构造一个Request队列。

本节讲解了requests的一些高级用法，这些用法在后面实战部分会经常用到，需要熟练掌握。

更多的用法可以参考 Requests 的官方文档：[http://docs.python-requests.org/](http://docs.python-requests.org/)。

