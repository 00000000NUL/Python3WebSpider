# 新浪微博

在前面我们讲解了Scrapy中各个模块基本使用方法以及代理池、Cookies池，接下来我们以一个反爬比较强的网站新浪微博为例来实战一下Scrapy的大规模爬取。

本次爬取的目标是用户的公开基本信息如昵称、头像等内容，用户的关注、粉丝列表以及发布的微博基本信息。

## 爬取思路

爬微博用户在这里我们以微博的几个大V为起始点，接着爬取他们各自的粉丝和关注的用户信息，然后再接着获取粉丝和关注的每个人的粉丝和关注的用户信息，以此类推下去进行递归爬取，这样如果一个用户与其他用户有社交网络上的关联那就会被抓取到，所以这样我们几乎可以做到所有用户的爬取。另外我们既然爬取到了每个用户，那么一定可以得到用户的唯一ID，再根据ID获取每个用户发布的微博即可。

## 爬取分析

在这里我们选取的爬取站点是[https://m.weibo.cn](https://m.weibo.cn)，此站点是微博移动端的站点，直接进入会直接调转到登录页面，这里其实只是主页做了登录限制，不过我们可以绕过，直接打开某个用户详情页面即可，例如打开周冬雨的微博，链接为[https://m.weibo.cn/u/1916655407](https://m.weibo.cn/u/1916655407)即可看到进入到其个人详情页面。

![](./assets/2017-08-01-14-13-31.jpg)

这时我们在页面最上方可以看到周冬雨的关注和粉丝数量，接下来我们点击关注二字进入到她的关注列表。

![](./assets/2017-08-01-14-15-13.jpg)

这时我们打开开发者工具，切换到XHR过滤器，然后一直下拉，即可看到下方就会出现很多Ajax请求，这些请求就是获取周冬雨的关注列表的Ajax请求。

![](./assets/2017-08-01-14-17-47.png)

我们打开第一个Ajax请求看一下，发现它的链接为[https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&luicode=10000011&lfid=1005051916655407&featurecode=20000320&type=uid&value=1916655407&page=2](https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&luicode=10000011&lfid=1005051916655407&featurecode=20000320&type=uid&value=1916655407&page=2)。

![](./assets/2017-08-01-14-18-47.jpg)

![](./assets/2017-08-01-14-20-28.jpg)

请求类型是GET类型，返回结果是Json格式，将其展开之后即可看到其关注的用户的基本信息，接下来我们只需要构造这个请求的参数就好了，可以看到此链接一共有7个参数。


![](./assets/2017-08-01-14-23-24.jpg)

其中最主要的参数就是containerid和page，有了这两个参数我们同样可以获取请求结果，所以在这里我们可以将接口精简为[https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&page=2](https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&page=2)，这里的container_id的前半部分是固定的，最后一部分又是用户的id，所以这里参数就可以构造出来了，只需要修改container_id最后的id和page参数即可获取分页形式的关注列表信息。

那么利用同样的方法我们也可以分析出用户详情的Ajax链接、用户微博列表的Ajax链接，所以在此总结如下：

```python
# 用户详情API
user_url = 'https://m.weibo.cn/api/container/getIndex?uid={uid}&type=uid&value={uid}&containerid=100505{uid}'
# 关注列表API
follow_url = 'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_{uid}&page={page}'
# 粉丝列表API
fan_url = 'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_{uid}&page={page}'
# 微博列表API
weibo_url = 'https://m.weibo.cn/api/container/getIndex?uid={uid}&type=uid&page={page}&containerid=107603{uid}'
```

此处的uid和page分别代表用户ID和分页页码。

注意这个API可能随着时间的变化或者微博的改版而变化，以实测为准。

所以我们就从几个大V开始抓取，抓取他们的粉丝、关注、微博信息，然后再递归抓取他们的粉丝和关注的用户的分析、关注、微博信息，递归抓取，最后保存下来微博用户的基本信息、关注和粉丝列表、发布的微博。

存储的数据库我们选择MongoDB，因为我们需要保存用户的粉丝和关注列表，用MongoDB可以实现更方便的存储。

## 代码实战

接下来我们用Scrapy来实现一下这个抓取过程，首先让我们创建一个项目，命令如下：

```
scrapy startproject weibo
```

随后进入到项目中，新建一个Spider，在这里取名为weibocn，命令如下：

```
scrapy genspider weibocn m.weibo.cn
```

我们首先修改一下Spider，将各个Ajax的URL配置一下，然后选取几个大V，将他们的ID赋值成一个列表，然后实现start_requests()方法，也就是依次抓取各个大V的个人详情，然后用parse_user()进行解析，实现如下：

```python
from scrapy import Request, Spider

class WeiboSpider(Spider):
    name = 'weibocn'
    allowed_domains = ['m.weibo.cn']
    user_url = 'https://m.weibo.cn/api/container/getIndex?uid={uid}&type=uid&value={uid}&containerid=100505{uid}'
    follow_url = 'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_{uid}&page={page}'
    fan_url = 'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_{uid}&page={page}'
    weibo_url = 'https://m.weibo.cn/api/container/getIndex?uid={uid}&type=uid&page={page}&containerid=107603{uid}'
    start_users = ['3217179555', '1742566624', '2282991915', '1288739185', '3952070245', '5878659096']
    
    def start_requests(self):
        for uid in self.start_users:
            yield Request(self.user_url.format(uid=uid), callback=self.parse_user)
    
    def parse_user(self, response):
        self.logger.debug(response)
```

接下来我们就需要解析用户的基本信息并生成Item了，所以在这里我们先定义几个Item，如用户、用户关系、微博的Item：

```python
from scrapy import Item, Field

class UserItem(Item):
    collection = 'users'
    id = Field()
    name = Field()
    avatar = Field()
    cover = Field()
    gender = Field()
    description = Field()
    fans_count = Field()
    follows_count = Field()
    weibos_count = Field()
    verified = Field()
    verified_reason = Field()
    verified_type = Field()
    follows = Field()
    fans = Field()
    crawled_at = Field()

class UserRelationItem(Item):
    collection = 'users'
    id = Field()
    follows = Field()
    fans = Field()

class WeiboItem(Item):
    collection = 'weibos'
    id = Field()
    attitudes_count = Field()
    comments_count = Field()
    reposts_count = Field()
    picture = Field()
    pictures = Field()
    source = Field()
    text = Field()
    raw_text = Field()
    thumbnail = Field()
    user = Field()
    created_at = Field()
    crawled_at = Field()
```

在这里还定义了collection字段，指明了保存的Collection的名称。用户的关注和分析列表我们在这里直接定义为一个单独的UserRelationItem，其中id就是用户的ID，follows就是用户关注列表，fans是粉丝列表，但这并不代表着我们会将关注和粉丝列表存到一个单独的Collection里面，后面我们会用Pipeline对各个Item进行处理，合并存储到用户的Collection里面，所以Item和Collection并不一定是完全对应的。

接下来我们开始解析一下用户的基本信息，实现parse_user()方法：

```python
def parse_user(self, response):
    """
    解析用户信息
    :param response: Response对象
    """
    result = json.loads(response.text)
    if result.get('userInfo'):
        user_info = result.get('userInfo')
        user_item = UserItem()
        field_map = {
            'id': 'id', 'name': 'screen_name', 'avatar': 'profile_image_url', 'cover': 'cover_image_phone',
            'gender': 'gender', 'description': 'description', 'fans_count': 'followers_count',
            'follows_count': 'follow_count', 'weibos_count': 'statuses_count', 'verified': 'verified',
            'verified_reason': 'verified_reason', 'verified_type': 'verified_type'
        }
        for field, attr in field_map.items():
            user_item[field] = user_info.get(attr)
        yield user_item
        # 关注
        uid = user_info.get('id')
        yield Request(self.follow_url.format(uid=uid, page=1), callback=self.parse_follows,
                      meta={'page': 1, 'uid': uid})
        # 粉丝
        yield Request(self.fan_url.format(uid=uid, page=1), callback=self.parse_fans,
                      meta={'page': 1, 'uid': uid})
        # 微博
        yield Request(self.weibo_url.format(uid=uid, page=1), callback=self.parse_weibos,
                      meta={'page': 1, 'uid': uid})
```

在这里我们一共做了两个操作，分别是：
* 解析Json提取用户信息并生成UserItem返回，在这里赋值的时候我们并没有采用常规的逐个赋值的方法，而是定义了一个字段映射关系，因为我们定义的字段名称可能和Json中用户的字段名称不同，所以在这里定义成一个字典，然后遍历字典实现逐个字段的赋值。
* 构造用户的关注、粉丝、微博的第一页的链接并生成Request，在这里需要的参数只有用户的ID，另外分页页码我们直接设置为1即可。

这样我们就成功解析了用户的基本信息并生成了接下来的Request。

那么接下来我们还需要保存用户的关注和粉丝列表，我们拿关注列表来说，其解析方法为parse_follows()，我们在这里实现如下：

```python
def parse_follows(self, response):
    """
    解析用户关注
    :param response: Response对象
    """
    result = json.loads(response.text)
    if result.get('ok') and result.get('cards') and len(result.get('cards')) and result.get('cards')[-1].get(
        'card_group'):
        # 解析用户
        follows = result.get('cards')[-1].get('card_group')
        for follow in follows:
            if follow.get('user'):
                uid = follow.get('user').get('id')
                yield Request(self.user_url.format(uid=uid), callback=self.parse_user)
        # 关注列表
        uid = response.meta.get('uid')
        user_relation_item = UserRelationItem()
        follows = [{'id': follow.get('user').get('id'), 'name': follow.get('user').get('screen_name')} for follow in
                   follows]
        user_relation_item['id'] = uid
        user_relation_item['follows'] = follows
        user_relation_item['fans'] = []
        yield user_relation_item
        # 下一页关注
        page = response.meta.get('page') + 1
        yield Request(self.follow_url.format(uid=uid, page=page),
                      callback=self.parse_follows, meta={'page': page, 'uid': uid})
```

那么在这个方法里面我们做了三件事，分别是：
* 解析关注列表中的每个用户信息并发起新的解析请求，这里我们首先解析了关注列表的信息，然后就可以得到用户的ID，然后再利用user_url构造访问用户详情的Request，回调就是刚才所定义的parse_user()方法。
* 提取用户关注列表内的关键信息并生成UserRelationItem，id字段直接设置成用户的ID，另外Json的返回数据中用户信息有很多冗余字段，在这里我们只提取了关注的用户的ID和用户名，然后把它赋值给follows字段，fans字段设置成空列表。这样我们就建立了一个存有用户ID和用户部分关注列表的UserRelationItem，后面我们会将同一个ID的UserRelationItem的关注和粉丝列表分别合并然后保存。
* 提取下一页关注，在这里我们只需要将此请求的分页页码加1即可，分页页码通过Request的meta属性进行传递，用Response的meta即可接收，这样我们构造出下一页的关注列表的Request并返回。

抓取粉丝列表和关注列表原理相同，在此不再赘述。

接下来我们还差一个方法的实现，那就是parse_weibos()，用来抓取用户的微博信息，实现如下：

```python
def parse_weibos(self, response):
    """
    解析微博列表
    :param response: Response对象
    """
    result = json.loads(response.text)
    if result.get('ok') and result.get('cards'):
        weibos = result.get('cards')
        for weibo in weibos:
            mblog = weibo.get('mblog')
            if mblog:
                weibo_item = WeiboItem()
                field_map = {
                    'id': 'id', 'attitudes_count': 'attitudes_count', 'comments_count': 'comments_count', 'created_at': 'created_at',
                    'reposts_count': 'reposts_count', 'picture': 'original_pic', 'pictures': 'pics',
                    'source': 'source', 'text': 'text', 'raw_text': 'raw_text', 'thumbnail': 'thumbnail_pic'
                }
                for field, attr in field_map.items():
                    weibo_item[field] = mblog.get(attr)
                weibo_item['user'] = response.meta.get('uid')
                yield weibo_item
        # 下一页微博
        uid = response.meta.get('uid')
        page = response.meta.get('page') + 1
        yield Request(self.weibo_url.format(uid=uid, page=page), callback=self.parse_weibos,
                      meta={'uid': uid, 'page': page})
```

在这里parse_weibos()方法干了两件事，分别是：
* 提取用户的微博信息，并生成WeiboItem，这里同样建立了一个字段映射表，然后实现批量字段赋值。
* 提取下一页的微博列表，这里同样需要传入用户ID和分页页码。

那么到现在为止微博的Spider就已经完成了，我们后面还需要对数据进行数据清洗存储和对接代理池、Coookies池来防止反爬虫。

### 数据清洗

首先我们可以注意到有些微博的时间可能不是标准的时间，比如它可能是显示刚刚、几分钟前、几小时钱、昨天等等，所以在这里我们需要将它们统一转化一下，修改例如可以实现一个parse_time()方法：

```python
def parse_time(self, date):
    if re.match('刚刚', date):
        date = time.strftime('%Y-%m-%d %H:%M', time.localtime(time.time()))
    if re.match('\d+分钟前', date):
        minute = re.match('(\d+)', date).group(1)
        date = time.strftime('%Y-%m-%d %H:%M', time.localtime(time.time() - float(minute) * 60))
    if re.match('\d+小时前', date):
        hour = re.match('(\d+)', date).group(1)
        date = time.strftime('%Y-%m-%d %H:%M', time.localtime(time.time() - float(hour) * 60 * 60))
    if re.match('昨天.*', date):
        date = re.match('昨天(.*)', date).group(1).strip()
        date = time.strftime('%Y-%m-%d', time.localtime() - 24 * 60 * 60) + ' ' + date
    if re.match('\d{2}-\d{2}', date):
        date = time.strftime('%Y-', time.localtime()) + date + ' 00:00'
    return date
```

我们在这里用正则来提取一些关键数字，然后再用time库来实现标准时间的转换。

以X分钟前的处理为例，爬取的时间会赋值为created_at字段，我们首先用正则匹配这个时间，表达式写作`\d+分钟前`，如果符合这个规则，那么就提取出其中的数字，接下来再利用time模块的strftime()方法，第一个参数传入要转换的时间格式，第二个参数就是时间戳，在这里我们用当前的时间戳减去此数字乘以60就是当时的时间戳，这样我们就可以得到格式化后的正确时间了。

然后Pipeline里面可以实现如下处理：

```python
class WeiboPipeline():
    def process_item(self, item, spider):
        if isinstance(item, WeiboItem):
            if item.get('created_at'):
                item['created_at'] = item['created_at'].strip()
                item['created_at'] = self.parse_time(item.get('created_at'))
```

另外我们在Spider里面没有对crawled_at这个字段进行赋值，它代表爬取时间，我们可以统一为其赋值即可，赋值为当前时间，实现如下：

```python
class TimePipeline():
    def process_item(self, item, spider):
        if isinstance(item, UserItem) or isinstance(item, WeiboItem):
            now = time.strftime('%Y-%m-%d %H:%M', time.localtime())
            item['crawled_at'] = now
        return item
```

在这里我们判断了Item是UserItem或WeiboItem类型就为它的crawled_at字段赋值为当前时间。

通过上面的两个Pipeline我们便完成了数据清洗工作，主要是时间的转换。

### 数据存储

数据清洗完毕之后，我们就要将其保存到MongoDB数据库了，我们在这里实现MongoPipeline类如下：

```python
import pymongo

class MongoPipeline(object):
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
        self.db[UserItem.collection].create_index([('id', pymongo.ASCENDING)])
        self.db[WeiboItem.collection].create_index([('id', pymongo.ASCENDING)])
    
    def close_spider(self, spider):
        self.client.close()
    
    def process_item(self, item, spider):
        if isinstance(item, UserItem) or isinstance(item, WeiboItem):
            self.db[item.collection].update({'id': item.get('id')}, {'$set': item}, True)
        if isinstance(item, UserRelationItem):
            self.db[item.collection].update(
                {'id': item.get('id')},
                {'$addToSet':
                    {
                        'follows': {'$each': item['follows']},
                        'fans': {'$each': item['fans']}
                    }
                }, True)
        return item
```

当前的MongoPipeline和前面我们所写的有所不同，主要是：
* 在open_spider()方法里面添加了Collection的索引，在这里为两个Item都做了索引，索引的字段是id，由于我们这次是大规模爬取，同时在爬取过程中涉及到数据的更新问题，所以我们为每个Collection建立了索引，建立了索引之后可以大大提高检索效率。
* 在process_item()方法里存储使用的是update()方法，第一个参数是查询条件，第二个参数是爬取的Item，这里我们使用了$set操作符，这样我们如果爬取到了重复的数据即可对数据进行更新，同时不会删除已存在的字段，如果这里不加$set操作符，那么会直接进行item替换，这样可能会导致已存在的字段如关注和粉丝列表清空，所以这里必须要加上$set操作符。第三个参数我们设置为了True，这个参数起到的作用是如果数据不存在，则插入数据。这样我们就可以做到数据存在即更新、数据不存在即插入，这样就达到了去重的效果。
* 对于用户的关注和粉丝列表，我们在这里使用了一个新的操作符，叫做 $addToSet，这个操作符可以向列表类型的字段插入数据同时去重，接下来它的值就是需要操作的字段名称，我们在这里又利用了$each操作符对需要插入的列表数据进行了遍历，这样就可以逐条插入用户的关注或粉丝数据到指定的字段了，关于该操作更多的解释可以参考MongoDB的官方文档，链接为[https://docs.mongodb.com/manual/reference/operator/update/addToSet/(https://docs.mongodb.com/manual/reference/operator/update/addToSet/)]。

### 防反爬虫

#### Cookies池对接

如果没有登录直接请求微博的API接口的话非常容易出现403状态码，这个情况我们在前文Cookies池一节也提到过，所以在这里我们实现一个Middleware，为每个Request添加随机的Cookies。

在这之前，我们需要先将Cookies池开启起来，使得API模块正常运行，例如在本地运行5000端口运行的话，访问[http://localhost:5000/weibo/random](http://localhost:5000/weibo/random)即可获取随机的Cookies，当然也可以将Cookies池部署到远程的服务器，这样只需要更改一下访问的链接就好了。

那么在这里我们将Cookies池在本地启动起来，再实现一个Middleware如下：

```python

class CookiesMiddleware():
    def __init__(self, cookies_url):
        self.logger = logging.getLogger(__name__)
        self.cookies_url = cookies_url
    
    def get_random_cookies(self):
        try:
            response = requests.get(self.cookies_url)
            if response.status_code == 200:
                cookies = json.loads(response.text)
                return cookies
        except requests.ConnectionError:
            return False
    
    def process_request(self, request, spider):
        self.logger.debug('正在获取Cookies')
        cookies = self.get_random_cookies()
        if cookies:
            request.cookies = cookies
            self.logger.debug('使用Cookies ' + json.dumps(cookies))
    
    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(
            cookies_url=settings.get('COOKIES_URL')
        )
```

在这里我们首先利用from_crawler()方法获取了COOKIES_URL这个变量，这就是刚才我们所说的接口，接下来我们实现了一个get_random_cookies()方法，这个方法主要就是请求此Cookies池接口然后获取接口返回的随机Cookies，如果成功获取则返回Cookies，否则返回False。

接下来在process_request()方法里我们给request对象的cookies属性赋值，其值就是获取的随机Cookies，这样我们就成功地为每一次请求赋值Cookies了。

如果启用了该Middleware，每个请求都会被赋值上随机的Cookies，这样我们就可以模拟登录之后的请求了，403状态码基本就不会出现了。

#### 代理池对接

另外微博还有一个反爬措施就是检测到同一IP请求量过大的时候就会出现414状态码，所以在这里如果我们遇到这样的情况可以切换代理，所以我们还需要将代理池运行起来，例如在本地5555端口运行的话，获取随机可用代理的地址为[http://localhost:5555/random](http://localhost:5555/random)，访问这个接口即可获取一个随机可用代理，接下来我们再实现一个Middleware，代码如下：

```python
class ProxyMiddleware():
    def __init__(self, proxy_url):
        self.logger = logging.getLogger(__name__)
        self.proxy_url = proxy_url
    
    def get_random_proxy(self):
        try:
            response = requests.get(self.proxy_url)
            if response.status_code == 200:
                proxy = response.text
                return proxy
        except requests.ConnectionError:
            return False
    
    def process_request(self, request, spider):
        if request.meta.get('retry_times'):
            proxy = self.get_random_proxy()
            if proxy:
                uri = 'https://{proxy}'.format(proxy=proxy)
                self.logger.debug('使用代理 ' + proxy)
                request.meta['proxy'] = uri

    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(
            proxy_url=settings.get('PROXY_URL')
        )
```

还是同样的原理，我们实现了一个get_random_proxy()方法用于请求代理池的接口获取随机代理，如果获取成功，则返回改代理，否则返回False。接下来在process_request()方法中给request对象的meta属性赋值一个proxy字段，该字段的值就是代理。

另外我们还注意到赋值代理的判断条件是当前retry_times不为空，也就是说当第一次请求失败之后才启用代理，原因是使用代理后访问速度会慢一些，所以我们在这里设置了一下只有重试的时候才启用代理，否则直接请求，这样就可以保证在没有被封禁的情况下直接请求，保证了爬取速度。

这样代理的Middleware就完成了。

接下来我们还需要在配置文件中启用这两个Middleware，修改settings.py如下：

```python
DOWNLOADER_MIDDLEWARES = {
    'weibo.middlewares.CookiesMiddleware': 554,
    'weibo.middlewares.ProxyMiddleware': 555,
}
```

注意这里的优先级设置，我们在前文中提到了Scrapy的默认Doanloader Middleware的设置是：

```python
{
    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,
}
```

要使得我们自定义的CookiesMiddleware生效，其需要在内置的CookiesMiddleware之前调用，而内置的CookiesMiddleware的优先级为700，所以在这里我们设置一个比700小的数字即可。

要使得我们自定义的ProxyMiddleware生效，其需要在内置的HttpProxyMiddleware之前调用，而内置的HttpProxyMiddleware的优先级为750，所以在这里我们设置一个比750小的数字即可。

## 运行

到此为止，整个微博爬虫就实现完毕了，我们运行如下命令启动一下爬虫：

```
scrapy crawl weibocn
```

类似的输出结果如下：

```python
2017-07-11 17:27:34 [urllib3.connectionpool] DEBUG: http://localhost:5000 "GET /weibo/random HTTP/1.1" 200 339
2017-07-11 17:27:34 [weibo.middlewares] DEBUG: 使用Cookies {"SCF": "AhzwTr_DxIGjgri_dt46_DoPzUqq-PSupu545JdozdHYJ7HyEb4pD3pe05VpbIpVyY1ciKRRWwUgojiO3jYwlBE.", "_T_WM": "8fe0bc1dad068d09b888d8177f1c1218", "SSOLoginState": "1501496388", "M_WEIBOCN_PARAMS": "uicode%3D20000174", "SUHB": "0tKqV4asxqYl4J", "SUB": "_2A250e3QUDeRhGeBM6VYX8y7NwjiIHXVXhBxcrDV6PUJbkdBeLXjckW2fUT8MWloekO4FCWVlIYJGJdGLnA.."}
2017-07-11 17:27:34 [weibocn] DEBUG: <200 https://m.weibo.cn/api/container/getIndex?uid=1742566624&type=uid&value=1742566624&containerid=1005051742566624>
2017-07-11 17:27:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://m.weibo.cn/api/container/getIndex?uid=1742566624&type=uid&value=1742566624&containerid=1005051742566624>
{'avatar': 'https://tva4.sinaimg.cn/crop.0.0.180.180.180/67dd74e0jw1e8qgp5bmzyj2050050aa8.jpg',
 'cover': 'https://tva3.sinaimg.cn/crop.0.0.640.640.640/6ce2240djw1e9oaqhwllzj20hs0hsdir.jpg',
 'crawled_at': '2017-07-11 17:27',
 'description': '成长，就是一个不断觉得以前的自己是个傻逼的过程！QQ:3539129630 '
                '，微信号：sixiangjujiao-weixin',
 'fans_count': 19202906,
 'follows_count': 1599,
 'gender': 'm',
 'id': 1742566624,
 'name': '思想聚焦',
 'verified': True,
 'verified_reason': '微博知名博主，校导网编辑',
 'verified_type': 0,
 'weibos_count': 58393}
```

运行一段时间后我们便可以到MongoDB数据库查看一下数据，爬取下来的数据如下：


![](./assets/2017-08-01-17-53-39.jpg)


![](./assets/2017-08-01-17-52-32.jpg)


本节代码：
[https://github.com/Python3WebSpider/Weibo](https://github.com/Python3WebSpider/Weibo)