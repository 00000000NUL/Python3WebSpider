# 基本库的使用

学习爬虫，最初的操作便是来模拟浏览器向服务器发出一个请求，那么我们需要从哪个地方做起呢？请求需要我们自己来构造吗？我们需要关心请求这个数据结构的实现吗？我们需要了解HTTP、TCP、IP层的网络传输通信吗？我们需要知道服务器的响应和应答原理吗？

可能你不知道无从下手，不用担心，Python的强大之处就是提供了功能齐全的类库来帮助我们完成这些请求，最基础的HTTP库有U
Urllib,Httplib2,Requests,Treq等。

拿Urllib这个库来说，有了它，你只需要关心请求的链接是什么，需要传的参数是什么以及可选的请求头设置就好了，不用深入到底层去了解它到底是怎样来传输和通信的。有了它，你两行代码就可以完成一个请求和响应的处理过程，得到网页内容，是不是感觉方便极了？

接下来，就让我们从最基础的部分开始了解这些库的使用方法吧。



如果你用过python2的urllib库的话，你会发现python3中的库发生了一定的变化，举个例子来说，python2中的基本请求函数urlopen方法，原本是在urlllib2这个库里的，在python3中它被统一移动到了urllib库的request模块中，另外还有一些函数的路径也发生了变化，如果你想从python2迁移到python3，工作量还是不小的。

在后面一节我会介绍一个更加好用的库reqeusts，它其实是基于urllib3库来实现的，不过urllib作为最最基本的请求库，我们还是需要了解一下它的基本用法和原理，还有后面的url解析函数库也是非常实用的。接下来我们就做一下讲解。

首先我们打开notebook文件，在这里我列出了一些基本的urllib函数的用法。我们首先看一下最基础的一个函数，urlopen函数，它可以为帮助我们发送一个request请求给服务器，第一个参数呢就是网站的url，第二个参数是一些额外数据，第三个参数是超时设置，后面的几个参数是CA证书设置，我们暂时用不到，重点看一下前三个参数。

read方法就获取了response的内容，它是bytes类型，我们还需要调用一个decode方法把它转为字符串。

以上内容我们就完成了最基本的Request的构造，它的一些基本参数都已经构建完成了，这样我们就可以完成大部分网站的爬取了。不过还有一些高级用法，比如设置代理，处理Cookie等等，这些就需要一些更加高级的操作了，那么它就需要一些Handler来实现了。

我们查看一下官方文档，

