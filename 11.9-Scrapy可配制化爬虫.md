# Scrapy通用爬虫

使用Scrapy我们可以轻松地完成一个站点爬虫的编写，但如果抓取的站点量非常大的话，比如这里我们需要爬取各大媒体的新闻信息，多个Spider可能包含很多重复代码。

如果我们可以将各个站点的Spider的公共部分保留下来，不同的部分单独提取出来做成单独的配置，如将爬取规则、页面解析方式等抽离出来做成一个配置文件，那么这样我们在新增一个爬虫的时候只需要实现这些网站的爬取规则和提取规则即可。

本节我们就来探究一下Scrapy通用爬虫的实现方法。

## CrawlSpider

在实现通用爬虫之前我们需要先了解一下CrawlSpider，其官方文档链接为：[http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider](http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider)。它是Scrapy提供的一个通用Spider，在Spider里面我们可以通过指定一些爬取规则来实现页面的提取，这些爬取规则有一个专门的数据结构Rule来表示，Rule里面包含了提取和跟进页面的配置，配置好之后，Spider会根据分析Rule来得到当前页面中的哪些链接需要继续爬取，哪些页面的爬取结果需要用哪个方法解析等等。

它是继承自Spider类实现的，除了Spider类中的所有方法和属性，它还各提供了一个非常重要的属性和方法，如下：

* rules，爬取规则属性，它是一个包含一个(或多个) Rule 对象的列表。 每个 Rule 对爬取网站的动作都做了定义，CrawlSpider会读取rules的每一个Rule并进行解析，根据它来对网站进行爬取。

* parse_start_url(response)，这是一个可重写的方法，当start_urls里面对应的Request得到Response时，该方法被调用，该方法会分析Response并必须返回 Item 对象或者Request 对象。

那么在这里最重要的一点莫过于Rule的定义了，它的定义和参数如下：

```python
class scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
```

下面对其参数依次说明：

* link_extractor，是一个 Link Extractor 对象。 利用它Spider可以知道从爬取的页面中提取哪些链接，提取出的链接会自动生成Request，它又是一个数据结构，一般常用LxmlLinkExtractor对象作为参数，其定义和参数如下：

```python
class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=False, unique=True, process_value=None, strip=True)
```

这里allow，是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进，deny则相反。allow_domains则会是定义了符合要求的域名，只有此域名的链接才会被跟进生成新的Request，相当于域名白名单，deny_domains则相反，相当于域名黑名单。restrict_xpaths定义了从当前页面中XPath匹配的区域提取链接，其值是XPath表达式或XPath表达式列表，restrict_css定义了从当前页面中CSS选择器匹配的区域提取链接，其值是CSS选择器或CSS选择器列表。另外还有一些其他的参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高，如果需要使用可以参考文档的参数说明，链接为：[http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml](http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml)。

* callback，即回调函数，和之前定义Request的callback相同的意义。 从link_extractor中每获取到链接时将会调用该函数。该回调函数接收一个response作为其第一个参数， 并返回一个包含 Item 或 Request 对象的列表。值得注意的是避免使用 parse 作为回调函数。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果覆盖了 parse 方法，CrawlSpider 将会运行失败。

* cb_kwargs，是一个字典，它包含了传递给回调函数的参数。

* follow，是一个布尔值，即True或False，它指定了根据该规则从response提取的链接是否需要跟进。 如果 callback 参数为None， follow 默认设置为 True ，否则默认为 False 。

* process_links，指定一个处理函数，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。

* process_request，同样是指定一个处理函数，根据该Rule提取到每个Request时都会调用该函数，用来对Request进行处理。该函数必须返回一个Request或者None。

以上便是CrawlSpider中的核心Rule的基本用法，但只看它的定义可能还不足以利用它来完成一个CrawlSpider爬虫，下面我们用一个实例来利用CrawlSpider实现一个新闻网站的爬取，以此来更好地理解它的用法。

## 目标

这里我们以中华网科技类新闻为例来了解一下CrawlSpider的用法，官网链接为：[http://tech.china.com/](http://tech.china.com/)，我们需要爬取它的科技类新闻内容，链接为：[http://tech.china.com/articles/](http://tech.china.com/articles/)，页面如图所示：

![](./assets/2017-08-10-00-49-15.png)

我们需要将新闻列表中的所有分页的新闻详情抓取下来，主要目的是为了了解CrawlSpider的基本用法。

## 实战

首先我们新建一个Scrapy项目，名称叫做 scrapycrawlspidertest，命令如下：

```
scrapy startproject scrapycrawlspidertest
```

接下来我们创建一个CrawlSpider，创建CrawlSpider需要制定一个模板，在这里我们可以先看下有哪些可用模板，命令如下：

```
scrapy genspider -l
```

运行结果：

```
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
```

在之前我们创建Spider的时候实际上是默认使用了第一个模板basic，这次如果我们要创建CrawlSpider，就需要使用第二个模板crawl，创建命令如下：

```
scrapy genspider -t crawl china tech.china.com
```

运行之后便会生成一个CrawlSpider，其内容如下：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class ChinaSpider(CrawlSpider):
    name = 'china'
    allowed_domains = ['tech.china.com']
    start_urls = ['http://tech.china.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i
```

可以发现这次生成的Spider内容就有所不同了，多了一个rules属性的定义，Rule的第一个参数是LinkExtractor，其实就是上文所说的LxmlLinkExtractor，只是名称不同。同时默认的回调函数也不再是parse，而是parse_item。

要实现新闻的爬取我们需要做的就是定义好Rule，然后实现解析函数就好了，下面我们就来一步步实现这个过程。

首先我们需要先将start_urls修改为起始链接，代码如下：

```python
start_urls = ['http://tech.china.com/articles/']
```

定义好了之后Spider就会爬取start_urls里面的每一个链接，所以这里第一个爬取的页面就是我们刚才所定义的链接，得到Response之后Spider就会根据每一个Rule来去提取这个页面内的超链接去生成进一步的Request，所以我们接下来就需要定义Rule来指定提取哪些链接。

当前页面如图所示：

![](./assets/2017-08-10-00-51-26.png)

这是新闻的列表页，所以很自然地下一步必然是将列表中的每条新闻详情的链接提取出来，我们这里直接指定这些链接所在区域即可，审查源代码可以发现这所有链接都在id为left_side的节点内，具体来说是其内的class为con_item的节点，如图所示：

![](./assets/2017-08-10-00-52-55.jpg)

所以此处我们可以用LinkExtractor的restrict_xpaths属性来指定，指定了之后Spider就会从这个区域提取所有的超链接并生成Request，但是同时注意到每篇文章的导航中可能还有一些其他的超链接标签，但是我们只想把需要的新闻链接提取出来，查看源码后可以观察到真正的新闻链接路径都是以article开头的，我们用一个正则表达式将其匹配出来再赋值给allow参数即可。另外这些链接对应的页面其实就是对应的新闻详情页，而我们需要解析的就是新闻的详情信息，所以此处还需要指定一个回调函数callback。

到现在我们分析了如上页面就可以构造出一个Rule了，代码如下：

```python
Rule(LinkExtractor(allow='article\/.*\.html', restrict_xpaths='//div[@id="left_side"]//div[@class="con_item"]'), callback='parse_item')
```

接下来当前页面我们还需要让它做到分页功能，所以还需要提取下一页的链接，分析网页源码之后可以发现它是在id为pageStyle的节点内，如图所示：

![](./assets/2017-08-10-01-07-09.jpg)

但是这里可以发现我们所需要的下一页节点和其他的分页链接区分度不高，要取出此链接我们可以直接用XPath的文本匹配方式，所以这里我们直接用LinkExtractor的restrict_xpaths属性来指定提取的链接即可。另外此分页链接对应的页面我们不需要像新闻详情页一样去提取详情信息，也就是不需要生成Item，所以不需要加callback参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个follow参数为True，代表继续跟进匹配分析。其实这个follow参数可以加可不加，因为当callback为空的时候，follow默认为True，所以此处Rule可以定义为：

```python
Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
```

所以现在rules就变成了：

```python
rules = (
    Rule(LinkExtractor(allow='article\/.*\.html', restrict_xpaths='//div[@id="left_side"]//div[@class="con_item"]'), callback='parse_item'),
    Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
)
```

接着我们运行一下代码，命令如下：

```
scrapy crawl china
```

即可发现现在已经可以实现页面的翻页和详情页的抓取了，我们仅仅通过定义了两个Rule即实现了这样的功能，运行效果如图所示：

![](./assets/2017-08-10-01-13-55.jpg)

接下来我们需要做的就是解析页面内容了，在这里我们将标题、发布时间、正文、来源提取出来即可，所以这里我们首先定义一个Item如下：

```python
from scrapy import Field, Item

class NewsItem(Item):
    title = Field()
    url = Field()
    text = Field()
    datetime = Field()
    source = Field()
    website = Field()
```

在这里的字段分别指新闻标题、链接、正文、发布时间、来源、站点名称，其中站点名称就直接赋值为中华网即可，因为既然是通用爬虫，肯定还有很多爬虫来爬取也来爬取同样结构的新闻内容，所以需要一个字段来区分一下站点名称。

详情页的预览图如下：

![](./assets/2017-08-10-01-16-34.png)

这里parse_item()方法的实现如下：

```python
def parse_item(self, response):
    item = NewsItem()
    item['title'] = response.xpath('//h1[@id="chan_newsTitle"]/text()').extract_first()
    item['url'] = response.url
    item['text'] = ''.join(response.xpath('//div[@id="chan_newsDetail"]//text()').extract()).strip()
    item['datetime'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('(\d+\-\d+\-\d+\s\d+\:\d+\:\d+)')
    item['source'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('来源：(.*)').strip()
    item['website'] = '中华网'
    yield item
```

提取的具体方法在此不再赘述，这样我们就把每条新闻的信息提取形成了一个NewsItem对象。

此时再运行一下，输出内容如下：

![](./assets/2017-08-10-01-33-01.jpg)

可以看到现在我们就可以成功将每条新闻的信息提取出来了，解析完毕。

最后关于存储在此不再赘述，可以自行定义Pipeline实现。

到此为止我们便通过实现CrawlSpider中的两个Rule和一个parse_item()方法就实现了中华网新闻的爬取，实现非常简洁。

如果我们想要扩展其他新闻站点，只需要配置站点的爬取规则Rule和最终的解析方法即可，Item都是通用的，Spider的结构都是相同的。

至此，我们可以说已经实现了爬虫的半通用化配置。

## 通用配置抽取

为什么说现在做到的这一步是半通用化呢？因为如果我们需要扩展其他站点的话仍然需要创建一个新的CrawlSpider，定义这个站点的Rule，而且还需要单独去实现parse_item()方法，实际上还有很多的代码是重复的，如CrawlSpider的变量、方法名几乎都是一样的，那么我们可不可以把它们共用一份代码，把完全不相同的地方抽离出来，做成可配置文件呢？

当然可以。那我们可以抽离出哪些部分呢？所有的变量均可以抽取，如name、allowed_domains、start_urls、rules等等，我们将这些变量在CrawlSpider初始化的时候赋值即可，所以这时我们就可以新建一个通用的Spider来实现一下，命令如下：

```
scrapy genspider -t crawl universal universal
```

这样我们就新建了一个全新的Spider，名称为universal，接下来我们将刚才所写的Spider内的属性全都抽离出来放到一个配置文件内，配置成一个Json，命名为china.json，放到一个configs文件夹，和spiders文件夹并列，代码如下：

```json
{
  "spider": "universal",
  "website": "中华网科技",
  "type": "新闻",
  "index": "http://tech.china.com/",
  "settings": {
    "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"
  },
  "start_urls": [
    "http://tech.china.com/articles/"
  ],
  "allowed_domains": [
    "tech.china.com"
  ],
  "rules": "(Rule(LinkExtractor(allow='article\/.*\\.html', restrict_xpaths='//div[@id=\"left_side\"]//div[@class=\"con_item\"]'),callback='parse_item'),Rule(LinkExtractor(restrict_xpaths='//div[@id=\"pageStyle\"]//a[contains(., \"下一页\")]')))"
}
```

在这里第一个字段spider即使用的Spider的名称，在这里使用universal，然后是站点的描述比如站点名称、类型、首页等等。随后的settings是该Spider特有的settings配置，如果要覆盖全局项目settings.py内的配置可以单独为其配置，随后是Spider的一些属性，如start_urls、allowed_domains、rules等，在这里rules因为其内涉及一些数据结构，所以在此我们直接定义成字符串的形式，注意将一些特殊字符加上转义符号。

这样我们就算把基本的配置抽取出来了，如果要启动爬虫只需要从该配置文件中读取然后动态加载到Spider中即可，所以我们需要定义一个读取该Json文件的方法，定义如下：

```python
from os.path import realpath, dirname
import json
def get_config(name):
    path = dirname(realpath(__file__)) + '/configs/' + name + '.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.loads(f.read())
```

定义了get_config方法之后我们只需要向其传入Json配置文件的名称即可获取此Json配置信息，随后我们定义一个入口文件run.py，放在项目根目录下，它的作用是启动Spider，定义如下：

```python
import sys
from scrapy.utils.project import get_project_settings
from scrapycrawlspidertest.spiders.universal import UniversalSpider
from scrapycrawlspidertest.utils import get_config
from scrapy.crawler import CrawlerProcess

def run():
    name = sys.argv[1]
    custom_settings = get_config(name)
    # 爬取使用的Spider名称
    spider = custom_settings.get('spider', 'universal')
    project_settings = get_project_settings()
    settings = dict(project_settings.copy())
    # 合并配置
    settings.update(custom_settings.get('settings'))
    process = CrawlerProcess(settings)
    # 启动爬虫
    process.crawl(spider, **{'name': name})
    process.start()

if __name__ == '__main__':
    run()
```

运行入口为run()，首先获取了命令行的参数并赋值为name，name就是Json文件的名称，其实也就相当于要爬取的目标网站的名称，在这里首先利用get_config()方法并传入该名称读取我们刚才定义的配置文件，然后获取爬取使用的spider的名称，随后获取了配置文件中的settings配置，然后和项目全局的settings做了合并，随后新建一个CrawlerProcess，传入爬取使用的配置，随后调用crawl()和start()方法即可启动爬取。

在universal这个Spider中我们新建一个__init__()方法，进行初始化配置，实现如下：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapycrawlspidertest.utils import get_config

class UniversalSpider(CrawlSpider):
    name = 'universal'
    def __init__(self, name, *args, **kwargs):
        config = get_config(name)
        self.config = config
        self.rules = eval(config.get('rules'))
        self.start_urls = config.get('start_urls')
        self.allowed_domains = config.get('allowed_domains')
        super(UniversalSpider, self).__init__(*args, **kwargs)
    
    def parse_item(self, response):
        i = {}
        return i
```

在__init__()方法中赋值了start_urls、allowed_domains、rules等属性，其中rules属性使用eval()方法将字符串转化为对象，这样就成功实现爬虫的基础配置了。

接下来我们可以执行如下命令运行爬虫：

```
python3 run.py china
```

运行此命令后，程序会首先读取Json配置文件，然后将配置中的一些属性赋值给Spider，然后启动爬取，运行效果完全相同，运行结果如下：

![](./assets/2017-08-10-15-59-24.jpg)

现在我们已经把Spider的基础属性实现了可配置化了，剩下的解析部分我们同样需要实现可配置化，原来的解析函数为：

```python
def parse_item(self, response):
    item = NewsItem()
    item['title'] = response.xpath('//h1[@id="chan_newsTitle"]/text()').extract_first()
    item['url'] = response.url
    item['text'] = ''.join(response.xpath('//div[@id="chan_newsDetail"]//text()').extract()).strip()
    item['datetime'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('(\d+\-\d+\-\d+\s\d+\:\d+\:\d+)')
    item['source'] = response.xpath('//div[@id="chan_newsInfo"]/text()').re_first('来源：(.*)').strip()
    item['website'] = '中华网'
    yield item
```

可以观察到解析的每一个字段都是以response为起点的，以title为例：

```python
item['title'] = response.xpath('//h1[@id="chan_newsTitle"]/text()').extract_first()
```

response对象首先调用了xpath()方法，传入XPath表达式，随后调用了extract_first()方法，参数为空，要实现其可配置化我们可以将方法名和方法参数顺次抽离出来，然后链式调用即可。

但是再看text的提取方式：

```python
item['text'] = ''.join(response.xpath('//div[@id="chan_newsDetail"]//text()').extract()).strip()
```

在调用完extract()方法之后会生成一个列表，随后我们调用了字符串的join()方法实现了列表的合并，但此时的调用方式就不同了，它不再是链式调用的方式，而是将解析的结果当做参数传递给jon()方法再得到返回结果，相当于外层包一个函数，我们在此简称为外包调用。join()方法调用完毕后最后还调用了strip()方法去除头尾空白字符，这又变成了链式调用的方式。

所以在抽离配置的时候我们需要兼顾这两种调用方式，在这里我们将链式调用和外包调用命名为chain和wrap，另外我们再将函数名称和参数抽离出来，这样提取配置就会变成如下内容：

```json
"item": {
    "class": "NewsItem",
    "attrs": {
      "title": [
        {
          "method": "xpath",
          "args": [
            "//h1[@id='chan_newsTitle']/text()"
          ]
        },
        {
          "method": "extract_first"
        }
      ],
      "url": [
        {
          "method": "get_attr",
          "args": [
            "url"
          ],
          "type": "wrap"
        }
      ],
      "text": [
        {
          "method": "xpath",
          "args": [
            "//div[@id='chan_newsDetail']//text()"
          ]
        },
        {
          "method": "extract"
        },
        {
          "method": "list2str",
          "type": "wrap"
        },
        {
          "method": "strip"
        }
      ],
      "datetime": [
        {
          "method": "xpath",
          "args": [
            "//div[@id='chan_newsInfo']/text()"
          ]
        },
        {
          "method": "re_first",
          "args": [
            "(\\d+\\-\\d+\\-\\d+\\s\\d+\\:\\d+\\:\\d+)"
          ]
        }
      ],
      "source": [
        {
          "method": "xpath",
          "args": [
            "//div[@id='chan_newsInfo']/text()"
          ]
        },
        {
          "method": "re_first",
          "args": [
            "来源：(.*)"
          ]
        },
        {
          "method": "strip"
        }
      ],
      "website": [
        {
          "method": "set_value",
          "args": [
            "中华网"
          ],
          "type": "wrap"
        }
      ]
    }
  }
```

将此配置加入到Json文件配置内即可，其中这里定义了class属性就是指定了生成Item的类型，attrs即为Item各个属性的提取方式配置。

以title为例，它的配置是一个列表，然后列表的每一个元素都是一个配置字典，如：

```json
{
  "method": "xpath",
  "args": [
    "//h1[@id='chan_newsTitle']/text()"
  ]
},
```

method代表方法名称，args是方法的参数，这样就相当于调用了xpath()方法并传递了args对应的参数，随后还有一个配置为：

```json
{
  "method": "extract_first"
}
```

这里只指定了method，而没有args，即代表没有参数，这样就会直接调用extract_first()方法。

所以如上的配置我们就可以利用某种逻辑将其动态转换为如下代码：

```python
item['title'] = response.xpath('//h1[@id="chan_newsTitle"]/text()').extract_first()
```

再以text的配置为例，我们在前文中提到过了，它有一个join()方法是将列表转为字符串，而且是外包调用的方法，所以在这里我们可以作如下处理，将其配置为：

```json
{
  "method": "list2str",
  "type": "wrap"
}
```

在这里我们指定了一个参数method，其值为list2str，这个方法就是将列表转为字符串的方法，我们可以自行定义，同时args()没有指定，代表没有额外的参数，就是当前对象本身，同时指定了type为wrap，代表是外包类型的调用。

所以最后text的配置我们就可以利用某种逻辑将其动态转换为如下代码：

```python
item['text'] = list2str(response.xpath('//div[@id="chan_newsDetail"]//text()').extract()).strip()
```

这些wrap类型的方法我们可以单独定义一个wraps.py文件，同时可以自行扩展：

```python
def list2str(list):
    """
    列表拼合成字符串
    :param result: 列表
    :return: 字符串
    """
    return ''.join(list)

def set_value(obj, value):
    """
    设置固定数值
    :param value: 数值
    :return: 数值
    """
    return value

def get_attr(obj, attr):
    """
    获取属性
    :param obj: 对象
    :param attr: 属性
    :return: 属性值
    """
    return getattr(obj, attr)
```

接下来基本的配置就完成了，最后最重要的就是实现parse_item()方法，实现如下：

```python
def parse_item(self, response):
    # 获取item配置
    item = self.config.get('item')
    if item:
        data = eval(item.get('class') + '()')
        # 动态获取属性配置
        for key, value in item.get('attrs').items():
            data[key] = response
            for process in value:
                type = process.get('type', 'chain')
                if type == 'chain':
                    # 动态调用函数和属性
                    if process.get('method'):
                        data[key] = getattr(data[key], process.get('method'))(*process.get('args', []))
                elif type == 'wrap':
                    args = [data[key]] + process.get('args', [])
                    data[key] = eval(process.get('method'))(*args)
        yield data
```

在这里首先获取了Item的配置信息，然后获取了class的配置，将其初始化，随后遍历了Item的各个属性依次进行提取。提取开始时首先将其赋值为response对象，然后依次调用配置的方法并传入配置的参数，方法的类型默认会设置为chain类型，即链式调用，通过getattr()方法实现，如果指定为wrap类型则进行外包形式调用，通过eval()方法实现。

循环完毕之后Item的所有属性都会被成功赋值，达到同样的提取效果。

重新运行，结果如下：

![](./assets/2017-08-10-17-52-31.jpg)

另外最后我们再回过头看一下start_urls的配置，在这里start_urls我们只可以配置具体的链接，但如果这些链接是100个，1000个呢？我们总不能将所有的链接全部列出来吧，所以在某些情况下start_urls也需要动态配置，所以这里我们也需要将其分成两类，一种是直接配置URL列表，一种是调用方法生成，我们分别定义为static和dynamic类型。

本例中的start_urls很明显是static类型的，所以将start_urls配置改写如下：

```json
"start_urls": {
  "type": "static",
  "value": [
    "http://tech.china.com/articles/"
  ]
}
```

如果是动态生成的我们可以调用方法传参数，例如：

```json
"start_urls": {
  "type": "dynamic",
  "method": "china",
  "args": [
    5, 10
  ]
}
```

例如这里我们定义为dynamic类型，指定方法为urls_china()，然后传入参数5和10，来生成第5到10页的链接，然后这样我们只需要实现该方法即可，可以统一新建一个urls.py文件，方法实现如下：

```python
def china(start, end):
    for page in range(start, end + 1):
        yield 'http://tech.china.com/articles/index_' + str(page) + '.html'
```

如有新增的可以自行配置，如某些链接需要用到时间戳，加密参数等等，均可通过方法实现。

接下来在Spider的__init__()方法中，start_urls的配置可以改写如下：

```python
from scrapycrawlspidertest import urls

start_urls = config.get('start_urls')
if start_urls:
    if start_urls.get('type') == 'static':
        self.start_urls = start_urls.get('value')
    elif start_urls.get('type') == 'dynamic':
        self.start_urls = list(eval('urls.' + start_urls.get('method'))(*start_urls.get('args', [])))
```

在这里通过判定start_urls的类型分别进行不同的处理，这样我们就可以实现start_urls的配置了。

至此，Spider的设置、起始链接、属性、提取方法都已经实现了全部的可配置化。

综上，整个项目的配置总结如下：
* spider，指定所使用的Spider的名称。
* settings，可以专门为Spider定制配置信息，会覆盖项目级别的配置。
* start_urls，指定爬虫爬取的起始链接。
* allowed_domains，允许爬取的站点。
* rules，站点的爬取规则。
* item，数据的提取规则。

以上就是Scrapy通用爬虫的实现，源代码为：[https://github.com/Python3WebSpider/ScrapyCrawlSpiderTest/tree/master](https://github.com/Python3WebSpider/ScrapyCrawlSpiderTest/tree/master)。

