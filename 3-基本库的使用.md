# 基本库的使用

学习爬虫，最初的操作便是来模拟浏览器向服务器发出一个请求，那么我们需要从哪个地方做起呢？请求需要我们自己来构造吗？我们需要关心请求这个数据结构的实现吗？我们需要了解HTTP、TCP、IP层的网络传输通信吗？我们需要知道服务器的响应和应答原理吗？

可能你不知道无从下手，不用担心，Python的强大之处就是提供了功能齐全的类库来帮助我们完成这些请求，最基础的HTTP库有urllib,httplib2,requests,treq等。

拿urllib这个库来说，有了它，你只需要关心请求的链接是什么，需要传的参数是什么以及可选的请求头设置就好了，不用深入到底层去了解它到底是怎样来传输和通信的。有了它，你两行代码就可以完成一个请求和响应的处理过程，得到网页内容，是不是感觉方便极了？

接下来，就让我们从最基础的部分开始了解这些库的使用方法吧。

我们在上节学习了基本的爬虫原理，需要向服务器发送一个请求，通过获取响应内容就可以得到网页的源代码了，那么这个过程怎样来实现呢？Python的Urllib库就为我们实现了这个过程，它是Python内置的HTTP请求库，有了它，你只需要关心请求的链接是什么，需要传的参数是什么以及请求头设置就好了，不用深入到底层去了解它到底是怎样来传输和通信的。另外urllib还提供了比较强大的url解析函数库，在本节课我们统一做一下讲解。

我们首先了解一下urllib库，它是python内置的HTTP请求库，也就是说你不需要额外安装，安装好python之后直接拿来用就好了，它包含四个模块，第一个模块request，它是最基本的http请求模块，我们可以用它来模拟发送一请求，就像在浏览器里输入网址然后敲击回车一样，只需要给库方法传入url还有额外的参数，就可以模拟实现这个过程了。第二个模块异常处理，如果出现请求错误，你可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。第三个parse模块是一个工具模块，提供了许多url处理方法，比如拆分，合并等等的方法。第四个模块是robot
parser，主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬的，其实用的比较少。重点对前三个模块进行下讲解。

如果你用过python2的urllib库的话，你会发现python3中的库发生了一定的变化，举个例子来说，python2中的基本请求函数urlopen方法，原本是在urlllib2这个库里的，在python3中它被统一移动到了urllib库的request模块中，另外还有一些函数的路径也发生了变化，如果你想从python2迁移到python3，工作量还是不小的。

在后面一节我会介绍一个更加好用的库reqeusts，它其实是基于urllib3库来实现的，不过urllib作为最最基本的请求库，我们还是需要了解一下它的基本用法和原理，还有后面的url解析函数库也是非常实用的。接下来我们就做一下讲解。

首先我们打开notebook文件，在这里我列出了一些基本的urllib函数的用法。我们首先看一下最基础的一个函数，urlopen函数，它可以为帮助我们发送一个request请求给服务器，第一个参数呢就是网站的url，第二个参数是一些额外数据，第三个参数是超时设置，后面的几个参数是CA证书设置，我们暂时用不到，重点看一下前三个参数。

read方法就获取了response的内容，它是bytes类型，我们还需要调用一个decode方法把它转为字符串。

以上内容我们就完成了最基本的Request的构造，它的一些基本参数都已经构建完成了，这样我们就可以完成大部分网站的爬取了。不过还有一些高级用法，比如设置代理，处理Cookie等等，这些就需要一些更加高级的操作了，那么它就需要一些Handler来实现了。

我们查看一下官方文档，

