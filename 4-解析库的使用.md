# 第四章 解析库的使用

上一节我们实现了一个最基本的爬虫，但提取页面信息时我们使用的是正则表达式，用过之后我们会发现构造一个正则表达式还是比较的繁琐的，而且万一有一点地方写错了就可能会导致匹配失败，所以使用正则来提取页面信息多多少少还是有些不方便的。

对于网页的节点来说，它可以定义 id、class 或其他的属性，而且节点之间还具有层次关系，在网页中可以通过 XPath 或 CSS 选择器来定位一个或多个节点。那么在页面解析时，我们利用 XPath 或 CSS 选择器来提取到某个节点，然后再调用相应的方法去获取它的正文内容或者属性不就可以提取我们想要的任意信息了吗？

在 Python 中，我们怎样来实现这个操作呢？不用担心，这种解析库已经非常多了，其中比较强大的库有 LXML、BeautifulSoup、PyQuery 等等，本章我们就来介绍一下这三个解析库的使用，有了它们，我们不用再为正则发愁，而且解析效率也会大大提高，实为爬虫必备利器。