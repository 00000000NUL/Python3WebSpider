# ScrapyRedis源码解析

ScrapyRedis这个库已经为我们提供了Scrapy分布式的队列、调度器、去重等功能，其GitHub地址为：[https://github.com/rmax/scrapy-redis](https://github.com/rmax/scrapy-redis)。

本节我们来分析一下它的源码，深入了解一下利用Redis怎样实现Scrapy分布式。

首先我们可以把源码Clone下来，执行如下命令：

```
git clone https://github.com/rmax/scrapy-redis.git
```

核心源码在 scrapy-redis/src/scrapy_redis 目录下。

## 爬取队列

首先我们从爬取队列入手，看下它的具体实现，源码文件为queue.py，在这里它有三个队列的实现，首先它实现了一个父类Base，提供一些基本方法和属性：

```python
class Base(object):
    """Per-spider base queue class"""
    def __init__(self, server, spider, key, serializer=None):
        if serializer is None:
            serializer = picklecompat
        if not hasattr(serializer, 'loads'):
            raise TypeError("serializer does not implement 'loads' function: %r"
                            % serializer)
        if not hasattr(serializer, 'dumps'):
            raise TypeError("serializer '%s' does not implement 'dumps' function: %r"
                            % serializer)
        self.server = server
        self.spider = spider
        self.key = key % {'spider': spider.name}
        self.serializer = serializer

    def _encode_request(self, request):
        obj = request_to_dict(request, self.spider)
        return self.serializer.dumps(obj)

    def _decode_request(self, encoded_request):
        obj = self.serializer.loads(encoded_request)
        return request_from_dict(obj, self.spider)

    def __len__(self):
        """Return the length of the queue"""
        raise NotImplementedError

    def push(self, request):
        """Push a request"""
        raise NotImplementedError

    def pop(self, timeout=0):
        """Pop a request"""
        raise NotImplementedError

    def clear(self):
        """Clear queue/stack"""
        self.server.delete(self.key)
```

首先看一下_encode_request()和_decode_request()方法，因为我们需要把一个Request对象存储到数据库中，但数据库无法直接存储对象，所以需要将Request序列化转成字符串再存储，而这两个方法就分别是序列化和反序列化的操作，利用pickle库来实现，一般在调用push()将Request存入数据库时会调用_encode_request()方法进行序列化，在调用pop()取出Request的时候会调用_decode_request()进行反序列化。

在父类中__len__()、push()和pop()方法都是未实现的，会直接抛出NotImplementedError，因此这个类是不能直接被使用的，所以必须要实现一个子类来重写这三个方法，而不同的子类就会有不同的实现，也就有着不同的功能。

那么接下来就需要定义一些子类来继承Base类，并重写这几个方法，那在源码中就有三个子类的实现，它们分别是FifoQueue、PriorityQueue、LifoQueue，我们分别来看下它们的实现原理。

首先是FifoQueue：

```python
class FifoQueue(Base):
    """Per-spider FIFO queue"""

    def __len__(self):
        """Return the length of the queue"""
        return self.server.llen(self.key)

    def push(self, request):
        """Push a request"""
        self.server.lpush(self.key, self._encode_request(request))

    def pop(self, timeout=0):
        """Pop a request"""
        if timeout > 0:
            data = self.server.brpop(self.key, timeout)
            if isinstance(data, tuple):
                data = data[1]
        else:
            data = self.server.rpop(self.key)
        if data:
            return self._decode_request(data)
```

可以看到这个类继承了Base类，并重写了__len__()、push()、pop()这三个方法，在这三个方法中都是对server对象的操作，而server对象就是一个Redis连接对象，我们可以直接调用其操作Redis的方法对数据库进行操作，可以看到这里的操作方法有llen()、lpush()、rpop()等，那这就代表此爬取队列是使用的Redis的列表List，序列化后的Request会被存入列表中，就是列表的其中一个元素，__len__()方法是获取列表的长度，push()方法中调用了lpush()操作，这代表从列表左侧存入数据，pop()方法中调用了rpop()操作，这代表从列表右侧取出数据。

所以Request在列表中的存取顺序是左侧进、右侧出，所以这是有序的进出，即先进先出，英文叫做First Input First Output，也被简称作Fifo，而此类的名称就叫做FifoQueue。

另外还有一个与之相反的实现类，叫做LifoQueue，实现如下：

```python
class LifoQueue(Base):
    """Per-spider LIFO queue."""

    def __len__(self):
        """Return the length of the stack"""
        return self.server.llen(self.key)

    def push(self, request):
        """Push a request"""
        self.server.lpush(self.key, self._encode_request(request))

    def pop(self, timeout=0):
        """Pop a request"""
        if timeout > 0:
            data = self.server.blpop(self.key, timeout)
            if isinstance(data, tuple):
                data = data[1]
        else:
            data = self.server.lpop(self.key)

        if data:
            return self._decode_request(data)
```

与FifoQueue不同的就是它的pop()方法，在这里使用的是lpop()操作，也就是从左侧出，而push()方法依然是使用的lpush()操作，是从左侧入。那么这样达到的效果就是先进后出、后进先出，英文叫做Last In First Out，简称为Lifo，而此类名称就叫做LifoQueue。同时这个存取方式类似栈的操作，所以其实也可以称作StackQueue。

另外在源码中还有一个子类实现，叫做PriorityQueue，顾名思义，它叫做优先级队列，实现如下：

```python
class PriorityQueue(Base):
    """Per-spider priority queue abstraction using redis' sorted set"""

    def __len__(self):
        """Return the length of the queue"""
        return self.server.zcard(self.key)

    def push(self, request):
        """Push a request"""
        data = self._encode_request(request)
        score = -request.priority
        self.server.execute_command('ZADD', self.key, score, data)

    def pop(self, timeout=0):
        """
        Pop a request
        timeout not support in this queue class
        """
        pipe = self.server.pipeline()
        pipe.multi()
        pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0)
        results, count = pipe.execute()
        if results:
            return self._decode_request(results[0])
```

在这里我们可以看到__len__()、push()、pop()方法中使用了server对象的zcard()、zadd()、zrange()操作，可以知道这里使用的存储结果是有序集合Sorted Set，在这个集合中每个元素都可以设置一个分数，那么这个分数就代表优先级。

在__len__()方法里调用了zcard()操作，返回的就是有序集合的大小，也就是爬取队列的长度，在push()方法中调用了zadd()操作，就是向集合中添加元素，这里的分数指定成Request的优先级的相反数，因为分数低的会排在集合的前面，所以这里高优先级的Request就会存在集合的最前面。pop()方法是首先调用了zrange()操作取出了集合的第一个元素，因为最高优先级的Request会存在集合最前面，所以第一个元素就是最高优先级的Request，然后再调用zremrangebyrank()操作将这个元素删除，这样就完成了取出并删除的操作。

此队列是默认使用的队列，也就是爬取队列默认是使用有序集合来存储的。

## 去重过滤

我们在前面说过Scrapy中的去重实现就是利用集合这个数据结构，但是在Scrapy分布式中去重就需要利用一个共享的集合了，那么在这里使用的就是Redis中的集合数据结构，我们来看下它的去重类是怎样实现的，源码文件是dupefilter.py，其内实现了一个RFPDupeFilter类，实现如下：

```python
class RFPDupeFilter(BaseDupeFilter):
    """Redis-based request duplicates filter.
    This class can also be used with default Scrapy's scheduler.
    """
    logger = logger
    def __init__(self, server, key, debug=False):
        """Initialize the duplicates filter.
        Parameters
        ----------
        server : redis.StrictRedis
            The redis server instance.
        key : str
            Redis key Where to store fingerprints.
        debug : bool, optional
            Whether to log filtered requests.
        """
        self.server = server
        self.key = key
        self.debug = debug
        self.logdupes = True

    @classmethod
    def from_settings(cls, settings):
        """Returns an instance from given settings.
        This uses by default the key ``dupefilter:<timestamp>``. When using the
        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as
        it needs to pass the spider name in the key.
        Parameters
        ----------
        settings : scrapy.settings.Settings
        Returns
        -------
        RFPDupeFilter
            A RFPDupeFilter instance.
        """
        server = get_redis_from_settings(settings)
        key = defaults.DUPEFILTER_KEY % {'timestamp': int(time.time())}
        debug = settings.getbool('DUPEFILTER_DEBUG')
        return cls(server, key=key, debug=debug)

    @classmethod
    def from_crawler(cls, crawler):
        """Returns instance from crawler.
        Parameters
        ----------
        crawler : scrapy.crawler.Crawler
        Returns
        -------
        RFPDupeFilter
            Instance of RFPDupeFilter.
        """
        return cls.from_settings(crawler.settings)

    def request_seen(self, request):
        """Returns True if request was already seen.
        Parameters
        ----------
        request : scrapy.http.Request
        Returns
        -------
        bool
        """
        fp = self.request_fingerprint(request)
        added = self.server.sadd(self.key, fp)
        return added == 0

    def request_fingerprint(self, request):
        """Returns a fingerprint for a given request.
        Parameters
        ----------
        request : scrapy.http.Request

        Returns
        -------
        str

        """
        return request_fingerprint(request)

    def close(self, reason=''):
        """Delete data on close. Called by Scrapy's scheduler.
        Parameters
        ----------
        reason : str, optional
        """
        self.clear()

    def clear(self):
        """Clears fingerprints data."""
        self.server.delete(self.key)

    def log(self, request, spider):
        """Logs given request.
        Parameters
        ----------
        request : scrapy.http.Request
        spider : scrapy.spiders.Spider
        """
        if self.debug:
            msg = "Filtered duplicate request: %(request)s"
            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
        elif self.logdupes:
            msg = ("Filtered duplicate request %(request)s"
                   " - no more duplicates will be shown"
                   " (see DUPEFILTER_DEBUG to show all duplicates)")
            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
            self.logdupes = False
```

在这里我们注意到同样实现了一个request_seen()方法，和Scrapy中的request_seen()方法实现极其类似，不过在这里集合使用的是server对象的sadd()操作，也就是集合不再是简单的一个简单数据结构了，在这里直接换成了数据库的存储方式。

鉴别重复的方式还是使用指纹，而指纹的获取同样是使用request_fingerprint()方法完成的。获取指纹之后就直接尝试向集合中添加这个指纹，如果添加成功，那么就代表这个指纹原本不存在于集合中，返回值就是1，而最后的返回结果是判定添加结果是否为0，如果为1，那这个判定结果就是False，也就是不重复，否则判定为重复。

这样我们就成功利用Redis的集合完成了指纹的记录和重复的验证。

## 调度器

ScrapyRedis还帮我们实现了一个配合Queue、DupeFilter使用的调度器Scheduler，源文件名称是scheduler.py。
在这里指定了一些配置，如SCHEDULER_FLUSH_ON_START即是否在爬取开始的时候清空爬取队列，SCHEDULER_PERSIST即是否在爬取结束后保持爬取队列不清除等，我们可以在settings.py里面自由配置，而此调度器很好的实现了对接。

接下来我们再看下两个核心的存取方法，实现如下：

```python
def enqueue_request(self, request):
    if not request.dont_filter and self.df.request_seen(request):
        self.df.log(request, self.spider)
        return False
    if self.stats:
        self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider)
    self.queue.push(request)
    return True

def next_request(self):
    block_pop_timeout = self.idle_before_close
    request = self.queue.pop(block_pop_timeout)
    if request and self.stats:
        self.stats.inc_value('scheduler/dequeued/redis', spider=self.spider)
    return request
```

enqueue_request()就是调度器向队列中添加Request，在这里做的核心操作就是调用Queue的push()操作，同时还有一些统计和日志操作，next_request()就是从队列中取Request，核心操作就是调用Queue的pop()操作，那么此时如果队列中还有Request，则会直接取出来，接着爬取，否则当队列为空时，则会重新开始爬取。

那么到现在为止我们就把之前所说的三个分布式的问题解决了，总结如下：
* 爬取队列的实现，在这里提供了三种队列，使用了Redis的列表或有序集合来维护。
* 去重的实现，使用了Redis的集合来保存Request的指纹来提供重复过滤。
* 中断后重新爬取的实现，中断后Redis的队列没有清空，再次启动时调度器的next_request()会从队列中取到下一个Request，继续爬取。

以上便是ScrapyRedis的核心源码解析，另外ScrapyRedis中还提供了Spider、Item Pipeline的实现，不过并不是必须要使用的，如有兴趣可以研究。

在下一节，我们会将ScrapyRedis集成到之前我们所实现的Scrapy新浪微博项目中，实现多台主机协同爬取。
