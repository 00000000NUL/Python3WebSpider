# PySpider用法详解

前面我们了解了PySpider的基本用法，我们通过非常少的代码和便捷的可视化操作就完成了一个爬虫的编写，本节我们来总结一下它的详细用法。

## 命令行

前面的实例中启动PySpider是通过如下命令启动的：

```
pyspider all
```

其实在启动的时候还有很多可配制参数，完整的命令行结构是这样的：

```
pyspider [OPTIONS] COMMAND [ARGS]
```

其中OPTIONS即为可配制列表，它可以指定如下参数：

```
Options:
  -c, --config FILENAME    指定配置文件名称
  --logging-config TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf
  --debug                  开启调试模式
  --queue-maxsize INTEGER  队列的最大长度
  --taskdb TEXT            taskdb的数据库连接字符串, 默认: sqlite
  --projectdb TEXT         projectdb的数据库连接字符串, 默认: sqlite
  --resultdb TEXT          resultdb的数据库连接字符串, 默认: sqlite
  --message-queue TEXT     消息队列连接字符串，默认: multiprocessing.Queue
  --phantomjs-proxy TEXT   PhantomJS使用的代理，ip:port 的形式
  --data-path TEXT         数据库存放的路径
  --version                PySpider的版本
  --help                   显示帮助信息
```

例如 -c 可以指定配置文件的名称，这是一个非常常用的配置，配置文件的样例结构如下：

```json
{
  "taskdb": "mysql+taskdb://username:password@host:port/taskdb",
  "projectdb": "mysql+projectdb://username:password@host:port/projectdb",
  "resultdb": "mysql+resultdb://username:password@host:port/resultdb",
  "message_queue": "amqp://username:password@host:port/%2F",
  "webui": {
    "username": "some_name",
    "password": "some_passwd",
    "need-auth": true
  }
}
```

比如如果我们要配置PySpider WebUI的访问认证，可以新建一个 pyspider.json，内容如下：

```json
{
  "webui": {
    "username": "root",
    "password": "123456",
    "need-auth": true
  }
}
```

这样我们通过在启动时指定配置文件即可配置PySpider WebUI的访问认证，用户名为root，密码为123456，命令如下：

```
pyspider -c pyspider.json all
```

运行之后打开[http://localhost:5000/](http://localhost:5000/)


![](./assets/2017-08-13-17-34-26.png)

另外我们也可以单独运行PySpider的某一个组件。

运行Scheduler的命令如下：

```
pyspider scheduler [OPTIONS]
```

运行时也可以指定各种配置，参数如下：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --inqueue-limit INTEGER  任务队列的最大长度，如果满了则新的任务会被忽略
  --delete-time INTEGER    设置为delete标记之前的删除时间
  --active-tasks INTEGER   当前活跃任务数量配置
  --loop-limit INTEGER     单轮最多调度的任务数量
  --scheduler-cls TEXT     Scheduler使用的类
  --help                   显示帮助信息
```

运行Fetcher的命令如下：

```
pyspider fetcher [OPTIONS]
```

参数配置如下：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --poolsize INTEGER      同时请求的个数
  --proxy TEXT            使用的代理
  --user-agent TEXT       使用的User-Agent
  --timeout TEXT          超时时间
  --fetcher-cls TEXT      Fetcher使用的类
  --help                  显示帮助信息
```


运行Processer的命令如下：

```
pyspider processor [OPTIONS]
```

参数配置如下：

```
Options:
  --processor-cls TEXT  Processor使用的类
  --help                显示帮助信息
```

运行WebUI的命令如下：

```
pyspider webui [OPTIONS]
```

参数配置如下：

```
Options:
  --host TEXT            运行地址
  --port INTEGER         运行端口
  --cdn TEXT             JS和CSS的CDN服务器
  --scheduler-rpc TEXT   Scheduler的xmlrpc路径
  --fetcher-rpc TEXT     Fetcher的xmlrpc路径
  --max-rate FLOAT       每个项目最大的rate值
  --max-burst FLOAT      每个项目最大的burst值
  --username TEXT        Auth验证的用户名
  --password TEXT        Auth验证的密码
  --need-auth            是否需要验证
  --webui-instance TEXT  运行时使用的Flask应用
  --help                 显示帮助信息
```

这里的配置和前面提到的配置文件参数是相同的，如我们想要改变WebUI的端口，该为5001，单独运行命令如下：

```
pyspider webui --port 5001
```

或者可以将其配置到Json文件中，配置如下：

```json
{
  "webui": {
    "port": 5001
  }
}
```

使用如下命令启动同样可以达到相同的效果：

```
pyspider -c pyspider.json webui
```

这样就可以在5001端口上运行WebUI了。

## crawl()方法

在前面的例子中我们使用crawl()方法实现了新请求的生成，但是只是指定了URL和Callback，在这里详细介绍一下它的参数配置。

### url

爬取时的URL，也可以定义成URL列表。

### callback

回调函数，即该URL对应的Response内容用哪个方法来解析，例如：

```python
def on_start(self):
    self.crawl('http://scrapy.org/', callback=self.index_page)
```

这里指定了callback为index_page，就代表爬取[http://scrapy.org/]('http://scrapy.org/)链接得到的Response会用index_page来解析。

index_page方法的第一个参数需要是 Response 对象，例如：

```python
def index_page(self, response):
    pass
```

方法中的response参数就是请求上述URL得到的Response对象，我们可以直接在index_page()方法中实现页面的解析。

### age

任务的有效时间，如果某个任务在有效时间内且已经被执行，则不会重复执行，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=10*24*60*60)
```

或者可以这样设置：

```python
@config(age=10 * 24 * 60 * 60)
def callback(self):
    pass
```

默认的有效时间为10天。

### priority

爬取任务的优先级，默认是0，数字越大会越优先被调度，例如：

```python
def index_page(self):
    self.crawl('http://www.example.org/page.html', callback=self.index_page)
    self.crawl('http://www.example.org/233.html', callback=self.detail_page,
               priority=1)
```

这样第二个任务就会被优先调用，优先爬取 233.html 这个链接。

### exetime

此参数可以设置定时任务，其值是时间戳，默认是0，即代表立即执行，实例如下：

```python
import time
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               exetime=time.time()+30*60)
```

这样定义之后该任务会在30分钟之后再执行。

### retries

可以定义重试次数，默认是3次。

### itag

该参数设置一个判定网页是否发生变化的节点值，在下次爬取时会判定次节点是否和上次相同，如果相同，则证明页面没有更新，则不会重复爬取。实例如下：

```python
def index_page(self, response):
    for item in response.doc('.item').items():
        self.crawl(item.find('a').attr.url, callback=self.detail_page,
                   itag=item.find('.update-time').text())
```

在这里设置了更新时间这个节点的值为itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。

### auto_recrawl

当开启时，任务在过期时会重新爬取，循环时间即定义的age时间长度，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=5*60*60, auto_recrawl=True)
```

在这里定义了age有效期为5小时，同时设置了auto_recrawl为True，这样任务就会每5小时执行一次。

### method

HTTP请求方式，默认是GET。

### params

GET请求参数，在这里可以方便地使用params定义GET请求参数，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback,
               params={'a': 123, 'b': 'c'})
    self.crawl('http://httpbin.org/get?a=123&b=c', callback=self.callback)
```

这里两个爬取任务是等价的。

### data

POST表单数据，当请求方式为POST时，可以通过此参数传递表单数据，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', data={'a': 123, 'b': 'c'})
```

### files

上传的文件，在这里需要指定文件名，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', files={field: {filename: 'content'}})
```

### user_agent

即爬取使用的User-Agent。

### headers

爬取时使用的Headers，即Request Headers。

### cookies

爬取时使用的Cookies，需要是字典格式。

### connect_timeout

在初始化连接时最长等待时间，默认是20秒。

### timeout

抓取网页时的最长等待时间，默认是120秒。

### allow_redirects

是否自动处理重定向，默认是True。

### validate_cert

是否验证证书，此选项对HTTPS请求有效，默认是True。

### proxy

爬取时使用的代理，支持用户名密码的配置，格式如 username:password@hostname:port，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback, proxy='127.0.0.1:9743')
```

也可以设置全局配置，如：

```python
class Handler(BaseHandler):
    crawl_config = {
        'proxy': '127.0.0.1:9743'
    }
```

### fetch_type

开启PhantomJS渲染，如果遇到JavaScript渲染的页面，抓取时如果指定此字段即可实现PhantomJS的对接，PySpider将会使用PhantomJS进行网页的抓取，实例如下：

```python
def on_start(self):
    self.crawl('https://www.taobao.com', callback=self.index_page, fetch_type='js')
```

这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。

### js_script

在页面加载完毕后执行的JavaScript脚本，如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               fetch_type='js', js_script='''
               function() {
                   window.scrollTo(0,document.body.scrollHeight);
                   return 123;
               }
               ''')
```

这样就可以在页面加载成功后执行页面混动的JavaScript代码，页面会下拉到最底部。

### js_run_at

JavaScript脚本运行的为止，是在页面节点开头还是结尾，默认是结尾，即document-end。

### js_viewport_width/js_viewport_height

JavaScript渲染页面时的窗口大小。

### load_images

在加载JavaScript页面时是否加载图片，默认是否。

### save

此参数也非常有用，可以在不同的方法之间传递参数，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               save={'page': 1})

def callback(self, response):
    return response.save['page']
```

这样在on_start()方法中生成了Request并传递了额外的参数page，在回调函数里可以通过 response的save对象接收到。

### cancel

取消任务，如果是一个ACTIVE状态的任务，则需要将force_update设置为True。

### force_update

即使任务处于ACTIVE状态，那也会强制更新状态。

以上便是crawl()方法的参数介绍，更加详细的描述可以参考：[http://docs.pyspider.org/en/latest/apis/self.crawl/](http://docs.pyspider.org/en/latest/apis/self.crawl/)。


## 任务区分

在PySpider判断两个任务是否是重复的是使用的是URL的MD5值作为任务的ID，如果ID相同，那么就会判定为任务相同，其中一个就不会爬取了。但是很多情况下我们可能请求的链接是同一个，但是仅仅POST的参数不同，这时可以重写task_id()方法，改变这个ID的计算方式，实例如下：

```python
import json
from pyspider.libs.utils import md5string
def get_taskid(self, task):
    return md5string(task['url']+json.dumps(task['fetch'].get('data', '')))
```

这里就重写了get_taskid()方法，利用URL和POST的参数来生成ID，这样即使URL相同，但是POST的参数不同，两个任务的ID就不同，不会被识别成重复任务。

## 全局配置

在PySpider中可以使用crawl_config来指定全局的配置，配置中的参数会和crawl()方法创建任务时的参数合并，如我们要全局配置一个Headers，那么可以定义如下：

```python
class Handler(BaseHandler):
    crawl_config = {
        'headers': {
            'User-Agent': 'GoogleBot',
        }
    }
```

## 定时爬取

我们可以通过设置every属性来设置多久的时间间隔爬取一次，示例如下：

```python
@every(minutes=24 * 60)
def on_start(self):
    for url in urllist:
        self.crawl(url, callback=self.index_page)
```

这样就可以设置每天执行一次爬取。

但是在上文中我们提到了任务的有效时间，如果在有效时间内则不会重复爬取，所以我们需要把有效时间设置得比重复时间更短才可以实现定时爬取。

比如下面的例子就无法做到每天爬取：

```python
@every(minutes=24 * 60)
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.index_page)

@config(age=10 * 24 * 60 * 60)
def index_page(self):
    pass
```

在这里任务的过期时间为10天，但是自动爬取的时间为1天，当第二次尝试重新爬取的时候，PySpider会监测到此任务尚未过期，所以不会执行爬取，所以我们需要将age设置得小于定时时间。

## 项目状态

每个项目都有6个状态，分别是TODO、STOP、CHECKING、DEBUG、RUNNING和PAUSE。

* TODO，是项目刚刚被创建还未实现时的状态。
* STOP，如果我们想停止此项目的抓取可以将其设置为STOP状态。
* CHECKING，当一个正在运行的项目被修改后就会变成此状态，一般是在中途发现错误需要调整的时候会遇到。
* DEBUG／RUNNING，这两个状态对项目本身运行没有区别，设置为任意一个状态都可以运行项目，但是对于我们来说可以用二者来区分项目是否已经测试通过。
* PAUSE，当爬取过程中出现连续多次错误时会自动设置为PAUSE状态并等待一定的时间继续爬取。

## 抓取进度

在抓取时可以看到抓取的进度，在progress部分会显示四个进度条，如图所示：

![](./assets/2017-08-08-02-46-46.jpg)

process中的5m、1h、1d指的是最近5分钟、1小时、1天内的请求情况，all代表所有的请求情况。

请求在这里有颜色表示，蓝色的代表等待被执行的任务，绿色代表成功的任务、黄色代表请求失败后等待重试的任务、红色代表失败次数过多而被忽略的任务，在这里都可以直观地看到爬取的进度和请求情况。

## 删除项目

PySpider中没有直接删除项目的选项，要删除任务需要将项目的状态设置为STOP，然后将分组的名称设置为delete，然后等待24小时后，项目会自动删除。

以上便是PySpider的常用用法，如要了解更多可以参考PySpider的官方文档：[http://docs.pyspider.org/](http://docs.pyspider.org/)。


