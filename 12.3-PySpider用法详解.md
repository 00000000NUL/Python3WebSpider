# 12.3 PySpider用法详解

前面我们了解了 PySpider 的基本用法，我们通过非常少的代码和便捷的可视化操作就完成了一个爬虫的编写，本节我们来总结一下它的详细用法。

### 1. 命令行

前面的实例中启动 PySpider 是通过如下命令启动的：

```
pyspider all
```

其实在启动的时候还有很多可配制参数，完整的命令行结构是这样的：

```
pyspider [OPTIONS] COMMAND [ARGS]
```

其中 OPTIONS 即为可配制列表，它可以指定如下参数：

```
Options:
  -c, --config FILENAME    指定配置文件名称
  --logging-config TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf
  --debug                  开启调试模式
  --queue-maxsize INTEGER  队列的最大长度
  --taskdb TEXT            taskdb的数据库连接字符串, 默认: sqlite
  --projectdb TEXT         projectdb的数据库连接字符串, 默认: sqlite
  --resultdb TEXT          resultdb的数据库连接字符串, 默认: sqlite
  --message-queue TEXT     消息队列连接字符串，默认: multiprocessing.Queue
  --phantomjs-proxy TEXT   PhantomJS使用的代理，ip:port 的形式
  --data-path TEXT         数据库存放的路径
  --version                PySpider的版本
  --help                   显示帮助信息
```

例如 -c 可以指定配置文件的名称，这是一个非常常用的配置，配置文件的样例结构如下：

```json
{
  "taskdb": "mysql+taskdb://username:password@host:port/taskdb",
  "projectdb": "mysql+projectdb://username:password@host:port/projectdb",
  "resultdb": "mysql+resultdb://username:password@host:port/resultdb",
  "message_queue": "amqp://username:password@host:port/%2F",
  "webui": {
    "username": "some_name",
    "password": "some_passwd",
    "need-auth": true
  }
}
```

比如如果我们要配置 PySpider WebUI 的访问认证，可以新建一个 pyspider.json，内容如下：

```json
{
  "webui": {
    "username": "root",
    "password": "123456",
    "need-auth": true
  }
}
```

这样我们通过在启动时指定配置文件即可配置 PySpider WebUI 的访问认证，用户名为 root，密码为 123456，命令如下：

```
pyspider -c pyspider.json all
```

运行之后打开：[http://localhost:5000/](http://localhost:5000/)，页面如 12-26 所示：

![](./pictures/12-26.png)

图 12-26 运行页面

另外我们也可以单独运行 PySpider 的某一个组件。

运行 Scheduler 的命令如下：

```
pyspider scheduler [OPTIONS]
```

运行时也可以指定各种配置，参数如下：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --inqueue-limit INTEGER  任务队列的最大长度，如果满了则新的任务会被忽略
  --delete-time INTEGER    设置为delete标记之前的删除时间
  --active-tasks INTEGER   当前活跃任务数量配置
  --loop-limit INTEGER     单轮最多调度的任务数量
  --scheduler-cls TEXT     Scheduler使用的类
  --help                   显示帮助信息
```

运行 Fetcher 的命令如下：

```
pyspider fetcher [OPTIONS]
```

参数配置如下：

```
Options:
  --xmlrpc / --no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --poolsize INTEGER      同时请求的个数
  --proxy TEXT            使用的代理
  --user-agent TEXT       使用的User-Agent
  --timeout TEXT          超时时间
  --fetcher-cls TEXT      Fetcher使用的类
  --help                  显示帮助信息
```


运行 Processer 的命令如下：

```
pyspider processor [OPTIONS]
```

参数配置如下：

```
Options:
  --processor-cls TEXT  Processor使用的类
  --help                显示帮助信息
```

运行 WebUI 的命令如下：

```
pyspider webui [OPTIONS]
```

参数配置如下：

```
Options:
  --host TEXT            运行地址
  --port INTEGER         运行端口
  --cdn TEXT             JS和CSS的CDN服务器
  --scheduler-rpc TEXT   Scheduler的xmlrpc路径
  --fetcher-rpc TEXT     Fetcher的xmlrpc路径
  --max-rate FLOAT       每个项目最大的rate值
  --max-burst FLOAT      每个项目最大的burst值
  --username TEXT        Auth验证的用户名
  --password TEXT        Auth验证的密码
  --need-auth            是否需要验证
  --webui-instance TEXT  运行时使用的Flask应用
  --help                 显示帮助信息
```

这里的配置和前面提到的配置文件参数是相同的，如我们想要改变 WebUI 的端口，该为 5001，单独运行命令如下：

```
pyspider webui --port 5001
```

或者可以将其配置到 Json 文件中，配置如下：

```json
{
  "webui": {
    "port": 5001
  }
}
```

使用如下命令启动同样可以达到相同的效果：

```
pyspider -c pyspider.json webui
```

这样就可以在 5001 端口上运行 WebUI 了。

### 2. crawl()方法

在前面的例子中我们使用 crawl() 方法实现了新请求的生成，但是只是指定了 URL 和 Callback，在这里详细介绍一下它的参数配置。

#### url

爬取时的 URL，也可以定义成 URL 列表。

#### callback

回调函数，即该 URL 对应的 Response 内容用哪个方法来解析，例如：

```python
def on_start(self):
    self.crawl('http://scrapy.org/', callback=self.index_page)
```

这里指定了 callback 为 index_page，就代表爬取：[http://scrapy.org/]('http://scrapy.org/) 链接得到的 Response 会用 index_page()方法 来解析。

index_page() 方法的第一个参数需要是 Response 对象，例如：

```python
def index_page(self, response):
    pass
```

方法中的 response参数就是请求上述 URL 得到的 Response对象，我们可以直接在 index_page() 方法中实现页面的解析。

#### age

任务的有效时间，如果某个任务在有效时间内且已经被执行，则不会重复执行，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=10*24*60*60)
```

或者可以这样设置：

```python
@config(age=10 * 24 * 60 * 60)
def callback(self):
    pass
```

默认的有效时间为 10 天。

#### priority

爬取任务的优先级，默认是 0，数字越大会越优先被调度，例如：

```python
def index_page(self):
    self.crawl('http://www.example.org/page.html', callback=self.index_page)
    self.crawl('http://www.example.org/233.html', callback=self.detail_page,
               priority=1)
```

这样第二个任务就会被优先调用，优先爬取 233.html 这个链接。

#### exetime

此参数可以设置定时任务，其值是时间戳，默认是 0，即代表立即执行，实例如下：

```python
import time
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               exetime=time.time()+30*60)
```

这样定义之后该任务会在 30 分钟之后再执行。

#### retries

可以定义重试次数，默认是 3 次。

#### itag

该参数设置一个判定网页是否发生变化的节点值，在下次爬取时会判定次节点是否和上次相同，如果相同，则证明页面没有更新，则不会重复爬取。实例如下：

```python
def index_page(self, response):
    for item in response.doc('.item').items():
        self.crawl(item.find('a').attr.url, callback=self.detail_page,
                   itag=item.find('.update-time').text())
```

在这里设置了更新时间这个节点的值为 itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。

#### auto_recrawl

当开启时，任务在过期时会重新爬取，循环时间即定义的 age 时间长度，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               age=5*60*60, auto_recrawl=True)
```

在这里定义了 age 有效期为 5 小时，同时设置了 auto_recrawl 为 True，这样任务就会每 5 小时执行一次。

#### method

HTTP 请求方式，默认是 GET，例如想发起 POST 请求可以设置为 POST。

#### params

GET 请求参数，在这里可以方便地使用 params 定义 GET 请求参数，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback,
               params={'a': 123, 'b': 'c'})
    self.crawl('http://httpbin.org/get?a=123&b=c', callback=self.callback)
```

这里两个爬取任务是等价的。

#### data

POST 表单数据，当请求方式为 POST 时，可以通过此参数传递表单数据，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', data={'a': 123, 'b': 'c'})
```

#### files

上传的文件，在这里需要指定文件名，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/post', callback=self.callback,
               method='POST', files={field: {filename: 'content'}})
```

#### user_agent

即爬取使用的 User-Agent。

#### headers

爬取时使用的 Headers，即 Request Headers。

#### cookies

爬取时使用的 Cookies，需要是字典格式。

#### connect_timeout

在初始化连接时最长等待时间，默认是 20 秒。

#### timeout

抓取网页时的最长等待时间，默认是 120 秒。

#### allow_redirects

是否自动处理重定向，默认是 True。

#### validate_cert

是否验证证书，此选项对 HTTPS 请求有效，默认是 True。

#### proxy

爬取时使用的代理，支持用户名密码的配置，格式如  username:password@hostname:port，实例如下：

```python
def on_start(self):
    self.crawl('http://httpbin.org/get', callback=self.callback, proxy='127.0.0.1:9743')
```

也可以设置全局配置，如：

```python
class Handler(BaseHandler):
    crawl_config = {
        'proxy': '127.0.0.1:9743'
    }
```

#### fetch_type

开启 PhantomJS 渲染，如果遇到 JavaScript 渲染的页面，抓取时如果指定此字段即可实现 PhantomJS 的对接，PySpider 将会使用 PhantomJS 进行网页的抓取，实例如下：

```python
def on_start(self):
    self.crawl('https://www.taobao.com', callback=self.index_page, fetch_type='js')
```

这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。

#### js_script

在页面加载完毕后执行的 JavaScript 脚本，如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               fetch_type='js', js_script='''
               function() {
                   window.scrollTo(0,document.body.scrollHeight);
                   return 123;
               }
               ''')
```

这样就可以在页面加载成功后执行页面混动的 JavaScript 代码，页面会下拉到最底部。

#### js_run_at

JavaScript 脚本运行的为止，是在页面节点开头还是结尾，默认是结尾，即 document-end。

#### js_viewport_width/js_viewport_height

JavaScript 渲染页面时的窗口大小。

#### load_images

在加载 JavaScript 页面时是否加载图片，默认是否。

#### save

此参数也非常有用，可以在不同的方法之间传递参数，例如：

```python
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.callback,
               save={'page': 1})

def callback(self, response):
    return response.save['page']
```

这样在 on_start() 方法中生成了 Request 并传递了额外的参数 page，在回调函数里可以通过 response 的 save 对象接收到。

#### cancel

取消任务，如果是一个 ACTIVE 状态的任务，则需要将force_update 设置为 True。

#### force_update

即使任务处于 ACTIVE 状态，那也会强制更新状态。

以上便是 crawl() 方法的参数介绍，更加详细的描述可以参考：[http://docs.pyspider.org/en/latest/apis/self.crawl/](http://docs.pyspider.org/en/latest/apis/self.crawl/)。


### 3. 任务区分

在 PySpider 判断两个任务是否是重复的是使用的是 URL 的 MD5 值作为任务的 ID，如果 ID 相同，那么就会判定为任务相同，其中一个就不会爬取了。但是很多情况下我们可能请求的链接是同一个，但是仅仅 POST 的参数不同，这时可以重写 task_id() 方法，改变这个 ID 的计算方式，实例如下：

```python
import json
from pyspider.libs.utils import md5string
def get_taskid(self, task):
    return md5string(task['url']+json.dumps(task['fetch'].get('data', '')))
```

这里就重写了 get_taskid() 方法，利用 URL 和 POST 的参数来生成 ID，这样即使 URL 相同，但是 POST 的参数不同，两个任务的 ID 就不同，不会被识别成重复任务。

### 4. 全局配置

在 PySpider 中可以使用 crawl_config 来指定全局的配置，配置中的参数会和 crawl() 方法创建任务时的参数合并，如我们要全局配置一个 Headers，那么可以定义如下：

```python
class Handler(BaseHandler):
    crawl_config = {
        'headers': {
            'User-Agent': 'GoogleBot',
        }
    }
```

### 5. 定时爬取

我们可以通过设置 every 属性来设置多久的时间间隔爬取一次，示例如下：

```python
@every(minutes=24 * 60)
def on_start(self):
    for url in urllist:
        self.crawl(url, callback=self.index_page)
```

这样就可以设置每天执行一次爬取。

但是在上文中我们提到了任务的有效时间，如果在有效时间内则不会重复爬取，所以我们需要把有效时间设置得比重复时间更短才可以实现定时爬取。

比如下面的例子就无法做到每天爬取：

```python
@every(minutes=24 * 60)
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.index_page)

@config(age=10 * 24 * 60 * 60)
def index_page(self):
    pass
```

在这里任务的过期时间为 10 天，但是自动爬取的时间为 1 天，当第二次尝试重新爬取的时候，PySpider 会监测到此任务尚未过期，所以不会执行爬取，所以我们需要将 age 设置得小于定时时间。

### 6. 项目状态

每个项目都有 6 个状态，分别是 TODO、STOP、CHECKING、DEBUG、RUNNING、PAUSE。

* TODO，是项目刚刚被创建还未实现时的状态。
* STOP，如果我们想停止此项目的抓取可以将其设置为STOP状态。
* CHECKING，当一个正在运行的项目被修改后就会变成此状态，一般是在中途发现错误需要调整的时候会遇到。
* DEBUG／RUNNING，这两个状态对项目本身运行没有区别，设置为任意一个状态都可以运行项目，但是对于我们来说可以用二者来区分项目是否已经测试通过。
* PAUSE，当爬取过程中出现连续多次错误时会自动设置为PAUSE状态并等待一定的时间继续爬取。

### 7. 抓取进度

在抓取时可以看到抓取的进度，在 progress 部分会显示四个进度条，如图 12-27 所示：

![](./pictures/12-27.jpg)

图 12-27 抓取进度

process 中的 5m、1h、1d 指的是最近 5 分钟、1 小时、1 天内的请求情况，all 代表所有的请求情况。

请求在这里有颜色表示，蓝色的代表等待被执行的任务，绿色代表成功的任务、黄色代表请求失败后等待重试的任务、红色代表失败次数过多而被忽略的任务，在这里都可以直观地看到爬取的进度和请求情况。

### 8. 删除项目

PySpider 中没有直接删除项目的选项，要删除任务需要将项目的状态设置为 STOP，然后将分组的名称设置为 delete，然后等待 24 小时后，项目会自动删除。

### 9. 结语

以上便是 PySpider 的常用用法，如要了解更多可以参考 PySpider 的官方文档：[http://docs.pyspider.org/](http://docs.pyspider.org/)。


